{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Md-Sanzid-Bin-Hossain/DeepBBWAE-Net/blob/main/Final_Main_Subject_1_superLearner_kinetics_Graph_neural_network%2BLSTM_series_CNN_10_treadmill_overground_slope_stair.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ_iJPPKh7zX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "461bb4f8-f75d-437f-c956-726087c44d57"
      },
      "source": [
        "# Let`s import all packages that we may need:\n",
        "import numpy\n",
        "import statistics\n",
        "from numpy import loadtxt\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import GRU,LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from statistics import stdev\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import resample\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy.signal import butter,filtfilt\n",
        "\n",
        "import sys\n",
        "import numpy as np # linear algebra\n",
        "from scipy.stats import randint\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL\n",
        "import matplotlib.pyplot as plt # this is used for the plot the graph\n",
        "import seaborn as sns # used for plot interactive graph.\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## for Deep-learing:\n",
        "import keras\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import np_utils\n",
        "import itertools\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Bidirectional\n",
        "#import constraint\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.regularizers import l2\n",
        "\n",
        "\n",
        "###  Library for attention layers\n",
        "\n",
        "import pandas as pd\n",
        "#import pyarrow.parquet as pq # Used to read the data\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.layers import * # Keras is the most friendly Neural Network library, this Kernel use a lot of layers classes\n",
        "from keras.models import Model\n",
        "from tqdm import tqdm # Processing time measurement\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend as K # The backend give us access to tensorflow operations and allow us to create the Attention class\n",
        "from keras import optimizers # Allow us to access the Adam class to modify some parameters\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold # Used to use Kfold to train our model\n",
        "from keras.callbacks import * # This object helps the model to train in a smarter way, avoiding overfitting\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "import pickle\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "import xgboost as xg\n",
        "\n",
        "### Early stopping\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3LfJSuph7zb",
        "outputId": "846594fa-a96c-433e-e5ee-5a2ff9b6d972"
      },
      "source": [
        "####### Subject 1 #######\n",
        "\n",
        "\n",
        "Features_1= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_treadmill_fast_features.csv', delimiter=',')\n",
        "Features_2= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_treadmill_normal_features.csv', delimiter=',')\n",
        "Features_3= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_treadmill_slow_1_features.csv', delimiter=',')\n",
        "Features_4= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_treadmill_slow_2_features.csv', delimiter=',')\n",
        "Features_5= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_treadmill_vfast_features.csv', delimiter=',')\n",
        "\n",
        "Features_6= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_overground_fast_features.csv', delimiter=',')\n",
        "Features_7= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_overground_normal_features.csv', delimiter=',')\n",
        "Features_8= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_overground_slow_features.csv', delimiter=',')\n",
        "Features_9= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_overground_vfast_features.csv', delimiter=',')\n",
        "\n",
        "Features_10= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_slope_1_features_ascent.csv', delimiter=',')\n",
        "Features_11= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_slope_1_features_decent.csv', delimiter=',')\n",
        "Features_12= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_slope_2_features_ascent.csv', delimiter=',')\n",
        "Features_13= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_slope_2_features_decent.csv', delimiter=',')\n",
        "Features_14= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_stair_1_features_ascent.csv', delimiter=',')\n",
        "Features_15= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_stair_1_features_decent.csv', delimiter=',')\n",
        "Features_16= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_stair_2_features_ascent.csv', delimiter=',')\n",
        "Features_17= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_stair_2_features_decent.csv', delimiter=',')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Output ###\n",
        "\n",
        "Output_1= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_treadmill_fast_v6.csv', delimiter=',')\n",
        "Output_2= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_treadmill_normal_v6.csv', delimiter=',')\n",
        "Output_3= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_treadmill_slow_1_v6.csv', delimiter=',')\n",
        "Output_4= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_treadmill_slow_2_v6.csv', delimiter=',')\n",
        "Output_5= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_treadmill_vfast_v6.csv', delimiter=',')\n",
        "\n",
        "Output_6= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_overground_fast.csv', delimiter=',')\n",
        "Output_7= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_overground_normal.csv', delimiter=',')\n",
        "Output_8= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_overground_slow.csv', delimiter=',')\n",
        "Output_9= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_overground_vfast.csv', delimiter=',')\n",
        "\n",
        "Output_10= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_slope_1_ascent.csv', delimiter=',')\n",
        "Output_11= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_slope_1_decent.csv', delimiter=',')\n",
        "Output_12= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_slope_2_ascent.csv', delimiter=',')\n",
        "Output_13= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_slope_2_decent.csv', delimiter=',')\n",
        "Output_14= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_stair_1_ascent.csv', delimiter=',')\n",
        "Output_15= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_stair_1_decent.csv', delimiter=',')\n",
        "Output_16= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_stair_2_ascent.csv', delimiter=',')\n",
        "Output_17= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_stair_2_decent.csv', delimiter=',')\n",
        "\n",
        "\n",
        "#### Slope and stair ###\n",
        "\n",
        "Features_18=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_slope_1_features.csv', header=None)\n",
        "Features_19= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_slope_2_features.csv', header=None)\n",
        "Features_20= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_stair_1_features.csv', header=None)\n",
        "Features_21=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_stair_2_features.csv', header=None)\n",
        "\n",
        "\n",
        "Output_18= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_slope_1.csv',skiprows = 1,header = None)\n",
        "Output_19= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_slope_2.csv', skiprows = 1,header = None)\n",
        "Output_20= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_stair_1.csv', skiprows = 1,header = None)\n",
        "Output_21= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T001_stair_2.csv', skiprows = 1,header = None)\n",
        "\n",
        "slope_1=np.concatenate((Features_18,Output_18),axis=1)\n",
        "slope_2=np.concatenate((Features_19,Output_19),axis=1)\n",
        "stair_1=np.concatenate((Features_20,Output_20),axis=1)\n",
        "stair_2=np.concatenate((Features_21,Output_21),axis=1)\n",
        "\n",
        "\n",
        "###          ###\n",
        "\n",
        "fast_dataset=np.concatenate((Features_1,Output_1),axis=1)\n",
        "normal_dataset=np.concatenate((Features_2,Output_2),axis=1)\n",
        "slow_1_dataset=np.concatenate((Features_3,Output_3),axis=1)\n",
        "slow_2_dataset=np.concatenate((Features_4,Output_4),axis=1)\n",
        "vfast_dataset=np.concatenate((Features_5,Output_5),axis=1)\n",
        "\n",
        "\n",
        "fast_dataset_o=np.concatenate((Features_6,Output_6),axis=1)\n",
        "normal_dataset_o=np.concatenate((Features_7,Output_7),axis=1)\n",
        "slow_dataset_o=np.concatenate((Features_8,Output_8),axis=1)\n",
        "vfast_dataset_o=np.concatenate((Features_9,Output_9),axis=1)\n",
        "\n",
        "\n",
        "slope_1_ascent=np.concatenate((Features_10,Output_10),axis=1)\n",
        "slope_1_decent=np.concatenate((Features_11,Output_11),axis=1)\n",
        "slope_2_ascent=np.concatenate((Features_12,Output_12),axis=1)\n",
        "slope_2_decent=np.concatenate((Features_13,Output_13),axis=1)\n",
        "stair_1_ascent=np.concatenate((Features_14,Output_14),axis=1)\n",
        "stair_1_decent=np.concatenate((Features_15,Output_15),axis=1)\n",
        "stair_2_ascent=np.concatenate((Features_16,Output_16),axis=1)\n",
        "stair_2_decent=np.concatenate((Features_17,Output_17),axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "subject_1_treadmill=np.concatenate((fast_dataset,normal_dataset,slow_2_dataset,vfast_dataset),axis=0)\n",
        "subject_1_overground=np.concatenate((fast_dataset_o,normal_dataset_o,slow_dataset_o,vfast_dataset_o),axis=0)\n",
        "# subject_1_stair_slope=np.concatenate((slope_1_ascent,slope_1_decent,slope_2_ascent,slope_2_decent,stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)\n",
        "subject_1_stair_slope=np.concatenate((stair_1_decent,stair_2_decent),axis=0)\n",
        "\n",
        "\n",
        "# subject_1=np.concatenate((subject_1_treadmill,subject_1_overground,subject_1_stair_slope),axis=0)\n",
        "# subject_1=np.concatenate((subject_1_treadmill,subject_1_overground),axis=0)\n",
        "\n",
        "subject_1=subject_1_treadmill\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#subject_1_stair_slope=np.concatenate((slope_1,slope_2,stair_1,stair_2),axis=0)\n",
        "# subject_1=subject_1_stair_slope\n",
        "\n",
        "              ### Extra features ###\n",
        "\n",
        "features_E_1=np.array([1,60.18,1.7,0.449,0.412,0.899])\n",
        "features_E_1=features_E_1.repeat(subject_1.shape[0])\n",
        "features_E_1=features_E_1.reshape(6,subject_1.shape[0])\n",
        "features_E_1=features_E_1.transpose()\n",
        "\n",
        "subject_1=np.concatenate((subject_1,features_E_1),axis=1)\n",
        "\n",
        "\n",
        "\n",
        "print(subject_1.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(54064, 166)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8rcHE_yh7zf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5e2310d-9fac-4d78-b63c-5057ecfe1d66"
      },
      "source": [
        "####### Subject 2 #######\n",
        "\n",
        "\n",
        "\n",
        "Features_1= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_treadmill_fast_features.csv', delimiter=',')\n",
        "Features_2= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_treadmill_normal_features.csv', delimiter=',')\n",
        "Features_3= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_treadmill_slow_1_features.csv', delimiter=',')\n",
        "Features_4= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_treadmill_slow_2_features.csv', delimiter=',')\n",
        "Features_5= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_treadmill_vfast_features.csv', delimiter=',')\n",
        "\n",
        "Features_6= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_overground_fast_features.csv', delimiter=',')\n",
        "Features_7= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_overground_normal_features.csv', delimiter=',')\n",
        "Features_8= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_overground_slow_features.csv', delimiter=',')\n",
        "Features_9= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_overground_vfast_features.csv', delimiter=',')\n",
        "\n",
        "Features_10= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_slope_1_features_ascent.csv', delimiter=',')\n",
        "Features_11= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_slope_1_features_decent.csv', delimiter=',')\n",
        "Features_12= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_slope_2_features_ascent.csv', delimiter=',')\n",
        "Features_13= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_slope_2_features_decent.csv', delimiter=',')\n",
        "Features_14= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_stair_1_features_ascent.csv', delimiter=',')\n",
        "Features_15= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_stair_1_features_decent.csv', delimiter=',')\n",
        "Features_16= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_stair_2_features_ascent.csv', delimiter=',')\n",
        "Features_17= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_stair_2_features_decent.csv', delimiter=',')\n",
        "\n",
        "\n",
        " ## output ##\n",
        "\n",
        "Output_1= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_treadmill_fast_v4.csv', delimiter=',')\n",
        "Output_2= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_treadmill_normal_v4.csv', delimiter=',')\n",
        "Output_3= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_treadmill_slow_1_v4.csv', delimiter=',')\n",
        "Output_4= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_treadmill_slow_2_v4.csv', delimiter=',')\n",
        "Output_5= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_treadmill_vfast_v4.csv', delimiter=',')\n",
        "\n",
        "Output_6= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_overground_fast.csv', delimiter=',')\n",
        "Output_7= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_overground_normal.csv', delimiter=',')\n",
        "Output_8= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_overground_slow.csv', delimiter=',')\n",
        "Output_9= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_overground_vfast.csv', delimiter=',')\n",
        "\n",
        "Output_10= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_slope_1_ascent.csv', delimiter=',')\n",
        "Output_11= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_slope_1_decent.csv', delimiter=',')\n",
        "Output_12= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_slope_2_ascent.csv', delimiter=',')\n",
        "Output_13= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_slope_2_decent.csv', delimiter=',')\n",
        "Output_14= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_stair_1_ascent.csv', delimiter=',')\n",
        "Output_15= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_stair_1_decent.csv', delimiter=',')\n",
        "Output_16= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_stair_2_ascent.csv', delimiter=',')\n",
        "Output_17= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_stair_2_decent.csv', delimiter=',')\n",
        "\n",
        "#### Slope and stair ###\n",
        "\n",
        "Features_18=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_slope_1_features.csv', header=None)\n",
        "Features_19= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_slope_2_features.csv', header=None)\n",
        "Features_20= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_stair_1_features.csv', header=None)\n",
        "Features_21=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_stair_2_features.csv', header=None)\n",
        "\n",
        "\n",
        "Output_18= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_slope_1.csv',skiprows = 1,header = None)\n",
        "Output_19= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_slope_2.csv', skiprows = 1,header = None)\n",
        "Output_20= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_stair_1.csv', skiprows = 1,header = None)\n",
        "Output_21= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T002_stair_2.csv', skiprows = 1,header = None)\n",
        "\n",
        "slope_1=np.concatenate((Features_18,Output_18),axis=1)\n",
        "slope_2=np.concatenate((Features_19,Output_19),axis=1)\n",
        "stair_1=np.concatenate((Features_20,Output_20),axis=1)\n",
        "stair_2=np.concatenate((Features_21,Output_21),axis=1)\n",
        "\n",
        "\n",
        "###          ###\n",
        "\n",
        "fast_dataset=np.concatenate((Features_1,Output_1),axis=1)\n",
        "normal_dataset=np.concatenate((Features_2,Output_2),axis=1)\n",
        "slow_1_dataset=np.concatenate((Features_3,Output_3),axis=1)\n",
        "slow_2_dataset=np.concatenate((Features_4,Output_4),axis=1)\n",
        "vfast_dataset=np.concatenate((Features_5,Output_5),axis=1)\n",
        "\n",
        "\n",
        "fast_dataset_o=np.concatenate((Features_6,Output_6),axis=1)\n",
        "normal_dataset_o=np.concatenate((Features_7,Output_7),axis=1)\n",
        "slow_dataset_o=np.concatenate((Features_8,Output_8),axis=1)\n",
        "vfast_dataset_o=np.concatenate((Features_9,Output_9),axis=1)\n",
        "\n",
        "\n",
        "slope_1_ascent=np.concatenate((Features_10,Output_10),axis=1)\n",
        "slope_1_decent=np.concatenate((Features_11,Output_11),axis=1)\n",
        "slope_2_ascent=np.concatenate((Features_12,Output_12),axis=1)\n",
        "slope_2_decent=np.concatenate((Features_13,Output_13),axis=1)\n",
        "stair_1_ascent=np.concatenate((Features_14,Output_14),axis=1)\n",
        "stair_1_decent=np.concatenate((Features_15,Output_15),axis=1)\n",
        "stair_2_ascent=np.concatenate((Features_16,Output_16),axis=1)\n",
        "stair_2_decent=np.concatenate((Features_17,Output_17),axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "subject_2_treadmill=np.concatenate((fast_dataset,normal_dataset,slow_2_dataset,vfast_dataset),axis=0)\n",
        "subject_2_overground=np.concatenate((fast_dataset_o,normal_dataset_o,slow_dataset_o,vfast_dataset_o),axis=0)\n",
        "subject_2_stair_slope=np.concatenate((slope_1_ascent,slope_1_decent,slope_2_ascent,slope_2_decent,stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)\n",
        "# subject_2_stair_slope=np.concatenate((stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)\n",
        "\n",
        "\n",
        "\n",
        "subject_2=np.concatenate((subject_2_treadmill,subject_2_overground,subject_2_stair_slope),axis=0)\n",
        "\n",
        "# subject_2=np.concatenate((subject_2_treadmill,subject_2_overground),axis=0)\n",
        "\n",
        "#subject_2_stair_slope=np.concatenate((slope_1,slope_2,stair_1,stair_2),axis=0)\n",
        "\n",
        "# subject_2=subject_2_stair_slope\n",
        "\n",
        "               ### Extra features ###\n",
        "\n",
        "              ### Extra features ###\n",
        "\n",
        "features_E_1=np.array([0,53.47,1.548,0.45,0.325,0.812])\n",
        "features_E_1=features_E_1.repeat(subject_2.shape[0])\n",
        "features_E_1=features_E_1.reshape(6,subject_2.shape[0])\n",
        "features_E_1=features_E_1.transpose()\n",
        "\n",
        "subject_2=np.concatenate((subject_2,features_E_1),axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(subject_2.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(97512, 166)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVxetqIIh7zh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d522ebdf-54ad-4507-85a6-482021d6050c"
      },
      "source": [
        "####### Subject 3 #######\n",
        "\n",
        "\n",
        "Features_1= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_treadmill_fast_features.csv', delimiter=',')\n",
        "Features_2= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_treadmill_normal_features.csv', delimiter=',')\n",
        "Features_3= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_treadmill_slow_1_features.csv', delimiter=',')\n",
        "Features_4= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_treadmill_slow_2_features.csv', delimiter=',')\n",
        "Features_5= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_treadmill_vfast_features.csv', delimiter=',')\n",
        "\n",
        "Features_6= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_overground_fast_features.csv', delimiter=',')\n",
        "Features_7= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_overground_normal_features.csv', delimiter=',')\n",
        "Features_8= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_overground_slow_features.csv', delimiter=',')\n",
        "Features_9= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_overground_vfast_features.csv', delimiter=',')\n",
        "\n",
        "Features_10= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_slope_1_features_ascent.csv', delimiter=',')\n",
        "Features_11= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_slope_1_features_decent.csv', delimiter=',')\n",
        "Features_12= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_slope_2_features_ascent.csv', delimiter=',')\n",
        "Features_13= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_slope_2_features_decent.csv', delimiter=',')\n",
        "Features_14= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_stair_1_features_ascent.csv', delimiter=',')\n",
        "Features_15= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_stair_1_features_decent.csv', delimiter=',')\n",
        "Features_16= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_stair_2_features_ascent.csv', delimiter=',')\n",
        "Features_17= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_stair_2_features_decent.csv', delimiter=',')\n",
        "\n",
        "\n",
        "\n",
        "Output_1= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_treadmill_fast_v2.csv', delimiter=',')\n",
        "Output_2= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_treadmill_normal_v2.csv', delimiter=',')\n",
        "Output_3= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_treadmill_slow_1_v2.csv', delimiter=',')\n",
        "Output_4= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_treadmill_slow_2_v2.csv', delimiter=',')\n",
        "Output_5= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_treadmill_vfast_v2.csv', delimiter=',')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## output ##\n",
        "\n",
        "Output_6= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_overground_fast.csv', delimiter=',')\n",
        "Output_7= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_overground_normal.csv', delimiter=',')\n",
        "Output_8= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_overground_slow.csv', delimiter=',')\n",
        "Output_9= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_overground_vfast.csv', delimiter=',')\n",
        "\n",
        "Output_10= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_slope_1_ascent.csv', delimiter=',')\n",
        "Output_11= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_slope_1_decent.csv', delimiter=',')\n",
        "Output_12= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_slope_2_ascent.csv', delimiter=',')\n",
        "Output_13= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_slope_2_decent.csv', delimiter=',')\n",
        "Output_14= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_stair_1_ascent.csv', delimiter=',')\n",
        "Output_15= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_stair_1_decent.csv', delimiter=',')\n",
        "Output_16= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_stair_2_ascent.csv', delimiter=',')\n",
        "Output_17= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_stair_2_decent.csv', delimiter=',')\n",
        "\n",
        "\n",
        "#### Slope and stair ###\n",
        "\n",
        "Features_18=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_slope_1_features.csv', header=None)\n",
        "Features_19= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_slope_2_features.csv', header=None)\n",
        "Features_20= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_stair_1_features.csv', header=None)\n",
        "Features_21=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_stair_2_features.csv', header=None)\n",
        "\n",
        "\n",
        "Output_18= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_slope_1.csv',skiprows = 1,header = None)\n",
        "Output_19= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_slope_2.csv', skiprows = 1,header = None)\n",
        "Output_20= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_stair_1.csv', skiprows = 1,header = None)\n",
        "Output_21= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T003_stair_2.csv', skiprows = 1,header = None)\n",
        "\n",
        "slope_1=np.concatenate((Features_18,Output_18),axis=1)\n",
        "slope_2=np.concatenate((Features_19,Output_19),axis=1)\n",
        "stair_1=np.concatenate((Features_20,Output_20),axis=1)\n",
        "stair_2=np.concatenate((Features_21,Output_21),axis=1)\n",
        "\n",
        "\n",
        "###          ###\n",
        "\n",
        "fast_dataset=np.concatenate((Features_1,Output_1),axis=1)\n",
        "normal_dataset=np.concatenate((Features_2,Output_2),axis=1)\n",
        "slow_1_dataset=np.concatenate((Features_3,Output_3),axis=1)\n",
        "slow_2_dataset=np.concatenate((Features_4,Output_4),axis=1)\n",
        "vfast_dataset=np.concatenate((Features_5,Output_5),axis=1)\n",
        "\n",
        "\n",
        "fast_dataset_o=np.concatenate((Features_6,Output_6),axis=1)\n",
        "normal_dataset_o=np.concatenate((Features_7,Output_7),axis=1)\n",
        "slow_dataset_o=np.concatenate((Features_8,Output_8),axis=1)\n",
        "vfast_dataset_o=np.concatenate((Features_9,Output_9),axis=1)\n",
        "\n",
        "\n",
        "slope_1_ascent=np.concatenate((Features_10,Output_10),axis=1)\n",
        "slope_1_decent=np.concatenate((Features_11,Output_11),axis=1)\n",
        "slope_2_ascent=np.concatenate((Features_12,Output_12),axis=1)\n",
        "slope_2_decent=np.concatenate((Features_13,Output_13),axis=1)\n",
        "stair_1_ascent=np.concatenate((Features_14,Output_14),axis=1)\n",
        "stair_1_decent=np.concatenate((Features_15,Output_15),axis=1)\n",
        "stair_2_ascent=np.concatenate((Features_16,Output_16),axis=1)\n",
        "stair_2_decent=np.concatenate((Features_17,Output_17),axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "subject_3_treadmill=np.concatenate((fast_dataset,normal_dataset,slow_2_dataset,vfast_dataset),axis=0)\n",
        "subject_3_overground=np.concatenate((fast_dataset_o,normal_dataset_o,slow_dataset_o,vfast_dataset_o),axis=0)\n",
        "subject_3_stair_slope=np.concatenate((slope_1_ascent,slope_1_decent,slope_2_ascent,slope_2_decent,stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)\n",
        "\n",
        "#subject_3_stair_slope=np.concatenate((stair_1_decent,stair_2_decent),axis=0)\n",
        "\n",
        "\n",
        "\n",
        "subject_3=np.concatenate((subject_3_treadmill,subject_3_overground,subject_3_stair_slope),axis=0)\n",
        "\n",
        "# subject_3=np.concatenate((subject_3_treadmill,subject_3_overground),axis=0)\n",
        "\n",
        "#subject_3_stair_slope=np.concatenate((slope_1,slope_2,stair_1,stair_2),axis=0)\n",
        "\n",
        "# subject_3=subject_3_stair_slope\n",
        "\n",
        "   ### Extra Features ###\n",
        "\n",
        "features_E_1=np.array([1,65.54,1.73,0.48,0.367,0.889])\n",
        "features_E_1=features_E_1.repeat(subject_3.shape[0])\n",
        "features_E_1=features_E_1.reshape(6,subject_3.shape[0])\n",
        "features_E_1=features_E_1.transpose()\n",
        "\n",
        "subject_3=np.concatenate((subject_3,features_E_1),axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(subject_3.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(98264, 166)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1gP58TYh7zj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da78fe77-ac0e-451a-bef4-16d83c96a187"
      },
      "source": [
        "####### Subject 4 #######\n",
        "\n",
        "\n",
        "Features_1= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_treadmill_fast_features.csv', delimiter=',')\n",
        "Features_2= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_treadmill_normal_features.csv', delimiter=',')\n",
        "Features_3= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_treadmill_slow_1_features.csv', delimiter=',')\n",
        "Features_4= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_treadmill_slow_2_features.csv', delimiter=',')\n",
        "Features_5= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_treadmill_vfast_features.csv', delimiter=',')\n",
        "\n",
        "Features_6= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_overground_fast_features.csv', delimiter=',')\n",
        "Features_7= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_overground_normal_features.csv', delimiter=',')\n",
        "Features_8= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_overground_slow_features.csv', delimiter=',')\n",
        "Features_9= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_overground_vfast_features.csv', delimiter=',')\n",
        "\n",
        "Features_10= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_slope_1_features_ascent.csv', delimiter=',')\n",
        "Features_11= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_slope_1_features_decent.csv', delimiter=',')\n",
        "Features_12= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_slope_2_features_ascent.csv', delimiter=',')\n",
        "Features_13= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_slope_2_features_decent.csv', delimiter=',')\n",
        "Features_14= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_stair_1_features_ascent.csv', delimiter=',')\n",
        "Features_15= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_stair_1_features_decent.csv', delimiter=',')\n",
        "Features_16= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_stair_2_features_ascent.csv', delimiter=',')\n",
        "Features_17= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_stair_2_features_decent.csv', delimiter=',')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Output_1= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_treadmill_fast_v2.csv', delimiter=',')\n",
        "Output_2= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_treadmill_normal_v2.csv', delimiter=',')\n",
        "Output_3= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_treadmill_slow_1_v2.csv', delimiter=',')\n",
        "Output_4= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_treadmill_slow_2_v2.csv', delimiter=',')\n",
        "Output_5= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_treadmill_vfast_v2.csv', delimiter=',')\n",
        "\n",
        "\n",
        "## output ##\n",
        "\n",
        "Output_6= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_overground_fast.csv', delimiter=',')\n",
        "Output_7= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_overground_normal.csv', delimiter=',')\n",
        "Output_8= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_overground_slow.csv', delimiter=',')\n",
        "Output_9= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_overground_vfast.csv', delimiter=',')\n",
        "\n",
        "Output_10= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_slope_1_ascent.csv', delimiter=',')\n",
        "Output_11= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_slope_1_decent.csv', delimiter=',')\n",
        "Output_12= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_slope_2_ascent.csv', delimiter=',')\n",
        "Output_13= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_slope_2_decent.csv', delimiter=',')\n",
        "Output_14= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_stair_1_ascent.csv', delimiter=',')\n",
        "Output_15= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_stair_1_decent.csv', delimiter=',')\n",
        "Output_16= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_stair_2_ascent.csv', delimiter=',')\n",
        "Output_17= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_stair_2_decent.csv', delimiter=',')\n",
        "\n",
        "\n",
        "#### Slope and stair ###\n",
        "\n",
        "Features_18=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_slope_1_features.csv', header=None)\n",
        "Features_19= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_slope_2_features.csv', header=None)\n",
        "Features_20= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_stair_1_features.csv', header=None)\n",
        "Features_21=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_stair_2_features.csv', header=None)\n",
        "\n",
        "\n",
        "Output_18= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_slope_1.csv',skiprows = 1,header = None)\n",
        "Output_19= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_slope_2.csv', skiprows = 1,header = None)\n",
        "Output_20= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_stair_1.csv', skiprows = 1,header = None)\n",
        "Output_21= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T004_stair_2.csv', skiprows = 1,header = None)\n",
        "\n",
        "slope_1=np.concatenate((Features_18,Output_18),axis=1)\n",
        "slope_2_1=np.concatenate((Features_19,Output_19),axis=1)\n",
        "stair_1=np.concatenate((Features_20,Output_20),axis=1)\n",
        "stair_2=np.concatenate((Features_21,Output_21),axis=1)\n",
        "\n",
        "###          ###\n",
        "\n",
        "fast_dataset=np.concatenate((Features_1,Output_1),axis=1)\n",
        "normal_dataset=np.concatenate((Features_2,Output_2),axis=1)\n",
        "slow_1_dataset=np.concatenate((Features_3,Output_3),axis=1)\n",
        "slow_2_dataset=np.concatenate((Features_4,Output_4),axis=1)\n",
        "vfast_dataset=np.concatenate((Features_5,Output_5),axis=1)\n",
        "\n",
        "\n",
        "fast_dataset_o=np.concatenate((Features_6,Output_6),axis=1)\n",
        "normal_dataset_o=np.concatenate((Features_7,Output_7),axis=1)\n",
        "slow_dataset_o=np.concatenate((Features_8,Output_8),axis=1)\n",
        "vfast_dataset_o=np.concatenate((Features_9,Output_9),axis=1)\n",
        "\n",
        "\n",
        "slope_1_ascent=np.concatenate((Features_10,Output_10),axis=1)\n",
        "slope_1_decent=np.concatenate((Features_11,Output_11),axis=1)\n",
        "slope_2_ascent=np.concatenate((Features_12,Output_12),axis=1)\n",
        "slope_2_decent=np.concatenate((Features_13,Output_13),axis=1)\n",
        "stair_1_ascent=np.concatenate((Features_14,Output_14),axis=1)\n",
        "stair_1_decent=np.concatenate((Features_15,Output_15),axis=1)\n",
        "stair_2_ascent=np.concatenate((Features_16,Output_16),axis=1)\n",
        "stair_2_decent=np.concatenate((Features_17,Output_17),axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "subject_4_treadmill=np.concatenate((fast_dataset,normal_dataset,slow_2_dataset,vfast_dataset),axis=0)\n",
        "subject_4_overground=np.concatenate((fast_dataset_o,normal_dataset_o,slow_dataset_o,vfast_dataset_o),axis=0)\n",
        "subject_4_stair_slope=np.concatenate((slope_1_ascent,slope_1_decent,slope_2_ascent,slope_2_decent,stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)\n",
        "\n",
        "\n",
        "# subject_4=subject_4_stair_slope\n",
        "# subject_4=np.concatenate((subject_4_treadmill,subject_4_overground),axis=0)\n",
        "\n",
        "subject_4=np.concatenate((subject_4_treadmill,subject_4_overground,subject_4_stair_slope),axis=0)\n",
        "\n",
        "              ### Extra features ###\n",
        "\n",
        "features_E_1=np.array([1,55.62,1.62,0.444,0.363,0.807])\n",
        "features_E_1=features_E_1.repeat(subject_4.shape[0])\n",
        "features_E_1=features_E_1.reshape(6,subject_4.shape[0])\n",
        "features_E_1=features_E_1.transpose()\n",
        "\n",
        "subject_4=np.concatenate((subject_4,features_E_1),axis=1)\n",
        "\n",
        "\n",
        "print(subject_4.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100113, 166)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG4fa-4zh7zl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d095e03c-a368-4612-c506-a75d53525e31"
      },
      "source": [
        "####### Subject 5 #######\n",
        "\n",
        "\n",
        "Features_1= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_treadmill_fast_features.csv', delimiter=',')\n",
        "Features_2= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_treadmill_normal_features.csv', delimiter=',')\n",
        "Features_3= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_treadmill_slow_1_features.csv', delimiter=',')\n",
        "Features_4= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_treadmill_slow_2_features.csv', delimiter=',')\n",
        "Features_5= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_treadmill_vfast_1_features.csv', delimiter=',')\n",
        "Features_6= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_treadmill_vfast_2_features.csv', delimiter=',')\n",
        "\n",
        "Features_7= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_overground_fast_features.csv', delimiter=',')\n",
        "Features_8= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_overground_normal_features.csv', delimiter=',')\n",
        "Features_9= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_overground_slow_features.csv', delimiter=',')\n",
        "Features_10= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_overground_vfast_features.csv', delimiter=',')\n",
        "\n",
        "Features_11= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_slope_1_features_ascent.csv', delimiter=',')\n",
        "Features_12= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_slope_1_features_decent.csv', delimiter=',')\n",
        "Features_13= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_slope_2_features_ascent.csv', delimiter=',')\n",
        "Features_14= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_slope_2_features_decent.csv', delimiter=',')\n",
        "Features_15= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_stair_1_features_ascent.csv', delimiter=',')\n",
        "Features_16= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_stair_1_features_decent.csv', delimiter=',')\n",
        "Features_17= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_stair_2_features_ascent.csv', delimiter=',')\n",
        "Features_18= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_stair_2_features_decent.csv', delimiter=',')\n",
        "\n",
        "\n",
        "\n",
        "Output_1= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_treadmill_fast_v9.csv', delimiter=',')\n",
        "Output_2= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_treadmill_normal_v9.csv', delimiter=',')\n",
        "Output_3= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_treadmill_slow_1_v9.csv', delimiter=',')\n",
        "Output_4= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_treadmill_slow_2_v9.csv', delimiter=',')\n",
        "Output_5= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_treadmill_vfast_1_v9.csv', delimiter=',')\n",
        "Output_6= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_treadmill_vfast_2_v9.csv', delimiter=',')\n",
        "\n",
        "## output ##\n",
        "\n",
        "Output_7= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_overground_fast.csv', delimiter=',')\n",
        "Output_8= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_overground_normal.csv', delimiter=',')\n",
        "Output_9= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_overground_slow.csv', delimiter=',')\n",
        "Output_10= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_overground_vfast.csv', delimiter=',')\n",
        "\n",
        "Output_11= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_slope_1_ascent.csv', delimiter=',')\n",
        "Output_12= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_slope_1_decent.csv', delimiter=',')\n",
        "Output_13= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_slope_2_ascent.csv', delimiter=',')\n",
        "Output_14= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_slope_2_decent.csv', delimiter=',')\n",
        "Output_15= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_stair_1_ascent.csv', delimiter=',')\n",
        "Output_16= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_stair_1_decent.csv', delimiter=',')\n",
        "Output_17= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_stair_2_ascent.csv', delimiter=',')\n",
        "Output_18= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_stair_2_decent.csv', delimiter=',')\n",
        "\n",
        "\n",
        "#### Slope and stair ###\n",
        "\n",
        "Features_19=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_slope_1_features.csv', header=None)\n",
        "Features_20= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_slope_2_features.csv', header=None)\n",
        "Features_21= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_stair_1_features.csv', header=None)\n",
        "Features_22=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_stair_2_features.csv', header=None)\n",
        "\n",
        "\n",
        "Output_19= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_slope_1.csv',skiprows = 1,header = None)\n",
        "Output_20= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_slope_2.csv', skiprows = 1,header = None)\n",
        "Output_21= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_stair_1.csv', skiprows = 1,header = None)\n",
        "Output_22= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T005_stair_2.csv', skiprows = 1,header = None)\n",
        "\n",
        "slope_1=np.concatenate((Features_18,Output_18),axis=1)\n",
        "slope_2=np.concatenate((Features_19,Output_19),axis=1)\n",
        "stair_1=np.concatenate((Features_20,Output_20),axis=1)\n",
        "stair_2=np.concatenate((Features_21,Output_21),axis=1)\n",
        "\n",
        "\n",
        "###          ###\n",
        "\n",
        "\n",
        "fast_dataset=np.concatenate((Features_1,Output_1),axis=1)\n",
        "normal_dataset=np.concatenate((Features_2,Output_2),axis=1)\n",
        "slow_1_dataset=np.concatenate((Features_3,Output_3),axis=1)\n",
        "slow_2_dataset=np.concatenate((Features_4,Output_4),axis=1)\n",
        "vfast_dataset_1=np.concatenate((Features_5,Output_5),axis=1)\n",
        "vfast_dataset_2=np.concatenate((Features_6,Output_6),axis=1)\n",
        "\n",
        "\n",
        "fast_dataset_o=np.concatenate((Features_7,Output_7),axis=1)\n",
        "normal_dataset_o=np.concatenate((Features_8,Output_8),axis=1)\n",
        "slow_dataset_o=np.concatenate((Features_9,Output_9),axis=1)\n",
        "vfast_dataset_o=np.concatenate((Features_10,Output_10),axis=1)\n",
        "\n",
        "\n",
        "slope_1_ascent=np.concatenate((Features_11,Output_11),axis=1)\n",
        "slope_1_decent=np.concatenate((Features_12,Output_12),axis=1)\n",
        "slope_2_ascent=np.concatenate((Features_13,Output_13),axis=1)\n",
        "slope_2_decent=np.concatenate((Features_14,Output_14),axis=1)\n",
        "stair_1_ascent=np.concatenate((Features_15,Output_15),axis=1)\n",
        "stair_1_decent=np.concatenate((Features_16,Output_16),axis=1)\n",
        "stair_2_ascent=np.concatenate((Features_17,Output_17),axis=1)\n",
        "stair_2_decent=np.concatenate((Features_18,Output_18),axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "subject_5_treadmill=np.concatenate((fast_dataset,normal_dataset,slow_2_dataset,vfast_dataset_1,vfast_dataset_2),axis=0)\n",
        "subject_5_overground=np.concatenate((fast_dataset_o,normal_dataset_o,slow_dataset_o,vfast_dataset_o),axis=0)\n",
        "subject_5_stair_slope=np.concatenate((slope_1_ascent,slope_1_decent,slope_2_ascent,slope_2_decent,stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#subject_5_stair_slope=np.concatenate((stair_1_decent,stair_2_decent),axis=0)\n",
        "\n",
        "\n",
        "subject_5=np.concatenate((subject_5_treadmill,subject_5_overground,subject_5_stair_slope),axis=0)\n",
        "\n",
        "# subject_5=np.concatenate((subject_5_treadmill,subject_5_overground),axis=0)\n",
        "\n",
        "#subject_5_stair_slope=np.concatenate((slope_1,slope_2,stair_1,stair_2),axis=0)\n",
        "# subject_5=subject_5_stair_slope\n",
        "\n",
        "              ### Extra features ###\n",
        "\n",
        "features_E_1=np.array([1,72.9,1.74,0.498,0.363,0.902])\n",
        "features_E_1=features_E_1.repeat(subject_5.shape[0])\n",
        "features_E_1=features_E_1.reshape(6,subject_5.shape[0])\n",
        "features_E_1=features_E_1.transpose()\n",
        "\n",
        "subject_5=np.concatenate((subject_5,features_E_1),axis=1)\n",
        "\n",
        "\n",
        "\n",
        "print(subject_5.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(88889, 166)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-Xc-bPJh2pQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "067d3f11-0793-475d-ed6e-a08942e0eb2a"
      },
      "source": [
        "#### Subject 6 ###\n",
        "\n",
        "Features_1= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_treadmill_fast_features.csv', delimiter=',')\n",
        "Features_2= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_treadmill_normal_features.csv', delimiter=',')\n",
        "Features_3= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_treadmill_slow_1_features.csv', delimiter=',')\n",
        "Features_4= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_treadmill_slow_2_features.csv', delimiter=',')\n",
        "Features_5= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_treadmill_vfast_features.csv', delimiter=',')\n",
        "\n",
        "Features_6= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_overground_fast_features.csv', delimiter=',')\n",
        "Features_7= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_overground_normal_features.csv', delimiter=',')\n",
        "Features_8= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_overground_slow_features.csv', delimiter=',')\n",
        "Features_9= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_overground_vfast_features.csv', delimiter=',')\n",
        "\n",
        "Features_10= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_slope_1_features_ascent.csv', delimiter=',')\n",
        "Features_11= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_slope_1_features_decent.csv', delimiter=',')\n",
        "Features_12= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_slope_2_features_ascent.csv', delimiter=',')\n",
        "Features_13= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_slope_2_features_decent.csv', delimiter=',')\n",
        "Features_14= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_stair_1_features_ascent.csv', delimiter=',')\n",
        "Features_15= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_stair_1_features_decent.csv', delimiter=',')\n",
        "Features_16= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_stair_2_features_ascent.csv', delimiter=',')\n",
        "Features_17= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_stair_2_features_decent.csv', delimiter=',')\n",
        "\n",
        "\n",
        "\n",
        "Output_1= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_treadmill_fast_v5.csv', delimiter=',')\n",
        "Output_2= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_treadmill_normal_v5.csv', delimiter=',')\n",
        "Output_3= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_treadmill_slow_1_v5.csv', delimiter=',')\n",
        "Output_4= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_treadmill_slow_2_v5.csv', delimiter=',')\n",
        "Output_5= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_treadmill_vfast_v5.csv', delimiter=',')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## output ##\n",
        "\n",
        "Output_6= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_overground_fast.csv', delimiter=',')\n",
        "Output_7= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_overground_normal.csv', delimiter=',')\n",
        "Output_8= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_overground_slow.csv', delimiter=',')\n",
        "Output_9= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_overground_vfast.csv', delimiter=',')\n",
        "\n",
        "Output_10= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_slope_1_ascent.csv', delimiter=',')\n",
        "Output_11= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_slope_1_decent.csv', delimiter=',')\n",
        "Output_12= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_slope_2_ascent.csv', delimiter=',')\n",
        "Output_13= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_slope_2_decent.csv', delimiter=',')\n",
        "Output_14= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_stair_1_ascent.csv', delimiter=',')\n",
        "Output_15= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_stair_1_decent.csv', delimiter=',')\n",
        "Output_16= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_stair_2_ascent.csv', delimiter=',')\n",
        "Output_17= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_stair_2_decent.csv', delimiter=',')\n",
        "\n",
        "\n",
        "\n",
        "#### Slope and stair ###\n",
        "\n",
        "Features_18=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_slope_1_features.csv', header=None)\n",
        "Features_19= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_slope_2_features.csv', header=None)\n",
        "Features_20= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_stair_1_features.csv', header=None)\n",
        "Features_21=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_stair_2_features.csv', header=None)\n",
        "\n",
        "\n",
        "Output_18= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_slope_1.csv',skiprows = 1,header = None)\n",
        "Output_19= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_slope_2.csv', skiprows = 1,header = None)\n",
        "Output_20= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_stair_1.csv', skiprows = 1,header = None)\n",
        "Output_21= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T006_stair_2.csv', skiprows = 1,header = None)\n",
        "\n",
        "slope_1=np.concatenate((Features_18,Output_18),axis=1)\n",
        "slope_2=np.concatenate((Features_19,Output_19),axis=1)\n",
        "stair_1=np.concatenate((Features_20,Output_20),axis=1)\n",
        "stair_2=np.concatenate((Features_21,Output_21),axis=1)\n",
        "\n",
        "\n",
        "###          ###\n",
        "\n",
        "\n",
        "fast_dataset=np.concatenate((Features_1,Output_1),axis=1)\n",
        "normal_dataset=np.concatenate((Features_2,Output_2),axis=1)\n",
        "slow_1_dataset=np.concatenate((Features_3,Output_3),axis=1)\n",
        "slow_2_dataset=np.concatenate((Features_4,Output_4),axis=1)\n",
        "vfast_dataset=np.concatenate((Features_5,Output_5),axis=1)\n",
        "\n",
        "\n",
        "fast_dataset_o=np.concatenate((Features_6,Output_6),axis=1)\n",
        "normal_dataset_o=np.concatenate((Features_7,Output_7),axis=1)\n",
        "slow_dataset_o=np.concatenate((Features_8,Output_8),axis=1)\n",
        "vfast_dataset_o=np.concatenate((Features_9,Output_9),axis=1)\n",
        "\n",
        "\n",
        "slope_1_ascent=np.concatenate((Features_10,Output_10),axis=1)\n",
        "slope_1_decent=np.concatenate((Features_11,Output_11),axis=1)\n",
        "slope_2_ascent=np.concatenate((Features_12,Output_12),axis=1)\n",
        "slope_2_decent=np.concatenate((Features_13,Output_13),axis=1)\n",
        "stair_1_ascent=np.concatenate((Features_14,Output_14),axis=1)\n",
        "stair_1_decent=np.concatenate((Features_15,Output_15),axis=1)\n",
        "stair_2_ascent=np.concatenate((Features_16,Output_16),axis=1)\n",
        "stair_2_decent=np.concatenate((Features_17,Output_17),axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "subject_6_treadmill=np.concatenate((fast_dataset,normal_dataset,slow_1_dataset,vfast_dataset),axis=0)\n",
        "subject_6_overground=np.concatenate((fast_dataset_o,normal_dataset_o,slow_dataset_o,vfast_dataset_o),axis=0)\n",
        "subject_6_stair_slope=np.concatenate((slope_1_ascent,slope_1_decent,slope_2_ascent,slope_2_decent,stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)\n",
        "\n",
        "#subject_6_stair_slope=np.concatenate((stair_1_decent,stair_2_decent),axis=0)\n",
        "\n",
        "# subject_6=subject_6_stair_slope\n",
        "subject_6=np.concatenate((subject_6_treadmill,subject_6_overground,subject_6_stair_slope),axis=0)\n",
        "\n",
        "# subject_6=np.concatenate((subject_6_treadmill,subject_6_overground),axis=0)\n",
        "\n",
        "#subject_6_stair_slope=np.concatenate((slope_1,slope_2,stair_1,stair_2),axis=0)\n",
        "\n",
        "              ### Extra features ###\n",
        "\n",
        "features_E_1=np.array([1,74.48,1.68,0.478,0.337,0.865])\n",
        "features_E_1=features_E_1.repeat(subject_6.shape[0])\n",
        "features_E_1=features_E_1.reshape(6,subject_6.shape[0])\n",
        "features_E_1=features_E_1.transpose()\n",
        "\n",
        "subject_6=np.concatenate((subject_6,features_E_1),axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(subject_6.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(102585, 166)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpfTEAcOaCQv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4de2f1a-6498-475e-f812-eced7d737999"
      },
      "source": [
        "#### Subject 7 ###\n",
        "\n",
        "Features_1= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_treadmill_fast_features.csv', delimiter=',')\n",
        "Features_2= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_treadmill_normal_features.csv', delimiter=',')\n",
        "Features_3= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_treadmill_slow_1_features.csv', delimiter=',')\n",
        "Features_4= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_treadmill_slow_2_features.csv', delimiter=',')\n",
        "Features_5= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_treadmill_vfast_features.csv', delimiter=',')\n",
        "\n",
        "Features_6= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_overground_fast_features.csv', delimiter=',')\n",
        "Features_7= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_overground_normal_features.csv', delimiter=',')\n",
        "Features_8= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_overground_slow_features.csv', delimiter=',')\n",
        "Features_9= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_overground_vfast_features.csv', delimiter=',')\n",
        "\n",
        "Features_10= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_slope_1_features_ascent.csv', delimiter=',')\n",
        "Features_11= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_slope_1_features_decent.csv', delimiter=',')\n",
        "Features_12= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_slope_2_features_ascent.csv', delimiter=',')\n",
        "Features_13= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_slope_2_features_decent.csv', delimiter=',')\n",
        "Features_14= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_stair_1_features_ascent.csv', delimiter=',')\n",
        "Features_15= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_stair_1_features_decent.csv', delimiter=',')\n",
        "Features_16= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_stair_2_features_ascent.csv', delimiter=',')\n",
        "Features_17= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_stair_2_features_decent.csv', delimiter=',')\n",
        "\n",
        "## features ##\n",
        "\n",
        "Output_1= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_treadmill_fast_v3.csv', delimiter=',')\n",
        "Output_2= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_treadmill_normal_v3.csv', delimiter=',')\n",
        "Output_3= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_treadmill_slow_1_v3.csv', delimiter=',')\n",
        "Output_4= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_treadmill_slow_2_v3.csv', delimiter=',')\n",
        "Output_5= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_treadmill_vfast_v3.csv', delimiter=',')\n",
        "\n",
        "## output ##\n",
        "\n",
        "Output_6= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_overground_fast.csv', delimiter=',')\n",
        "Output_7= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_overground_normal.csv', delimiter=',')\n",
        "Output_8= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_overground_slow.csv', delimiter=',')\n",
        "Output_9= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_overground_vfast.csv', delimiter=',')\n",
        "\n",
        "Output_10= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_slope_1_ascent.csv', delimiter=',')\n",
        "Output_11= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_slope_1_decent.csv', delimiter=',')\n",
        "Output_12= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_slope_2_ascent.csv', delimiter=',')\n",
        "Output_13= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_slope_2_decent.csv', delimiter=',')\n",
        "Output_14= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_stair_1_ascent.csv', delimiter=',')\n",
        "Output_15= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_stair_1_decent.csv', delimiter=',')\n",
        "Output_16= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_stair_2_ascent.csv', delimiter=',')\n",
        "Output_17= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_stair_2_decent.csv', delimiter=',')\n",
        "\n",
        "\n",
        "#### Slope and stair ###\n",
        "\n",
        "Features_18=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_slope_1_features.csv', header=None)\n",
        "Features_19= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_slope_2_features.csv', header=None)\n",
        "Features_20= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_stair_1_features.csv', header=None)\n",
        "Features_21=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_stair_2_features.csv', header=None)\n",
        "\n",
        "\n",
        "Output_18= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_slope_1.csv',skiprows = 1,header = None)\n",
        "Output_19= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_slope_2.csv', skiprows = 1,header = None)\n",
        "Output_20= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_stair_1.csv', skiprows = 1,header = None)\n",
        "Output_21= pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/P002_T007_stair_2.csv', skiprows = 1,header = None)\n",
        "\n",
        "slope_1=np.concatenate((Features_18,Output_18),axis=1)\n",
        "slope_2=np.concatenate((Features_19,Output_19),axis=1)\n",
        "stair_1=np.concatenate((Features_20,Output_20),axis=1)\n",
        "stair_2=np.concatenate((Features_21,Output_21),axis=1)\n",
        "\n",
        "\n",
        "###          ###\n",
        "\n",
        "\n",
        "fast_dataset=np.concatenate((Features_1,Output_1),axis=1)\n",
        "normal_dataset=np.concatenate((Features_2,Output_2),axis=1)\n",
        "slow_1_dataset=np.concatenate((Features_3,Output_3),axis=1)\n",
        "slow_2_dataset=np.concatenate((Features_4,Output_4),axis=1)\n",
        "vfast_dataset=np.concatenate((Features_5,Output_5),axis=1)\n",
        "\n",
        "\n",
        "fast_dataset_o=np.concatenate((Features_6,Output_6),axis=1)\n",
        "normal_dataset_o=np.concatenate((Features_7,Output_7),axis=1)\n",
        "slow_dataset_o=np.concatenate((Features_8,Output_8),axis=1)\n",
        "vfast_dataset_o=np.concatenate((Features_9,Output_9),axis=1)\n",
        "\n",
        "\n",
        "slope_1_ascent=np.concatenate((Features_10,Output_10),axis=1)\n",
        "slope_1_decent=np.concatenate((Features_11,Output_11),axis=1)\n",
        "slope_2_ascent=np.concatenate((Features_12,Output_12),axis=1)\n",
        "slope_2_decent=np.concatenate((Features_13,Output_13),axis=1)\n",
        "stair_1_ascent=np.concatenate((Features_14,Output_14),axis=1)\n",
        "stair_1_decent=np.concatenate((Features_15,Output_15),axis=1)\n",
        "stair_2_ascent=np.concatenate((Features_16,Output_16),axis=1)\n",
        "stair_2_decent=np.concatenate((Features_17,Output_17),axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "subject_7_treadmill=np.concatenate((fast_dataset,normal_dataset,slow_1_dataset,slow_2_dataset,vfast_dataset),axis=0)\n",
        "subject_7_overground=np.concatenate((fast_dataset_o,normal_dataset_o,slow_dataset_o,vfast_dataset_o),axis=0)\n",
        "subject_7_stair_slope=np.concatenate((slope_1_ascent,slope_1_decent,slope_2_ascent,slope_2_decent,stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)\n",
        "\n",
        "#subject_7_stair_slope=np.concatenate((stair_1_decent,stair_2_decent),axis=0)\n",
        "\n",
        "\n",
        "\n",
        "subject_7=np.concatenate((subject_7_treadmill,subject_7_overground,subject_7_stair_slope),axis=0)\n",
        "#\n",
        "# subject_7=np.concatenate((subject_7_treadmill,subject_7_overground),axis=0)\n",
        "\n",
        "# subject_7_stair_slope=np.concatenate((slope_1,slope_2,stair_1,stair_2),axis=0)\n",
        "# subject_7=subject_7_stair_slope\n",
        "\n",
        "              ### Extra features ###\n",
        "\n",
        "features_E_1=np.array([1,68.9,1.67,0.443,0.349,0.838])\n",
        "features_E_1=features_E_1.repeat(subject_7.shape[0])\n",
        "features_E_1=features_E_1.reshape(6,subject_7.shape[0])\n",
        "features_E_1=features_E_1.transpose()\n",
        "\n",
        "subject_7=np.concatenate((subject_7,features_E_1),axis=1)\n",
        "\n",
        "\n",
        "\n",
        "print(subject_7.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(103870, 166)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtaB8gQPIWBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78083a33-012d-43b2-850d-ec8cbf5d1a25"
      },
      "source": [
        "#### Subject 8 ###\n",
        "\n",
        "Features_1= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_treadmill_fast_features.csv', delimiter=',')\n",
        "Features_2= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_treadmill_normal_features.csv', delimiter=',')\n",
        "Features_3= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_treadmill_slow_1_features.csv', delimiter=',')\n",
        "Features_4= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_treadmill_slow_2_features.csv', delimiter=',')\n",
        "Features_5= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_treadmill_vfast_features.csv', delimiter=',')\n",
        "\n",
        "\n",
        "Features_6= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_overground_fast_features.csv', delimiter=',')\n",
        "Features_7= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_overground_normal_features.csv', delimiter=',')\n",
        "Features_8= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_overground_slow_features.csv', delimiter=',')\n",
        "Features_9= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_overground_vfast_features.csv', delimiter=',')\n",
        "\n",
        "Features_10= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_slope_1_features_ascent.csv', delimiter=',')\n",
        "Features_11= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_slope_1_features_decent.csv', delimiter=',')\n",
        "Features_12= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_slope_2_features_ascent.csv', delimiter=',')\n",
        "Features_13= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_slope_2_features_decent.csv', delimiter=',')\n",
        "Features_14= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_stair_1_features_ascent.csv', delimiter=',')\n",
        "Features_15= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_stair_1_features_decent.csv', delimiter=',')\n",
        "Features_16= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_stair_2_features_ascent.csv', delimiter=',')\n",
        "Features_17= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_stair_2_features_decent.csv', delimiter=',')\n",
        "\n",
        "\n",
        "\n",
        "Output_1= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_treadmill_fast_v5.csv', delimiter=',')\n",
        "Output_2= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_treadmill_normal_v5.csv', delimiter=',')\n",
        "Output_3= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_treadmill_slow_1_v5.csv', delimiter=',')\n",
        "Output_4= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_treadmill_slow_2_v5.csv', delimiter=',')\n",
        "Output_5= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_treadmill_vfast_v5.csv', delimiter=',')\n",
        "\n",
        "## output ##\n",
        "\n",
        "Output_6= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_overground_fast.csv', delimiter=',')\n",
        "Output_7= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_overground_normal.csv', delimiter=',')\n",
        "Output_8= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_overground_slow.csv', delimiter=',')\n",
        "Output_9= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_overground_vfast.csv', delimiter=',')\n",
        "\n",
        "Output_10= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_slope_1_ascent.csv', delimiter=',')\n",
        "Output_11= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_slope_1_decent.csv', delimiter=',')\n",
        "Output_12= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_slope_2_ascent.csv', delimiter=',')\n",
        "Output_13= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_slope_2_decent.csv', delimiter=',')\n",
        "Output_14= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_stair_1_ascent.csv', delimiter=',')\n",
        "Output_15= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_stair_1_decent.csv', delimiter=',')\n",
        "Output_16= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_stair_2_ascent.csv', delimiter=',')\n",
        "Output_17= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T008_stair_2_decent.csv', delimiter=',')\n",
        "\n",
        "\n",
        "\n",
        "fast_dataset=np.concatenate((Features_1,Output_1),axis=1)\n",
        "normal_dataset_1=np.concatenate((Features_2,Output_2),axis=1)\n",
        "slow_1_dataset=np.concatenate((Features_3,Output_3),axis=1)\n",
        "slow_2_dataset=np.concatenate((Features_4,Output_4),axis=1)\n",
        "vfast_dataset=np.concatenate((Features_5,Output_5),axis=1)\n",
        "\n",
        "normal_dataset=normal_dataset_1[0:12000,:]\n",
        "\n",
        "\n",
        "fast_dataset_o=np.concatenate((Features_6,Output_6),axis=1)\n",
        "normal_dataset_o=np.concatenate((Features_7,Output_7),axis=1)\n",
        "slow_dataset_o=np.concatenate((Features_8,Output_8),axis=1)\n",
        "vfast_dataset_o=np.concatenate((Features_9,Output_9),axis=1)\n",
        "\n",
        "\n",
        "slope_1_ascent=np.concatenate((Features_10,Output_10),axis=1)\n",
        "slope_1_decent=np.concatenate((Features_11,Output_11),axis=1)\n",
        "slope_2_ascent=np.concatenate((Features_12,Output_12),axis=1)\n",
        "slope_2_decent=np.concatenate((Features_13,Output_13),axis=1)\n",
        "stair_1_ascent=np.concatenate((Features_14,Output_14),axis=1)\n",
        "stair_1_decent=np.concatenate((Features_15,Output_15),axis=1)\n",
        "stair_2_ascent=np.concatenate((Features_16,Output_16),axis=1)\n",
        "stair_2_decent=np.concatenate((Features_17,Output_17),axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "subject_8_treadmill=np.concatenate((fast_dataset,normal_dataset,slow_1_dataset,vfast_dataset),axis=0)\n",
        "subject_8_overground=np.concatenate((fast_dataset_o,slow_dataset_o,normal_dataset_o,vfast_dataset_o),axis=0)\n",
        "subject_8_stair_slope=np.concatenate((slope_1_ascent,slope_1_decent,slope_2_ascent,slope_2_decent,stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)\n",
        "\n",
        "\n",
        "\n",
        "subject_8=np.concatenate((subject_8_treadmill,subject_8_overground,subject_8_stair_slope),axis=0)\n",
        "\n",
        "# subject_8=np.concatenate((subject_8_treadmill,subject_8_overground),axis=0)\n",
        "\n",
        "# subject_8=subject_8_stair_slope\n",
        "\n",
        "              ### Extra features ###\n",
        "\n",
        "features_E_1=np.array([0,65.18,1.61,0.481,0.362,0.876])\n",
        "features_E_1=features_E_1.repeat(subject_8.shape[0])\n",
        "features_E_1=features_E_1.reshape(6,subject_8.shape[0])\n",
        "features_E_1=features_E_1.transpose()\n",
        "\n",
        "subject_8=np.concatenate((subject_8,features_E_1),axis=1)\n",
        "\n",
        "\n",
        "\n",
        "print(subject_8.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(90659, 166)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jgl62VVaIadv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29db7d5d-1711-4957-9255-74fab7dd0132"
      },
      "source": [
        "#### Subject 9 ###\n",
        "\n",
        "Features_1= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_treadmill_fast_features.csv', delimiter=',')\n",
        "Features_2= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_treadmill_normal_features.csv', delimiter=',')\n",
        "Features_3= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_treadmill_slow_1_features.csv', delimiter=',')\n",
        "Features_4= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_treadmill_slow_2_features.csv', delimiter=',')\n",
        "Features_5= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_treadmill_vfast_features.csv', delimiter=',')\n",
        "\n",
        "Features_6= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_overground_fast_features.csv', delimiter=',')\n",
        "Features_7= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_overground_normal_features.csv', delimiter=',')\n",
        "Features_8= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_overground_slow_features.csv', delimiter=',')\n",
        "Features_9= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_overground_vfast_features.csv', delimiter=',')\n",
        "\n",
        "Features_10= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_slope_1_features_ascent.csv', delimiter=',')\n",
        "Features_11= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_slope_1_features_decent.csv', delimiter=',')\n",
        "Features_12= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_slope_2_features_ascent.csv', delimiter=',')\n",
        "Features_13= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_slope_2_features_decent.csv', delimiter=',')\n",
        "Features_14= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_stair_1_features_ascent.csv', delimiter=',')\n",
        "Features_15= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_stair_1_features_decent.csv', delimiter=',')\n",
        "Features_16= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_stair_2_features_ascent.csv', delimiter=',')\n",
        "Features_17= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_stair_2_features_decent.csv', delimiter=',')\n",
        "\n",
        "\n",
        "\n",
        "Output_1= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_treadmill_fast_v3.csv', delimiter=',')\n",
        "Output_2= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_treadmill_normal_v3.csv', delimiter=',')\n",
        "Output_3= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_treadmill_slow_1_v3.csv', delimiter=',')\n",
        "Output_4= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_treadmill_slow_2_v3.csv', delimiter=',')\n",
        "Output_5= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_treadmill_vfast_v3.csv', delimiter=',')\n",
        "\n",
        "## output ##\n",
        "\n",
        "Output_6= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_overground_fast.csv', delimiter=',')\n",
        "Output_7= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_overground_normal.csv', delimiter=',')\n",
        "Output_8= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_overground_slow.csv', delimiter=',')\n",
        "Output_9= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_overground_vfast.csv', delimiter=',')\n",
        "\n",
        "Output_10= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_slope_1_ascent.csv', delimiter=',')\n",
        "Output_11= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_slope_1_decent.csv', delimiter=',')\n",
        "Output_12= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_slope_2_ascent.csv', delimiter=',')\n",
        "Output_13= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_slope_2_decent.csv', delimiter=',')\n",
        "Output_14= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_stair_1_ascent.csv', delimiter=',')\n",
        "Output_15= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_stair_1_decent.csv', delimiter=',')\n",
        "Output_16= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_stair_2_ascent.csv', delimiter=',')\n",
        "Output_17= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T009_stair_2_decent.csv', delimiter=',')\n",
        "\n",
        "\n",
        "\n",
        "fast_dataset=np.concatenate((Features_1,Output_1),axis=1)\n",
        "normal_dataset=np.concatenate((Features_2,Output_2),axis=1)\n",
        "slow_1_dataset=np.concatenate((Features_3,Output_3),axis=1)\n",
        "slow_2_dataset=np.concatenate((Features_4,Output_4),axis=1)\n",
        "vfast_dataset_1=np.concatenate((Features_5,Output_5),axis=1)\n",
        "vfast_dataset=vfast_dataset_1[0:3000,:]\n",
        "\n",
        "\n",
        "\n",
        "fast_dataset_o=np.concatenate((Features_6,Output_6),axis=1)\n",
        "normal_dataset_o=np.concatenate((Features_7,Output_7),axis=1)\n",
        "slow_dataset_o=np.concatenate((Features_8,Output_8),axis=1)\n",
        "vfast_dataset_o=np.concatenate((Features_9,Output_9),axis=1)\n",
        "\n",
        "\n",
        "slope_1_ascent=np.concatenate((Features_10,Output_10),axis=1)\n",
        "slope_1_decent=np.concatenate((Features_11,Output_11),axis=1)\n",
        "slope_2_ascent=np.concatenate((Features_12,Output_12),axis=1)\n",
        "slope_2_decent=np.concatenate((Features_13,Output_13),axis=1)\n",
        "stair_1_ascent=np.concatenate((Features_14,Output_14),axis=1)\n",
        "stair_1_decent=np.concatenate((Features_15,Output_15),axis=1)\n",
        "stair_2_ascent=np.concatenate((Features_16,Output_16),axis=1)\n",
        "stair_2_decent=np.concatenate((Features_17,Output_17),axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "subject_9_treadmill=np.concatenate((fast_dataset,normal_dataset,slow_1_dataset,vfast_dataset),axis=0)\n",
        "subject_9_overground=np.concatenate((fast_dataset_o,slow_dataset_o,normal_dataset_o,vfast_dataset_o),axis=0)\n",
        "subject_9_stair_slope=np.concatenate((slope_1_ascent,slope_1_decent,slope_2_ascent,slope_2_decent,stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)\n",
        "\n",
        "#subject_9_stair_slope=np.concatenate((stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)\n",
        "\n",
        "subject_9=np.concatenate((subject_9_treadmill,subject_9_overground,subject_9_stair_slope),axis=0)\n",
        "\n",
        "# subject_9=np.concatenate((subject_9_treadmill,subject_9_overground),axis=0)\n",
        "\n",
        "# subject_9=subject_9_stair_slope\n",
        "\n",
        "              ### Extra features ###\n",
        "\n",
        "features_E_1=np.array([0,57.91,1.59,0.468,0.337,0.878])\n",
        "features_E_1=features_E_1.repeat(subject_9.shape[0])\n",
        "features_E_1=features_E_1.reshape(6,subject_9.shape[0])\n",
        "features_E_1=features_E_1.transpose()\n",
        "\n",
        "subject_9=np.concatenate((subject_9,features_E_1),axis=1)\n",
        "\n",
        "\n",
        "print(subject_9.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(95461, 166)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L9FF316FgBF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0703b2f-25a6-46dc-c0f3-a85af521d249"
      },
      "source": [
        "#### Subject 10 ###\n",
        "\n",
        "Features_1= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_treadmill_fast_features_v1.csv', delimiter=',')\n",
        "Features_2= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_treadmill_normal_features_v1.csv', delimiter=',')\n",
        "Features_3= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_treadmill_slow_1_features_v1.csv', delimiter=',')\n",
        "Features_4= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_treadmill_slow_2_features_v1.csv', delimiter=',')\n",
        "Features_5= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_treadmill_vfast_features_v1.csv', delimiter=',')\n",
        "\n",
        "Features_6= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_overground_fast_features.csv', delimiter=',')\n",
        "Features_7= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_overground_normal_features.csv', delimiter=',')\n",
        "Features_8= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_overground_slow_features.csv', delimiter=',')\n",
        "Features_9= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_overground_vfast_features.csv', delimiter=',')\n",
        "\n",
        "Features_10= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_slope_1_features_ascent.csv', delimiter=',')\n",
        "Features_11= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_slope_1_features_decent.csv', delimiter=',')\n",
        "Features_12= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_slope_2_features_ascent.csv', delimiter=',')\n",
        "Features_13= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_slope_2_features_decent.csv', delimiter=',')\n",
        "Features_14= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_stair_1_features_ascent.csv', delimiter=',')\n",
        "Features_15= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_stair_1_features_decent.csv', delimiter=',')\n",
        "Features_16= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_stair_2_features_ascent.csv', delimiter=',')\n",
        "Features_17= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_stair_2_features_decent.csv', delimiter=',')\n",
        "\n",
        "\n",
        "\n",
        "Output_1= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_treadmill_fast_v6.csv', delimiter=',')\n",
        "Output_2= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_treadmill_normal_v6.csv', delimiter=',')\n",
        "Output_3= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_treadmill_slow_1_v6.csv', delimiter=',')\n",
        "Output_4= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_treadmill_slow_2_v6.csv', delimiter=',')\n",
        "Output_5= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_treadmill_vfast_v6.csv', delimiter=',')\n",
        "\n",
        "## output ##\n",
        "\n",
        "Output_6= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_overground_fast.csv', delimiter=',')\n",
        "Output_7= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_overground_normal.csv', delimiter=',')\n",
        "Output_8= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_overground_slow.csv', delimiter=',')\n",
        "Output_9= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_overground_vfast.csv', delimiter=',')\n",
        "\n",
        "Output_10= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_slope_1_ascent.csv', delimiter=',')\n",
        "Output_11= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_slope_1_decent.csv', delimiter=',')\n",
        "Output_12= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_slope_2_ascent.csv', delimiter=',')\n",
        "Output_13= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_slope_2_decent.csv', delimiter=',')\n",
        "Output_14= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_stair_1_ascent.csv', delimiter=',')\n",
        "Output_15= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_stair_1_decent.csv', delimiter=',')\n",
        "Output_16= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_stair_2_ascent.csv', delimiter=',')\n",
        "Output_17= loadtxt('/content/drive/My Drive/data for machine learning_stair_slope/P002_T010_stair_2_decent.csv', delimiter=',')\n",
        "\n",
        "\n",
        "\n",
        "fast_dataset=np.concatenate((Features_1,Output_1),axis=1)\n",
        "normal_dataset=np.concatenate((Features_2,Output_2),axis=1)\n",
        "slow_1_dataset=np.concatenate((Features_3,Output_3),axis=1)\n",
        "slow_2_dataset=np.concatenate((Features_4,Output_4),axis=1)\n",
        "vfast_dataset=np.concatenate((Features_5,Output_5),axis=1)\n",
        "\n",
        "\n",
        "\n",
        "fast_dataset_o=np.concatenate((Features_6,Output_6),axis=1)\n",
        "normal_dataset_o=np.concatenate((Features_7,Output_7),axis=1)\n",
        "slow_dataset_o=np.concatenate((Features_8,Output_8),axis=1)\n",
        "vfast_dataset_o=np.concatenate((Features_9,Output_9),axis=1)\n",
        "\n",
        "\n",
        "slope_1_ascent=np.concatenate((Features_10,Output_10),axis=1)\n",
        "slope_1_decent=np.concatenate((Features_11,Output_11),axis=1)\n",
        "slope_2_ascent=np.concatenate((Features_12,Output_12),axis=1)\n",
        "slope_2_decent=np.concatenate((Features_13,Output_13),axis=1)\n",
        "stair_1_ascent=np.concatenate((Features_14,Output_14),axis=1)\n",
        "stair_1_decent=np.concatenate((Features_15,Output_15),axis=1)\n",
        "stair_2_ascent=np.concatenate((Features_16,Output_16),axis=1)\n",
        "stair_2_decent=np.concatenate((Features_17,Output_17),axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "subject_10_treadmill=np.concatenate((fast_dataset,normal_dataset,slow_2_dataset,vfast_dataset),axis=0)\n",
        "subject_10_overground=np.concatenate((fast_dataset_o,slow_dataset_o,normal_dataset_o,vfast_dataset_o),axis=0)\n",
        "subject_10_stair_slope=np.concatenate((slope_1_ascent,slope_1_decent,slope_2_ascent,slope_2_decent,stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)\n",
        "\n",
        "\n",
        "subject_10=np.concatenate((subject_10_treadmill,subject_10_overground,subject_10_stair_slope),axis=0)\n",
        "\n",
        "# subject_10=np.concatenate((subject_10_treadmill,subject_10_overground),axis=0)\n",
        "# subject_10=subject_10_stair_slope\n",
        "\n",
        "\n",
        "features_E_1=np.array([0,57.91,1.59,0.468,0.337,0.878])\n",
        "features_E_1=features_E_1.repeat(subject_10.shape[0])\n",
        "features_E_1=features_E_1.reshape(6,subject_10.shape[0])\n",
        "features_E_1=features_E_1.transpose()\n",
        "\n",
        "subject_10=np.concatenate((subject_10,features_E_1),axis=1)\n",
        "\n",
        "\n",
        "\n",
        "print(subject_10.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(107860, 166)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFlJ7iDTh7zn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c64ef68-7c15-42e8-f9d4-eeed118c195e"
      },
      "source": [
        "train_dataset=np.concatenate((subject_7,subject_9,subject_2,subject_3,subject_10,subject_4,subject_6,subject_8,subject_5),axis=0)\n",
        "test_dataset=subject_1\n",
        "\n",
        "\n",
        "print(train_dataset.shape,test_dataset.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(885213, 166) (54064, 166)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e71AmCPYh7zp",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a5ecefe-1923-4126-a9d7-02019380a797"
      },
      "source": [
        "# Sensor 1- Sternum\n",
        "# Sensor 2-Sacrum\n",
        "# Sensor 3-R_thigh\n",
        "# Sensor 4-L_thigh\n",
        "# Sensor 5-R_shank\n",
        "# Sensor 6-L_shank\n",
        "# Sensor 7-R_dorsal\n",
        "  # Sensor 8-L_dorsal\n",
        "\n",
        "\n",
        "# Train features #\n",
        "train_1=train_dataset[:,0:8]\n",
        "train_2=train_dataset[:,17:25]\n",
        "train_3=train_dataset[:,34:42]\n",
        "train_4=train_dataset[:,51:59]\n",
        "train_5=train_dataset[:,68:76]\n",
        "train_6=train_dataset[:,85:93]\n",
        "train_7=train_dataset[:,102:110]\n",
        "train_8=train_dataset[:,119:127]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_7_1=train_dataset[:,103:105]\n",
        "train_7_2=train_dataset[:,106:108]\n",
        "\n",
        "train_8_1=train_dataset[:,120:122]\n",
        "train_8_2=train_dataset[:,123:125]\n",
        "\n",
        "\n",
        "## Extra Features\n",
        "\n",
        "train_extra=train_dataset[:,160:166]\n",
        "\n",
        "#x_1=train_4\n",
        "\n",
        "#x_train=np.concatenate((train_7_1,train_7_2,train_8_1,train_8_2),axis=1)\n",
        "\n",
        "x_train=np.concatenate((train_7,train_8),axis=1)\n",
        "#x_train=np.concatenate((train_3,train_4,train_5,train_6,train_7,train_8),axis=1)\n",
        "\n",
        "#x_train=train_extra\n",
        "\n",
        "\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "\n",
        "#train_X_1=scaler.fit_transform(x_train)\n",
        "\n",
        "\n",
        "train_X_1=x_train\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Test features #\n",
        "\n",
        "test_1=test_dataset[:,0:8]\n",
        "test_2=test_dataset[:,17:25]\n",
        "test_3=test_dataset[:,34:42]\n",
        "test_4=test_dataset[:,51:59]\n",
        "test_5=test_dataset[:,68:76]\n",
        "test_6=test_dataset[:,85:93]\n",
        "test_7=test_dataset[:,102:110]\n",
        "test_8=test_dataset[:,119:127]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_7_1=test_dataset[:,103:105]\n",
        "test_7_2=test_dataset[:,106:108]\n",
        "\n",
        "test_8_1=test_dataset[:,120:122]\n",
        "test_8_2=test_dataset[:,123:125]\n",
        "\n",
        "   ### Extra features  ###\n",
        "\n",
        "test_extra=test_dataset[:,160:166]\n",
        "\n",
        "#x_test=np.concatenate((test_7_1,test_7_2,test_8_1,test_8_2),axis=1)\n",
        "\n",
        "\n",
        "x_test=np.concatenate((test_7,test_8),axis=1)\n",
        "#x_test=np.concatenate((test_3,test_4,test_5,test_6,test_7,test_8),axis=1)\n",
        "\n",
        "#x_test=test_extra\n",
        "\n",
        "\n",
        "#test_X_1=scaler.fit_transform(x_test)\n",
        "\n",
        "test_X_1=x_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "f=88\n",
        "\n",
        "y_1_1=train_dataset[:,(f+55):(f+56)]\n",
        "y_1_2=train_dataset[:,(f+58):(f+60)]\n",
        "y_1_3=train_dataset[:,(f+62):(f+63)]\n",
        "y_1_4=train_dataset[:,(f+65):(f+67)]\n",
        "\n",
        "\n",
        "\n",
        "#train_y_1= np.concatenate((y_1_R,y_1_R_1,y_1_L,y_1_L_1),axis=1)     # 1-horizontal, 0-vertical---main\n",
        "\n",
        "\n",
        "train_y_1=np.concatenate((y_1_1,y_1_2,y_1_3,y_1_4),axis=1)\n",
        "\n",
        "\n",
        "\n",
        "y_2_1=test_dataset[:,(f+55):(f+56)]\n",
        "y_2_2=test_dataset[:,(f+58):(f+60)]\n",
        "y_2_3=test_dataset[:,(f+62):(f+63)]\n",
        "y_2_4=test_dataset[:,(f+65):(f+67)]\n",
        "\n",
        "\n",
        "\n",
        "test_y_1= np.concatenate((y_2_1,y_2_2,y_2_3,y_2_4),axis=1)\n",
        "\n",
        "#test_y_1= np.concatenate((y_2_1,y_2_2),axis=1)\n",
        "# test_y_1=y_2_1\n",
        "\n",
        "\n",
        "print(train_X_1.shape,test_X_1.shape,train_y_1.shape,test_y_1.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(885213, 16) (54064, 16) (885213, 6) (54064, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAHdOf8Zh7zr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "214cf64d-4e62-4945-8c4b-69a8eaf3849b"
      },
      "source": [
        "L1=len(train_X_1)\n",
        "L2=len(test_X_1)\n",
        "\n",
        "\n",
        "\n",
        "w=80\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "a1=L1//w\n",
        "b1=L1%w\n",
        "\n",
        "a2=L2//w\n",
        "b2=L2%w\n",
        "\n",
        "\n",
        "     #### Features ####\n",
        "train_X_2=train_X_1[L1-w+b1:L1,:]\n",
        "test_X_2=test_X_1[L2-w+b2:L2,:]\n",
        "\n",
        "    #### Output ####\n",
        "\n",
        "train_y_2=train_y_1[L1-w+b1:L1,:]\n",
        "test_y_2=test_y_1[L2-w+b2:L2,:]\n",
        "\n",
        "\n",
        "     #### Features ####\n",
        "\n",
        "train_X=np.concatenate((train_X_1,train_X_2),axis=0)\n",
        "test_X=np.concatenate((test_X_1,test_X_2),axis=0)\n",
        "\n",
        "\n",
        "    #### Output ####\n",
        "\n",
        "train_y=np.concatenate((train_y_1,train_y_2),axis=0)\n",
        "test_y=np.concatenate((test_y_1,test_y_2),axis=0)\n",
        "\n",
        "\n",
        "    #### Reshaping ####\n",
        "train_X_3 = train_X.reshape((a1+1,w,train_X.shape[1]))\n",
        "test_X = test_X.reshape((a2+1,w,test_X.shape[1]))\n",
        "\n",
        "# train_y_3= train_y.reshape((a1+1,w,train_y.shape[1]))\n",
        "# test_y= test_y.reshape((a2+1,w,test_y.shape[1]))\n",
        "\n",
        "###### Single output #####\n",
        "train_y_3= train_y.reshape((a1+1,6*w))\n",
        "test_y= test_y.reshape((a2+1,6*w))\n",
        "\n",
        "# train_y_3= train_y.reshape((a1+1,w))\n",
        "# test_y= test_y.reshape((a2+1,w))\n",
        "\n",
        "\n",
        "train_len=len(train_X_3)\n",
        "\n",
        "#train_X=train_X_3\n",
        "# train_y=train_y_3\n",
        "\n",
        "test_X_1D=test_X\n",
        "\n",
        "print(train_X_3.shape,train_y_3.shape,test_X.shape,test_y.shape,train_len,test_X_1D.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11066, 80, 16) (11066, 480) (676, 80, 16) (676, 480) 11066 (676, 80, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVN0TzObh7zu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f81c242c-052c-4909-8200-a68c8e55d0ad"
      },
      "source": [
        "train_X_1D, X_validation_1D, train_y_5, Y_validation = train_test_split(train_X_3,train_y_3, test_size=0.20, random_state=True)\n",
        "#train_X_1D, X_validation_1D_ridge, train_y, Y_validation_ridge = train_test_split(train_X_1D_m,train_y_m, test_size=0.10, random_state=True)\n",
        "\n",
        "print(train_X_1D.shape,train_y_5.shape,X_validation_1D.shape,Y_validation.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8852, 80, 16) (8852, 480) (2214, 80, 16) (2214, 480)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QTWRNSph7zt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "372a9115-2382-4db4-e58e-f250617893ca"
      },
      "source": [
        "train_X_2D=train_X_1D.reshape(train_X_1D.shape[0],train_X_1D.shape[1],8,2)\n",
        "test_X_2D=test_X_1D.reshape(test_X_1D.shape[0],test_X_1D.shape[1],8,2)\n",
        "X_validation_2D= X_validation_1D.reshape(X_validation_1D.shape[0],X_validation_1D.shape[1],8,2)\n",
        "#X_validation_2D_ridge= X_validation_1D_ridge.reshape(X_validation_1D_ridge.shape[0],X_validation_1D_ridge.shape[1],8,2)\n",
        "\n",
        "\n",
        "print(train_X_2D.shape,test_X_2D.shape,X_validation_2D.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8852, 80, 8, 2) (676, 80, 8, 2) (2214, 80, 8, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgOhMcrR6XX_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40a59e5a-89b1-4f86-b16f-6358d2429f11"
      },
      "source": [
        "Bag_samples=train_X_2D.shape[0]\n",
        "print(Bag_samples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfXxtxxnWdN4"
      },
      "source": [
        "#         ### bagboost model 1 ###\n",
        "\n",
        "\n",
        "\n",
        "# # # n_models=10\n",
        "\n",
        "# # # for i in range(n_models):\n",
        "\n",
        "# #     # The shape of the input image.\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.ensemble import GradientBoostingRegressor\n",
        "# from sklearn.multioutput import MultiOutputRegressor\n",
        "# import pickle\n",
        "# from lightgbm import LGBMRegressor\n",
        "# from sklearn.linear_model import Ridge\n",
        "# import xgboost as xg\n",
        "# from sklearn.utils import resample\n",
        "\n",
        "\n",
        "\n",
        "# i=0\n",
        "# n_splits = 10\n",
        "# features=8\n",
        "\n",
        "\n",
        "# for _ in range(n_splits):\n",
        "# \t# select indexes\n",
        "#   ix = [i for i in range(len(train_X_2D))]\n",
        "#   train_ix = resample(ix, replace=True, n_samples=Bag_samples)\n",
        "#   test_ix = [x for x in ix if x not in train_ix]\n",
        "# \t# select data\n",
        "#   train_X=train_X_2D[train_ix]\n",
        "#   X_validation_bag=train_X_2D[test_ix]\n",
        "#   train_y=train_y_5[train_ix]\n",
        "#   y_validation_bag=train_y_5[test_ix]\n",
        "\n",
        "#   ### Model ###\n",
        "\n",
        "#   inputs = tf.keras.layers.Input( shape=( w ,features, 2) )\n",
        "\n",
        "#   inputs_1 =BatchNormalization()(inputs)\n",
        "\n",
        "\n",
        "#   X=Conv2D(64, (3, 3), activation='relu',kernel_regularizer=l2(0.00001),padding='same')(inputs_1)\n",
        "#   X=BatchNormalization()(X)\n",
        "#   X=MaxPooling2D((2, 2))(X)\n",
        "#   X=Conv2D(64, (3, 3), activation='relu',kernel_regularizer=l2(0.00001), padding='same')(X)\n",
        "#   X=BatchNormalization()(X)\n",
        "#   X=MaxPooling2D((2, 2))(X)\n",
        "#   X=Conv2D(128, (3, 3), activation='relu',kernel_regularizer=l2(0.00001), padding='same')(X)\n",
        "#   X=BatchNormalization()(X)\n",
        "#   X=MaxPooling2D((2, 2))(X)\n",
        "\n",
        "#   dense=Dense(512, activation='relu')(X)\n",
        "#   dense=Dropout(0.35)(dense)\n",
        "#   dense=Dense(256,activation='relu')(dense)\n",
        "#   dense=Dropout(0.35)(dense)\n",
        "#   dense=Flatten()(dense)\n",
        "#   output=Dense(6*w,bias_regularizer=l2(0.00001), activation='linear')(dense)\n",
        "#   model_1 = Model(inputs, output)\n",
        "#   sgd = SGD(lr=0.001, decay=1e-5, momentum=0.9, nesterov=True)\n",
        "#   model_1.compile(loss='mean_squared_error', optimizer='Adam')\n",
        "\n",
        "\n",
        "\n",
        "#   history=model_1.fit(train_X, train_y, epochs=250, batch_size=64, validation_data=(X_validation_2D,Y_validation), verbose=2, shuffle=False)\n",
        "\n",
        "#   filename_1 = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/bagging/model_' + str(i + 1) + '.h5'\n",
        "#   model_1.save(filename_1)\n",
        "#   print('>Saved %s' % filename_1)\n",
        "\n",
        "\n",
        "#   X_train_bag=model_1.predict(X_validation_2D)\n",
        "#   X_train_bag=X_train_bag.reshape(X_validation_2D.shape[0]*w,6)\n",
        "\n",
        "#   train_y_bag=Y_validation.reshape(X_validation_2D.shape[0]*w,6)\n",
        "\n",
        "#   model = MultiOutputRegressor(GradientBoostingRegressor(n_estimators=200,verbose=2))\n",
        "#   model.fit(X_train_bag, train_y_bag)\n",
        "\n",
        "#   pkl_filename = \"pickle_model.pkl\"\n",
        "#   with open(pkl_filename, 'wb') as file:\n",
        "#       pickle.dump(model, file)\n",
        "\n",
        "\n",
        "#   filename= '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/boosting/model_' + str(i + 1) + '.pkl'\n",
        "#   with open(filename, 'wb') as file:\n",
        "#       pickle.dump(model, file)\n",
        "#   print('>Saved %s' % filename)\n",
        "#   i=i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHbhrIqcWkPJ"
      },
      "source": [
        "#          ### Base model 1  ###\n",
        "\n",
        "# # n_models=10\n",
        "\n",
        "# # for i in range(n_models):\n",
        "# from lightgbm import LGBMRegressor\n",
        "# from sklearn.linear_model import Ridge\n",
        "# import xgboost as xg\n",
        "\n",
        "\n",
        "# inputs = tf.keras.layers.Input( shape=( w ,features, 2) )\n",
        "\n",
        "# inputs_1 =BatchNormalization()(inputs)\n",
        "\n",
        "\n",
        "# X=Conv2D(64, (3, 3), activation='relu',kernel_regularizer=l2(0.00001),padding='same')(inputs_1)\n",
        "# X=BatchNormalization()(X)\n",
        "# X=MaxPooling2D((2, 2))(X)\n",
        "# X=Conv2D(64, (3, 3), activation='relu',kernel_regularizer=l2(0.00001), padding='same')(X)\n",
        "# X=BatchNormalization()(X)\n",
        "# X=MaxPooling2D((2, 2))(X)\n",
        "# X=Conv2D(128, (3, 3), activation='relu',kernel_regularizer=l2(0.00001), padding='same')(X)\n",
        "# X=BatchNormalization()(X)\n",
        "# X=MaxPooling2D((2, 2))(X)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# dense=Dense(512, activation='relu')(X)\n",
        "# dense=Dropout(0.35)(dense)\n",
        "# dense=Dense(256,activation='relu')(dense)\n",
        "# dense=Dropout(0.35)(dense)\n",
        "# dense=Flatten()(dense)\n",
        "# output=Dense(6*w,bias_regularizer=l2(0.00001), activation='linear')(dense)\n",
        "# model_1 = Model(inputs, output)\n",
        "# sgd = SGD(lr=0.001, decay=1e-5, momentum=0.9, nesterov=True)\n",
        "# model_1.compile(loss='mean_squared_error', optimizer='Adam')\n",
        "\n",
        "\n",
        "# history=model_1.fit(train_X_2D, train_y_5, epochs=250, batch_size=64, validation_data=(X_validation_2D, Y_validation), verbose=2, shuffle=False)\n",
        "\n",
        "\n",
        "# # # # summarize history for loss\n",
        "# plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['val_loss'])\n",
        "# plt.title('model loss')\n",
        "# plt.ylabel('loss')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train', 'test'], loc='upper right')\n",
        "# plt.show()\n",
        "# model_1.summary()\n",
        "\n",
        "# filename = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/model_base.h5'\n",
        "# model_1.save(filename)\n",
        "# print('>Saved %s' % filename)\n",
        "\n",
        "\n",
        "\n",
        "# X_train_bag=model_1.predict(X_validation_2D)\n",
        "# X_train_bag=X_train_bag.reshape(X_validation_2D.shape[0]*w,6)\n",
        "\n",
        "# train_y_bag=Y_validation.reshape(X_validation_2D.shape[0]*w,6)\n",
        "\n",
        "# model = MultiOutputRegressor(GradientBoostingRegressor(n_estimators=200,verbose=2))\n",
        "# model.fit(X_train_bag, train_y_bag)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# filename_1= '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/model_Base_GB_1.pkl'\n",
        "# with open(filename_1, 'wb') as file:\n",
        "#     pickle.dump(model, file)\n",
        "# print('>Saved %s' % filename_1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaRws8o28seK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f63a428e-bcd4-4ae4-9d35-797790583ef6"
      },
      "source": [
        "# load models from file\n",
        "def load_all_models(n_models):\n",
        "    all_models = list()\n",
        "    for i in range(n_models):\n",
        "        # define filename for this ensemble\n",
        "        filename = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/bagging/model_' + str(i + 1) + '.h5'\n",
        "        # load model from file\n",
        "        model = tf.keras.models.load_model(filename)\n",
        "        # add to list of members\n",
        "        all_models.append(model)\n",
        "        print('>loaded %s' % filename)\n",
        "    return all_models\n",
        "\n",
        "\n",
        "\n",
        "       ### Load model and do prediction ###\n",
        "\n",
        "n_members =10\n",
        "members = load_all_models(n_members)\n",
        "\n",
        "print('Loaded %d models' % len(members))\n",
        "\n",
        "\n",
        "# create stacked model input dataset as outputs from the ensemble\n",
        "def stacked_dataset(members, inputX, inputX_lstm):\n",
        "    stackX = None\n",
        "    l=1\n",
        "    for model in members:\n",
        "        # make prediction\n",
        "        yhat = model.predict(inputX, verbose=0)\n",
        "        # stack predictions into [rows, members, probabilities]\n",
        "        if stackX is None:\n",
        "            stackX = yhat\n",
        "        else:\n",
        "            stackX = np.dstack((stackX, yhat))\n",
        "    # flatten predictions to [rows, members x probabilities]\n",
        "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1],stackX.shape[2]))\n",
        "    return stackX\n",
        "\n",
        "             ### Function for boosting model ###\n",
        "\n",
        "\n",
        "#  models from file\n",
        "def load_all_boosting_models(n_models):\n",
        "    all_models_boost = list()\n",
        "    for i in range(n_members_boost):\n",
        "        # define filename for this ensemble\n",
        "        filename_boost = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/boosting/model_' + str(i + 1) + '.pkl'\n",
        "        # load model from file\n",
        "        with open(filename_boost, 'rb') as fp:\n",
        "            model_boost = pickle.load(fp)\n",
        "        # add to list of members\n",
        "        all_models_boost.append(model_boost)\n",
        "        print('>loaded %s' % filename_boost)\n",
        "    return all_models_boost\n",
        "\n",
        "\n",
        "\n",
        "           ### Loading and predicting the boosting result\n",
        "\n",
        "n_members_boost =10\n",
        "members_boost = load_all_boosting_models(n_members_boost)\n",
        "\n",
        "print('Loaded %d models' % len(members_boost))\n",
        "\n",
        "\n",
        "# create stacked model input dataset as outputs from the ensemble\n",
        "def stacked_dataset_boost(members_boost, stackedX_test_1):\n",
        "    stackX_boost = None\n",
        "    l=0\n",
        "    for model in members_boost:\n",
        "        # make prediction\n",
        "        stackedX_test_2=stackedX_test_1[:,:,l:l+1]\n",
        "        stackedX_test_3=stackedX_test_2.reshape(stackedX_test_2.shape[0]*w,6)\n",
        "        yhat_boost = model.predict(stackedX_test_3)\n",
        "        l=l+1\n",
        "        # stack predictions into [rows, members, probabilities]\n",
        "        if stackX_boost is None:\n",
        "            stackX_boost = yhat_boost\n",
        "        else:\n",
        "            stackX_boost = np.dstack((stackX_boost, yhat_boost))\n",
        "    stackX_boost= stackX_boost.reshape((stackX_boost.shape[0], stackX_boost.shape[1],stackX_boost.shape[2]))\n",
        "    return stackX_boost\n",
        "\n",
        "        ### Fit test data on baggging ensemble ###\n",
        "\n",
        "# fit a model based on the outputs from the ensemble members\n",
        "def fit_stacked_model_test(members, inputX,inputX_lstm, inputy):\n",
        "    # create dataset using ensemble\n",
        "    from sklearn.linear_model import Ridge\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    stackedX = stacked_dataset(members, inputX,inputX_lstm)\n",
        "    return stackedX\n",
        "\n",
        "\n",
        "### Test data ###\n",
        "\n",
        "stackedX_test= fit_stacked_model_test(members, test_X_2D,test_X_1D, test_y)\n",
        "print(stackedX_test.shape)\n",
        "\n",
        "### Train data ###\n",
        "\n",
        "stackedX_train= fit_stacked_model_test(members, train_X_2D,train_X_1D, train_y_5)\n",
        "print(stackedX_train.shape)\n",
        "\n",
        "        ### Function for boosting ensemble for test data ###\n",
        "\n",
        "def fit_stacked_model_test_boost(members, stackedX_test):\n",
        "\n",
        "    stackedX_boost = stacked_dataset_boost(members_boost, stackedX_test)\n",
        "    return stackedX_boost\n",
        "\n",
        "        ### prediciton of boosting ensemble for test data ###\n",
        "\n",
        "stackedX_test_boost= fit_stacked_model_test_boost(members_boost,stackedX_test )\n",
        "# stackedX_test_boost=stackedX_test_boost.reshape(stackedX_test_boost.shape[0],stackedX_test_boost.shape[1]*stackedX_test_boost.shape[2])\n",
        "print(stackedX_test_boost.shape)\n",
        "\n",
        "\n",
        "           ### prediciton of boosting ensemble for train data ###\n",
        "\n",
        "stackedX_train_boost= fit_stacked_model_test_boost(members_boost,stackedX_train )\n",
        "# stackedX_train_boost=stackedX_train_boost.reshape(stackedX_train_boost.shape[0],stackedX_train_boost.shape[1]*stackedX_train_boost.shape[2])\n",
        "print(stackedX_train_boost.shape)\n",
        "\n",
        "\n",
        "       #### Bagging Results---test data ###\n",
        "\n",
        "\n",
        "A_1=stackedX_test[:,:,0:1]\n",
        "A_1=A_1.reshape(A_1.shape[0]*A_1.shape[1])\n",
        "\n",
        "A_2=stackedX_test[:,:,1:2]\n",
        "A_2=A_2.reshape(A_2.shape[0]*A_2.shape[1])\n",
        "\n",
        "A_3=stackedX_test[:,:,2:3]\n",
        "A_3=A_3.reshape(A_3.shape[0]*A_3.shape[1])\n",
        "\n",
        "A_4=stackedX_test[:,:,3:4]\n",
        "A_4=A_4.reshape(A_4.shape[0]*A_4.shape[1])\n",
        "\n",
        "A_5=stackedX_test[:,:,4:5]\n",
        "A_5=A_5.reshape(A_5.shape[0]*A_5.shape[1])\n",
        "\n",
        "A_6=stackedX_test[:,:,5:6]\n",
        "A_6=A_6.reshape(A_6.shape[0]*A_6.shape[1])\n",
        "\n",
        "A_7=stackedX_test[:,:,6:7]\n",
        "A_7=A_7.reshape(A_7.shape[0]*A_7.shape[1])\n",
        "\n",
        "A_8=stackedX_test[:,:,7:8]\n",
        "A_8=A_8.reshape(A_8.shape[0]*A_8.shape[1])\n",
        "\n",
        "A_9=stackedX_test[:,:,8:9]\n",
        "A_9=A_9.reshape(A_9.shape[0]*A_9.shape[1])\n",
        "\n",
        "A_10=stackedX_test[:,:,9:10]\n",
        "A_10=A_10.reshape(A_10.shape[0]*A_10.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "B_1=np.array([A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10])\n",
        "B=np.average(B_1, weights = [1,1,1,1,1,1,1,1,1,1],axis=0)\n",
        "yhat_bag_1=B.reshape(test_y.shape[0],test_y.shape[1])\n",
        "print(yhat_bag_1.shape)\n",
        "\n",
        "\n",
        "       ####Boosting Results---test data ###\n",
        "\n",
        "\n",
        "\n",
        "A_1=stackedX_test_boost[:,:,0:1]\n",
        "A_1=A_1.reshape(A_1.shape[0]*A_1.shape[1])\n",
        "\n",
        "A_2=stackedX_test_boost[:,:,1:2]\n",
        "A_2=A_2.reshape(A_2.shape[0]*A_2.shape[1])\n",
        "\n",
        "A_3=stackedX_test_boost[:,:,2:3]\n",
        "A_3=A_3.reshape(A_3.shape[0]*A_3.shape[1])\n",
        "\n",
        "A_4=stackedX_test_boost[:,:,3:4]\n",
        "A_4=A_4.reshape(A_4.shape[0]*A_4.shape[1])\n",
        "\n",
        "A_5=stackedX_test_boost[:,:,4:5]\n",
        "A_5=A_5.reshape(A_5.shape[0]*A_5.shape[1])\n",
        "\n",
        "A_6=stackedX_test_boost[:,:,5:6]\n",
        "A_6=A_6.reshape(A_6.shape[0]*A_6.shape[1])\n",
        "\n",
        "A_7=stackedX_test_boost[:,:,6:7]\n",
        "A_7=A_7.reshape(A_7.shape[0]*A_7.shape[1])\n",
        "\n",
        "A_8=stackedX_test_boost[:,:,7:8]\n",
        "A_8=A_8.reshape(A_8.shape[0]*A_8.shape[1])\n",
        "\n",
        "A_9=stackedX_test_boost[:,:,8:9]\n",
        "A_9=A_9.reshape(A_9.shape[0]*A_9.shape[1])\n",
        "\n",
        "A_10=stackedX_test_boost[:,:,9:10]\n",
        "A_10=A_10.reshape(A_10.shape[0]*A_10.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "B_1=np.array([A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10])\n",
        "B=np.average(B_1, weights = [1,1,1,1,1,1,1,1,1,1],axis=0)\n",
        "yhat_bagboost_1=B.reshape(test_y.shape[0],test_y.shape[1])\n",
        "print(yhat_bag_1.shape)\n",
        "\n",
        "    ### prediciton for base model ###\n",
        "\n",
        "\n",
        "filename_base = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/model_base.h5'\n",
        "model_base = tf.keras.models.load_model(filename_base)\n",
        "yhat_base_1=model_base.predict(test_X_2D)\n",
        "\n",
        "     ### BaseBoost Result ###\n",
        "\n",
        "X_test_bag=yhat_base_1\n",
        "X_test_bag=X_test_bag.reshape(test_X_2D.shape[0]*w,6)\n",
        "\n",
        "filename_base_boost='/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/model_Base_GB_1.pkl'\n",
        "with open(filename_base_boost, 'rb') as fp:\n",
        "            model_Baseboost = pickle.load(fp)\n",
        "\n",
        "yhat_baseboost_1=model_Baseboost.predict(X_test_bag)\n",
        "yhat_baseboost_1=yhat_baseboost_1.reshape(test_X_2D.shape[0],w*6)\n",
        "print(yhat_baseboost_1.shape)\n",
        "\n",
        "       #### Bagging Results---train data ###\n",
        "\n",
        "A_1=stackedX_train[:,:,0:1]\n",
        "A_1=A_1.reshape(A_1.shape[0]*A_1.shape[1])\n",
        "\n",
        "A_2=stackedX_train[:,:,1:2]\n",
        "A_2=A_2.reshape(A_2.shape[0]*A_2.shape[1])\n",
        "\n",
        "A_3=stackedX_train[:,:,2:3]\n",
        "A_3=A_3.reshape(A_3.shape[0]*A_3.shape[1])\n",
        "\n",
        "A_4=stackedX_train[:,:,3:4]\n",
        "A_4=A_4.reshape(A_4.shape[0]*A_4.shape[1])\n",
        "\n",
        "A_5=stackedX_train[:,:,4:5]\n",
        "A_5=A_5.reshape(A_5.shape[0]*A_5.shape[1])\n",
        "\n",
        "A_6=stackedX_train[:,:,5:6]\n",
        "A_6=A_6.reshape(A_6.shape[0]*A_6.shape[1])\n",
        "\n",
        "A_7=stackedX_train[:,:,6:7]\n",
        "A_7=A_7.reshape(A_7.shape[0]*A_7.shape[1])\n",
        "\n",
        "A_8=stackedX_train[:,:,7:8]\n",
        "A_8=A_8.reshape(A_8.shape[0]*A_8.shape[1])\n",
        "\n",
        "A_9=stackedX_train[:,:,8:9]\n",
        "A_9=A_9.reshape(A_9.shape[0]*A_9.shape[1])\n",
        "\n",
        "A_10=stackedX_train[:,:,9:10]\n",
        "A_10=A_10.reshape(A_10.shape[0]*A_10.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "B_1=np.array([A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10])\n",
        "B=np.average(B_1, weights = [1,1,1,1,1,1,1,1,1,1],axis=0)\n",
        "yhat_train_bag_1=B.reshape(train_y_5.shape[0],train_y_5.shape[1])\n",
        "print(yhat_train_bag_1.shape)\n",
        "\n",
        "\n",
        " ##### Boosting Train Results ####\n",
        "\n",
        "A_1=stackedX_train_boost[:,:,0:1]\n",
        "A_1=A_1.reshape(A_1.shape[0]*A_1.shape[1])\n",
        "\n",
        "A_2=stackedX_train_boost[:,:,1:2]\n",
        "A_2=A_2.reshape(A_2.shape[0]*A_2.shape[1])\n",
        "\n",
        "A_3=stackedX_train_boost[:,:,2:3]\n",
        "A_3=A_3.reshape(A_3.shape[0]*A_3.shape[1])\n",
        "\n",
        "A_4=stackedX_train_boost[:,:,3:4]\n",
        "A_4=A_4.reshape(A_4.shape[0]*A_4.shape[1])\n",
        "\n",
        "A_5=stackedX_train_boost[:,:,4:5]\n",
        "A_5=A_5.reshape(A_5.shape[0]*A_5.shape[1])\n",
        "\n",
        "A_6=stackedX_train_boost[:,:,5:6]\n",
        "A_6=A_6.reshape(A_6.shape[0]*A_6.shape[1])\n",
        "\n",
        "A_7=stackedX_train_boost[:,:,6:7]\n",
        "A_7=A_7.reshape(A_7.shape[0]*A_7.shape[1])\n",
        "\n",
        "A_8=stackedX_train_boost[:,:,7:8]\n",
        "A_8=A_8.reshape(A_8.shape[0]*A_8.shape[1])\n",
        "\n",
        "A_9=stackedX_train_boost[:,:,8:9]\n",
        "A_9=A_9.reshape(A_9.shape[0]*A_9.shape[1])\n",
        "\n",
        "A_10=stackedX_train_boost[:,:,9:10]\n",
        "A_10=A_10.reshape(A_10.shape[0]*A_10.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "B_1=np.array([A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10])\n",
        "B=np.average(B_1, weights = [1,1,1,1,1,1,1,1,1,1],axis=0)\n",
        "yhat_train_bagboost_1=B.reshape(train_y_5.shape[0],train_y_5.shape[1])\n",
        "print(yhat_train_bagboost_1.shape)\n",
        "\n",
        "\n",
        "### BaseBoost train results ###\n",
        "\n",
        "yhat_train_base_1=model_base.predict(train_X_2D)\n",
        "\n",
        "X_train_bag=yhat_train_base_1\n",
        "X_train_bag=X_train_bag.reshape(train_X_2D.shape[0]*w,6)\n",
        "\n",
        "yhat_train_baseboost_1=model_Baseboost.predict(X_train_bag)\n",
        "yhat_train_baseboost_1=yhat_train_baseboost_1.reshape(train_X_2D.shape[0],w*6)\n",
        "\n",
        "print(yhat_train_baseboost_1.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/bagging/model_1.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/bagging/model_2.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/bagging/model_3.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/bagging/model_4.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/bagging/model_5.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/bagging/model_6.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/bagging/model_7.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/bagging/model_8.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/bagging/model_9.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/bagging/model_10.h5\n",
            "Loaded 10 models\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/boosting/model_1.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/boosting/model_2.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/boosting/model_3.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/boosting/model_4.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/boosting/model_5.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/boosting/model_6.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/boosting/model_7.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/boosting/model_8.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/boosting/model_9.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/conv2D/boosting/model_10.pkl\n",
            "Loaded 10 models\n",
            "(676, 480, 10)\n",
            "(8852, 480, 10)\n",
            "(54080, 6, 10)\n",
            "(708160, 6, 10)\n",
            "(676, 480)\n",
            "(676, 480)\n",
            "(676, 480)\n",
            "(8852, 480)\n",
            "(8852, 480)\n",
            "(8852, 480)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9fUj-u0Wq5o"
      },
      "source": [
        "#        ### bagboost model 2 ###\n",
        "\n",
        "# # # n_models=10\n",
        "\n",
        "# # # for i in range(n_models):\n",
        "\n",
        "# #     # The shape of the input image.\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.ensemble import GradientBoostingRegressor\n",
        "# from sklearn.multioutput import MultiOutputRegressor\n",
        "# import pickle\n",
        "# from lightgbm import LGBMRegressor\n",
        "# from sklearn.linear_model import Ridge\n",
        "\n",
        "# import xgboost as xg\n",
        "\n",
        "\n",
        "# # kfold = KFold(n_splits=10, shuffle=True)\n",
        "# i=0\n",
        "# # # enumerate splits\n",
        "# # for train_ix,test_ix in kfold.split(train_X_1D):\n",
        "# #   # get data\n",
        "\n",
        "# n_splits = 10\n",
        "\n",
        "\n",
        "# for _ in range(n_splits):\n",
        "# \t# select indexes\n",
        "#   ix = [i for i in range(len(train_X_2D))]\n",
        "#   train_ix = resample(ix, replace=True, n_samples=Bag_samples)\n",
        "#   test_ix = [x for x in ix if x not in train_ix]\n",
        "#   train_X=train_X_1D[train_ix]\n",
        "#   train_y=train_y_5[train_ix]\n",
        "\n",
        "\n",
        "#   inputs = tf.keras.layers.Input( shape=(w,16) )\n",
        "#   inputs_1 =BatchNormalization()(inputs)\n",
        "#   model_1=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1)\n",
        "#   model_1=Dropout(0.5)(model_1)\n",
        "#   model_1=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)\n",
        "#   model_1=Dropout(0.5)(model_1)\n",
        "\n",
        "#   model_1=Flatten()(model_1)\n",
        "\n",
        "#   output=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)\n",
        "#   model_2 = Model(inputs, output)\n",
        "#   sgd = SGD(lr=0.001, decay=1e-5, momentum=0.9, nesterov=True)\n",
        "#   model_2.compile(loss='mean_squared_error', optimizer='Adam')\n",
        "\n",
        "#   history=model_2.fit(train_X, train_y, epochs=70, batch_size=64, validation_data=(X_validation_1D, Y_validation), verbose=2, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#   # # # summarize history for loss\n",
        "#   plt.plot(history.history['loss'])\n",
        "#   plt.plot(history.history['val_loss'])\n",
        "#   plt.title('model loss')\n",
        "#   plt.ylabel('loss')\n",
        "#   plt.xlabel('epoch')\n",
        "#   plt.legend(['train', 'test'], loc='upper right')\n",
        "#   plt.show()\n",
        "#   model_2.summary()\n",
        "\n",
        "#   filename_1 = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/bagging/model_' + str(i + 1) + '.h5'\n",
        "#   model_2.save(filename_1)\n",
        "#   print('>Saved %s' % filename_1)\n",
        "\n",
        "#   X_train_bag=model_2.predict(X_validation_1D)\n",
        "#   X_train_bag=X_train_bag.reshape(X_validation_1D.shape[0]*w,6)\n",
        "#   train_y_bag=Y_validation.reshape(X_validation_1D.shape[0]*w,6)\n",
        "\n",
        "#   model = MultiOutputRegressor(GradientBoostingRegressor(n_estimators=200,verbose=2))\n",
        "#   model.fit(X_train_bag, train_y_bag)\n",
        "\n",
        "#   filename= '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/boosting/model_' + str(i + 1) + '.pkl'\n",
        "#   with open(filename, 'wb') as file:\n",
        "#       pickle.dump(model, file)\n",
        "#   #pickle.dump(model, open(\"pima.pickle.dat\", \"wb\"))\n",
        "#   print('>Saved %s' % filename)\n",
        "#   i=i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k_AGJ42Wwt1"
      },
      "source": [
        "\n",
        "#   ### base model 2 ###\n",
        "\n",
        "\n",
        "# inputs = tf.keras.layers.Input( shape=(w,16) )\n",
        "# inputs_1 =BatchNormalization()(inputs)\n",
        "# model_1=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1)\n",
        "# model_1=Dropout(0.5)(model_1)\n",
        "# model_1=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)\n",
        "# model_1=Dropout(0.5)(model_1)\n",
        "\n",
        "# model_1=Flatten()(model_1)\n",
        "\n",
        "# output=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)\n",
        "# model_2 = Model(inputs, output)\n",
        "# sgd = SGD(lr=0.001, decay=1e-5, momentum=0.9, nesterov=True)\n",
        "# model_2.compile(loss='mean_squared_error', optimizer='Adam')\n",
        "\n",
        "# history=model_2.fit(train_X_1D, train_y_5, epochs=70, batch_size=64, validation_data=(X_validation_1D, Y_validation), verbose=2, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # # # summarize history for loss\n",
        "# plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['val_loss'])\n",
        "# plt.title('model loss')\n",
        "# plt.ylabel('loss')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train', 'test'], loc='upper right')\n",
        "# plt.show()\n",
        "# model_2.summary()\n",
        "\n",
        "# filename = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/model_base.h5'\n",
        "# model_2.save(filename)\n",
        "# print('>Saved %s' % filename)\n",
        "\n",
        "\n",
        "\n",
        "# X_train_bag=model_2.predict(X_validation_1D)\n",
        "# X_train_bag=X_train_bag.reshape(X_validation_1D.shape[0]*w,6)\n",
        "# train_y_bag=Y_validation.reshape(X_validation_1D.shape[0]*w,6)\n",
        "\n",
        "\n",
        "# model = MultiOutputRegressor(GradientBoostingRegressor(n_estimators=200,verbose=2))\n",
        "# model.fit(X_train_bag, train_y_bag)\n",
        "\n",
        "# filename_1= '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/model_Base_GB_1.pkl'\n",
        "# with open(filename_1, 'wb') as file:\n",
        "#     pickle.dump(model, file)\n",
        "# print('>Saved %s' % filename_1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoyfrNafc8Lm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e85e8a74-15da-4df3-ef0e-5c0b0ccf7138"
      },
      "source": [
        "# load models from file\n",
        "def load_all_models(n_models):\n",
        "    all_models = list()\n",
        "    for i in range(n_models):\n",
        "        # define filename for this ensemble\n",
        "        filename = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/bagging/model_' + str(i + 1) + '.h5'\n",
        "        # load model from file\n",
        "        model = tf.keras.models.load_model(filename)\n",
        "        # add to list of members\n",
        "        all_models.append(model)\n",
        "        print('>loaded %s' % filename)\n",
        "    return all_models\n",
        "\n",
        "\n",
        "\n",
        "       ### Load model and do prediction ###\n",
        "\n",
        "n_members =10\n",
        "members = load_all_models(n_members)\n",
        "\n",
        "print('Loaded %d models' % len(members))\n",
        "\n",
        "\n",
        "# create stacked model input dataset as outputs from the ensemble\n",
        "def stacked_dataset(members, inputX, inputX_lstm):\n",
        "    stackX = None\n",
        "    l=1\n",
        "    for model in members:\n",
        "        # make prediction\n",
        "        yhat = model.predict(inputX_lstm, verbose=0)\n",
        "        # stack predictions into [rows, members, probabilities]\n",
        "        if stackX is None:\n",
        "            stackX = yhat\n",
        "        else:\n",
        "            stackX = np.dstack((stackX, yhat))\n",
        "    # flatten predictions to [rows, members x probabilities]\n",
        "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1],stackX.shape[2]))\n",
        "    return stackX\n",
        "\n",
        "             ### Function for boosting model ###\n",
        "\n",
        "\n",
        "#  models from file\n",
        "def load_all_boosting_models(n_models):\n",
        "    all_models_boost = list()\n",
        "    for i in range(n_members_boost):\n",
        "        # define filename for this ensemble\n",
        "        filename_boost = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/boosting/model_' + str(i + 1) + '.pkl'\n",
        "        # load model from file\n",
        "        with open(filename_boost, 'rb') as fp:\n",
        "            model_boost = pickle.load(fp)\n",
        "        # add to list of members\n",
        "        all_models_boost.append(model_boost)\n",
        "        print('>loaded %s' % filename_boost)\n",
        "    return all_models_boost\n",
        "\n",
        "\n",
        "\n",
        "           ### Loading and predicting the boosting result\n",
        "\n",
        "n_members_boost =10\n",
        "members_boost = load_all_boosting_models(n_members_boost)\n",
        "\n",
        "print('Loaded %d models' % len(members_boost))\n",
        "\n",
        "\n",
        "# create stacked model input dataset as outputs from the ensemble\n",
        "def stacked_dataset_boost(members_boost, stackedX_test_1):\n",
        "    stackX_boost = None\n",
        "    l=0\n",
        "    for model in members_boost:\n",
        "        # make prediction\n",
        "        stackedX_test_2=stackedX_test_1[:,:,l:l+1]\n",
        "        stackedX_test_3=stackedX_test_2.reshape(stackedX_test_2.shape[0]*w,6)\n",
        "        yhat_boost = model.predict(stackedX_test_3)\n",
        "        l=l+1\n",
        "        # stack predictions into [rows, members, probabilities]\n",
        "        if stackX_boost is None:\n",
        "            stackX_boost = yhat_boost\n",
        "        else:\n",
        "            stackX_boost = np.dstack((stackX_boost, yhat_boost))\n",
        "    stackX_boost= stackX_boost.reshape((stackX_boost.shape[0], stackX_boost.shape[1],stackX_boost.shape[2]))\n",
        "    return stackX_boost\n",
        "\n",
        "        ### Fit test data on baggging ensemble ###\n",
        "\n",
        "# fit a model based on the outputs from the ensemble members\n",
        "def fit_stacked_model_test(members, inputX,inputX_lstm, inputy):\n",
        "    # create dataset using ensemble\n",
        "    from sklearn.linear_model import Ridge\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    stackedX = stacked_dataset(members, inputX,inputX_lstm)\n",
        "    return stackedX\n",
        "\n",
        "\n",
        "### Test data ###\n",
        "\n",
        "stackedX_test= fit_stacked_model_test(members, test_X_2D,test_X_1D, test_y)\n",
        "print(stackedX_test.shape)\n",
        "\n",
        "### Train data ###\n",
        "\n",
        "stackedX_train= fit_stacked_model_test(members, train_X_2D,train_X_1D, train_y_5)\n",
        "print(stackedX_train.shape)\n",
        "\n",
        "        ### Function for boosting ensemble for test data ###\n",
        "\n",
        "def fit_stacked_model_test_boost(members, stackedX_test):\n",
        "\n",
        "    stackedX_boost = stacked_dataset_boost(members_boost, stackedX_test)\n",
        "    return stackedX_boost\n",
        "\n",
        "        ### prediciton of boosting ensemble for test data ###\n",
        "\n",
        "stackedX_test_boost= fit_stacked_model_test_boost(members_boost,stackedX_test )\n",
        "print(stackedX_test_boost.shape)\n",
        "\n",
        "\n",
        "           ### prediciton of boosting ensemble for train data ###\n",
        "\n",
        "stackedX_train_boost= fit_stacked_model_test_boost(members_boost,stackedX_train )\n",
        "print(stackedX_train_boost.shape)\n",
        "\n",
        "\n",
        "       #### Bagging Results ###\n",
        "\n",
        "\n",
        "A_1=stackedX_test[:,:,0:1]\n",
        "A_1=A_1.reshape(A_1.shape[0]*A_1.shape[1])\n",
        "\n",
        "A_2=stackedX_test[:,:,1:2]\n",
        "A_2=A_2.reshape(A_2.shape[0]*A_2.shape[1])\n",
        "\n",
        "A_3=stackedX_test[:,:,2:3]\n",
        "A_3=A_3.reshape(A_3.shape[0]*A_3.shape[1])\n",
        "\n",
        "A_4=stackedX_test[:,:,3:4]\n",
        "A_4=A_4.reshape(A_4.shape[0]*A_4.shape[1])\n",
        "\n",
        "A_5=stackedX_test[:,:,4:5]\n",
        "A_5=A_5.reshape(A_5.shape[0]*A_5.shape[1])\n",
        "\n",
        "A_6=stackedX_test[:,:,5:6]\n",
        "A_6=A_6.reshape(A_6.shape[0]*A_6.shape[1])\n",
        "\n",
        "A_7=stackedX_test[:,:,6:7]\n",
        "A_7=A_7.reshape(A_7.shape[0]*A_7.shape[1])\n",
        "\n",
        "A_8=stackedX_test[:,:,7:8]\n",
        "A_8=A_8.reshape(A_8.shape[0]*A_8.shape[1])\n",
        "\n",
        "A_9=stackedX_test[:,:,8:9]\n",
        "A_9=A_9.reshape(A_9.shape[0]*A_9.shape[1])\n",
        "\n",
        "A_10=stackedX_test[:,:,9:10]\n",
        "A_10=A_10.reshape(A_10.shape[0]*A_10.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "B_1=np.array([A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10])\n",
        "B=np.average(B_1, weights = [1,1,1,1,1,1,1,1,1,1],axis=0)\n",
        "yhat_bag_2=B.reshape(test_y.shape[0],test_y.shape[1])\n",
        "print(yhat_bag_2.shape)\n",
        "\n",
        "       ####Boosting Results---test data ###\n",
        "\n",
        "\n",
        "\n",
        "A_1=stackedX_test_boost[:,:,0:1]\n",
        "A_1=A_1.reshape(A_1.shape[0]*A_1.shape[1])\n",
        "\n",
        "A_2=stackedX_test_boost[:,:,1:2]\n",
        "A_2=A_2.reshape(A_2.shape[0]*A_2.shape[1])\n",
        "\n",
        "A_3=stackedX_test_boost[:,:,2:3]\n",
        "A_3=A_3.reshape(A_3.shape[0]*A_3.shape[1])\n",
        "\n",
        "A_4=stackedX_test_boost[:,:,3:4]\n",
        "A_4=A_4.reshape(A_4.shape[0]*A_4.shape[1])\n",
        "\n",
        "A_5=stackedX_test_boost[:,:,4:5]\n",
        "A_5=A_5.reshape(A_5.shape[0]*A_5.shape[1])\n",
        "\n",
        "A_6=stackedX_test_boost[:,:,5:6]\n",
        "A_6=A_6.reshape(A_6.shape[0]*A_6.shape[1])\n",
        "\n",
        "A_7=stackedX_test_boost[:,:,6:7]\n",
        "A_7=A_7.reshape(A_7.shape[0]*A_7.shape[1])\n",
        "\n",
        "A_8=stackedX_test_boost[:,:,7:8]\n",
        "A_8=A_8.reshape(A_8.shape[0]*A_8.shape[1])\n",
        "\n",
        "A_9=stackedX_test_boost[:,:,8:9]\n",
        "A_9=A_9.reshape(A_9.shape[0]*A_9.shape[1])\n",
        "\n",
        "A_10=stackedX_test_boost[:,:,9:10]\n",
        "A_10=A_10.reshape(A_10.shape[0]*A_10.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "B_1=np.array([A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10])\n",
        "B=np.average(B_1, weights = [1,1,1,1,1,1,1,1,1,1],axis=0)\n",
        "yhat_bagboost_2=B.reshape(test_y.shape[0],test_y.shape[1])\n",
        "print(yhat_bagboost_2.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### prediciton for base model ###\n",
        "\n",
        "\n",
        "filename_base = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/model_base.h5'\n",
        "model_base = tf.keras.models.load_model(filename_base)\n",
        "yhat_base_2=model_base.predict(test_X_1D)\n",
        "\n",
        "     ### BaseBoost Result ###\n",
        "\n",
        "X_test_bag=yhat_base_2\n",
        "X_test_bag=X_test_bag.reshape(test_X_1D.shape[0]*w,6)\n",
        "\n",
        "filename_base_boost='/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/model_Base_GB_1.pkl'\n",
        "with open(filename_base_boost, 'rb') as fp:\n",
        "            model_Baseboost = pickle.load(fp)\n",
        "\n",
        "yhat_baseboost_2=model_Baseboost.predict(X_test_bag)\n",
        "yhat_baseboost_2=yhat_baseboost_2.reshape(test_X_2D.shape[0],w*6)\n",
        "print(yhat_baseboost_2.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "       #### Bagging Results---train data ###\n",
        "\n",
        "A_1=stackedX_train[:,:,0:1]\n",
        "A_1=A_1.reshape(A_1.shape[0]*A_1.shape[1])\n",
        "\n",
        "A_2=stackedX_train[:,:,1:2]\n",
        "A_2=A_2.reshape(A_2.shape[0]*A_2.shape[1])\n",
        "\n",
        "A_3=stackedX_train[:,:,2:3]\n",
        "A_3=A_3.reshape(A_3.shape[0]*A_3.shape[1])\n",
        "\n",
        "A_4=stackedX_train[:,:,3:4]\n",
        "A_4=A_4.reshape(A_4.shape[0]*A_4.shape[1])\n",
        "\n",
        "A_5=stackedX_train[:,:,4:5]\n",
        "A_5=A_5.reshape(A_5.shape[0]*A_5.shape[1])\n",
        "\n",
        "A_6=stackedX_train[:,:,5:6]\n",
        "A_6=A_6.reshape(A_6.shape[0]*A_6.shape[1])\n",
        "\n",
        "A_7=stackedX_train[:,:,6:7]\n",
        "A_7=A_7.reshape(A_7.shape[0]*A_7.shape[1])\n",
        "\n",
        "A_8=stackedX_train[:,:,7:8]\n",
        "A_8=A_8.reshape(A_8.shape[0]*A_8.shape[1])\n",
        "\n",
        "A_9=stackedX_train[:,:,8:9]\n",
        "A_9=A_9.reshape(A_9.shape[0]*A_9.shape[1])\n",
        "\n",
        "A_10=stackedX_train[:,:,9:10]\n",
        "A_10=A_10.reshape(A_10.shape[0]*A_10.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "B_1=np.array([A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10])\n",
        "B=np.average(B_1, weights = [1,1,1,1,1,1,1,1,1,1],axis=0)\n",
        "yhat_train_bag_2=B.reshape(train_y_5.shape[0],train_y_5.shape[1])\n",
        "print(yhat_train_bag_2.shape)\n",
        "\n",
        " ##### Boosting Train Results ####\n",
        "\n",
        "A_1=stackedX_train_boost[:,:,0:1]\n",
        "A_1=A_1.reshape(A_1.shape[0]*A_1.shape[1])\n",
        "\n",
        "A_2=stackedX_train_boost[:,:,1:2]\n",
        "A_2=A_2.reshape(A_2.shape[0]*A_2.shape[1])\n",
        "\n",
        "A_3=stackedX_train_boost[:,:,2:3]\n",
        "A_3=A_3.reshape(A_3.shape[0]*A_3.shape[1])\n",
        "\n",
        "A_4=stackedX_train_boost[:,:,3:4]\n",
        "A_4=A_4.reshape(A_4.shape[0]*A_4.shape[1])\n",
        "\n",
        "A_5=stackedX_train_boost[:,:,4:5]\n",
        "A_5=A_5.reshape(A_5.shape[0]*A_5.shape[1])\n",
        "\n",
        "A_6=stackedX_train_boost[:,:,5:6]\n",
        "A_6=A_6.reshape(A_6.shape[0]*A_6.shape[1])\n",
        "\n",
        "A_7=stackedX_train_boost[:,:,6:7]\n",
        "A_7=A_7.reshape(A_7.shape[0]*A_7.shape[1])\n",
        "\n",
        "A_8=stackedX_train_boost[:,:,7:8]\n",
        "A_8=A_8.reshape(A_8.shape[0]*A_8.shape[1])\n",
        "\n",
        "A_9=stackedX_train_boost[:,:,8:9]\n",
        "A_9=A_9.reshape(A_9.shape[0]*A_9.shape[1])\n",
        "\n",
        "A_10=stackedX_train_boost[:,:,9:10]\n",
        "A_10=A_10.reshape(A_10.shape[0]*A_10.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "B_1=np.array([A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10])\n",
        "B=np.average(B_1, weights = [1,1,1,1,1,1,1,1,1,1],axis=0)\n",
        "yhat_train_bagboost_2=B.reshape(train_y_5.shape[0],train_y_5.shape[1])\n",
        "print(yhat_train_bagboost_2.shape)\n",
        "\n",
        "\n",
        "\n",
        "### BaseBoost train results ###\n",
        "\n",
        "yhat_train_base_2=model_base.predict(train_X_1D)\n",
        "\n",
        "X_train_bag=yhat_train_base_2\n",
        "X_train_bag=X_train_bag.reshape(train_X_2D.shape[0]*w,6)\n",
        "\n",
        "yhat_train_baseboost_2=model_Baseboost.predict(X_train_bag)\n",
        "yhat_train_baseboost_2=yhat_train_baseboost_2.reshape(train_X_2D.shape[0],w*6)\n",
        "\n",
        "print(yhat_train_baseboost_2.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/bagging/model_1.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/bagging/model_2.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/bagging/model_3.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/bagging/model_4.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/bagging/model_5.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/bagging/model_6.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/bagging/model_7.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/bagging/model_8.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/bagging/model_9.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/bagging/model_10.h5\n",
            "Loaded 10 models\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/boosting/model_1.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/boosting/model_2.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/boosting/model_3.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/boosting/model_4.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/boosting/model_5.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/boosting/model_6.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/boosting/model_7.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/boosting/model_8.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/boosting/model_9.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM/boosting/model_10.pkl\n",
            "Loaded 10 models\n",
            "(676, 480, 10)\n",
            "(8852, 480, 10)\n",
            "(54080, 6, 10)\n",
            "(708160, 6, 10)\n",
            "(676, 480)\n",
            "(676, 480)\n",
            "(676, 480)\n",
            "(8852, 480)\n",
            "(8852, 480)\n",
            "(8852, 480)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABT1HwbyW1n-"
      },
      "source": [
        "\n",
        "# #### Bagboost Model 3 ####\n",
        "\n",
        "\n",
        "\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.ensemble import GradientBoostingRegressor\n",
        "# from sklearn.multioutput import MultiOutputRegressor\n",
        "# import pickle\n",
        "# from lightgbm import LGBMRegressor\n",
        "# from sklearn.linear_model import Ridge\n",
        "# import xgboost as xg\n",
        "\n",
        "\n",
        "# i=0\n",
        "# n_splits = 10\n",
        "\n",
        "\n",
        "# for _ in range(n_splits):\n",
        "# \t# select indexes\n",
        "#   ix = [i for i in range(len(train_X_2D))]\n",
        "#   train_ix = resample(ix, replace=True, n_samples=Bag_samples)\n",
        "#   test_ix = [x for x in ix if x not in train_ix]\n",
        "#   train_X=train_X_1D[train_ix]\n",
        "#   train_y=train_y_5[train_ix]\n",
        "\n",
        "#   inputs = tf.keras.layers.Input( shape=(80,16) )\n",
        "\n",
        "#   inputs_1 =BatchNormalization()(inputs)\n",
        "\n",
        "#   model_1=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1)\n",
        "#   model_1=Dropout(0.5)(model_1)\n",
        "#   model_1=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)\n",
        "#   model_1=Dropout(0.5)(model_1)\n",
        "\n",
        "#   CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(model_1)\n",
        "#   CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "#   CNN=BatchNormalization()(CNN)\n",
        "#   CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "#   CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "#   CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "#   CNN=BatchNormalization()(CNN)\n",
        "#   CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "#   CNN=Dense(512, activation='relu')(CNN)\n",
        "#   CNN=Dropout(0.5)(CNN)\n",
        "#   CNN=Dense(256, activation='relu')(CNN)\n",
        "#   CNN=Dropout(0.5)(CNN)\n",
        "#   CNN=Flatten()(CNN)\n",
        "\n",
        "\n",
        "#   output=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "#   model_1 = Model(inputs, output)\n",
        "#   model_1.compile(loss='mean_squared_error', optimizer='Adam')\n",
        "\n",
        "#   history=model_1.fit(train_X, train_y, epochs=200, batch_size=64, validation_data=(X_validation_1D, Y_validation), verbose=2, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "#   # # # summarize history for loss\n",
        "#   plt.plot(history.history['loss'])\n",
        "#   plt.plot(history.history['val_loss'])\n",
        "#   plt.title('model loss')\n",
        "#   plt.ylabel('loss')\n",
        "#   plt.xlabel('epoch')\n",
        "#   plt.legend(['train', 'test'], loc='upper right')\n",
        "#   plt.show()\n",
        "#   model_1.summary()\n",
        "\n",
        "#   filename_1 = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/bagging/model_' + str(i + 1) + '.h5'\n",
        "#   model_1.save(filename_1)\n",
        "#   print('>Saved %s' % filename_1)\n",
        "\n",
        "\n",
        "\n",
        "#   X_train_bag=model_1.predict(X_validation_1D)\n",
        "#   X_train_bag=X_train_bag.reshape(X_validation_1D.shape[0]*w,6)\n",
        "#   train_y_bag=Y_validation.reshape(X_validation_1D.shape[0]*w,6)\n",
        "\n",
        "#   model = MultiOutputRegressor(GradientBoostingRegressor(n_estimators=200,verbose=2))\n",
        "#   model.fit(X_train_bag, train_y_bag)\n",
        "\n",
        "#   filename= '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/boosting/model_' + str(i + 1) + '.pkl'\n",
        "#   with open(filename, 'wb') as file:\n",
        "#       pickle.dump(model, file)\n",
        "#   print('>Saved %s' % filename)\n",
        "#   i=i+1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlPqVFBgW3u2"
      },
      "source": [
        "#    ### Base model 3 ###\n",
        "\n",
        "# inputs = tf.keras.layers.Input( shape=(80,16) )\n",
        "\n",
        "# inputs_1 =BatchNormalization()(inputs)\n",
        "\n",
        "# model_1=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1)\n",
        "# model_1=Dropout(0.5)(model_1)\n",
        "# model_1=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)\n",
        "# model_1=Dropout(0.5)(model_1)\n",
        "\n",
        "# CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(model_1)\n",
        "# CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "# CNN=BatchNormalization()(CNN)\n",
        "# CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "# CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "# CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "# CNN=BatchNormalization()(CNN)\n",
        "# CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "# CNN=Dense(512, activation='relu')(CNN)\n",
        "# CNN=Dropout(0.5)(CNN)\n",
        "# CNN=Dense(256, activation='relu')(CNN)\n",
        "# CNN=Dropout(0.5)(CNN)\n",
        "# CNN=Flatten()(CNN)\n",
        "\n",
        "\n",
        "# output=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "# model_1 = Model(inputs, output)\n",
        "# model_1.compile(loss='mean_squared_error', optimizer='Adam')\n",
        "\n",
        "# history=model_1.fit(train_X_1D, train_y_5, epochs=200, batch_size=64, validation_data=(X_validation_1D, Y_validation), verbose=2, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "# # # # summarize history for loss\n",
        "# plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['val_loss'])\n",
        "# plt.title('model loss')\n",
        "# plt.ylabel('loss')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train', 'test'], loc='upper right')\n",
        "# plt.show()\n",
        "# model_1.summary()\n",
        "\n",
        "# filename = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/model_base.h5'\n",
        "# model_1.save(filename)\n",
        "# print('>Saved %s' % filename)\n",
        "\n",
        "\n",
        "# X_train_bag=model_1.predict(X_validation_1D)\n",
        "# X_train_bag=X_train_bag.reshape(X_validation_1D.shape[0]*w,6)\n",
        "# train_y_bag=Y_validation.reshape(X_validation_1D.shape[0]*w,6)\n",
        "\n",
        "\n",
        "# model = MultiOutputRegressor(GradientBoostingRegressor(n_estimators=200,verbose=2))\n",
        "# model.fit(X_train_bag, train_y_bag)\n",
        "\n",
        "# filename_1= '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/model_Base_GB_1.pkl'\n",
        "# with open(filename_1, 'wb') as file:\n",
        "#     pickle.dump(model, file)\n",
        "# print('>Saved %s' % filename_1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApI1azjvqdmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb486257-d00e-4489-c35d-b86b0b521a6a"
      },
      "source": [
        "# load models from file\n",
        "def load_all_models(n_models):\n",
        "    all_models = list()\n",
        "    for i in range(n_models):\n",
        "        # define filename for this ensemble\n",
        "        filename = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/bagging/model_' + str(i + 1) + '.h5'\n",
        "        # load model from file\n",
        "        model = tf.keras.models.load_model(filename)\n",
        "        # add to list of members\n",
        "        all_models.append(model)\n",
        "        print('>loaded %s' % filename)\n",
        "    return all_models\n",
        "\n",
        "\n",
        "\n",
        "       ### Load model and do prediction ###\n",
        "\n",
        "n_members =10\n",
        "members = load_all_models(n_members)\n",
        "\n",
        "print('Loaded %d models' % len(members))\n",
        "\n",
        "\n",
        "# create stacked model input dataset as outputs from the ensemble\n",
        "def stacked_dataset(members, inputX, inputX_lstm):\n",
        "    stackX = None\n",
        "    l=1\n",
        "    for model in members:\n",
        "        # make prediction\n",
        "        yhat = model.predict(inputX_lstm, verbose=0)\n",
        "        # stack predictions into [rows, members, probabilities]\n",
        "        if stackX is None:\n",
        "            stackX = yhat\n",
        "        else:\n",
        "            stackX = np.dstack((stackX, yhat))\n",
        "    # flatten predictions to [rows, members x probabilities]\n",
        "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1],stackX.shape[2]))\n",
        "    return stackX\n",
        "\n",
        "             ### Function for boosting model ###\n",
        "\n",
        "\n",
        "#  models from file\n",
        "def load_all_boosting_models(n_models):\n",
        "    all_models_boost = list()\n",
        "    for i in range(n_members_boost):\n",
        "        # define filename for this ensemble\n",
        "        filename_boost = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/boosting/model_' + str(i + 1) + '.pkl'\n",
        "        # load model from file\n",
        "        with open(filename_boost, 'rb') as fp:\n",
        "            model_boost = pickle.load(fp)\n",
        "        # add to list of members\n",
        "        all_models_boost.append(model_boost)\n",
        "        print('>loaded %s' % filename_boost)\n",
        "    return all_models_boost\n",
        "\n",
        "\n",
        "\n",
        "           ### Loading and predicting the boosting result\n",
        "\n",
        "n_members_boost =10\n",
        "members_boost = load_all_boosting_models(n_members_boost)\n",
        "\n",
        "print('Loaded %d models' % len(members_boost))\n",
        "\n",
        "\n",
        "# create stacked model input dataset as outputs from the ensemble\n",
        "def stacked_dataset_boost(members_boost, stackedX_test_1):\n",
        "    stackX_boost = None\n",
        "    l=0\n",
        "    for model in members_boost:\n",
        "        # make prediction\n",
        "        stackedX_test_2=stackedX_test_1[:,:,l:l+1]\n",
        "        stackedX_test_3=stackedX_test_2.reshape(stackedX_test_2.shape[0]*w,6)\n",
        "        yhat_boost = model.predict(stackedX_test_3)\n",
        "        l=l+1\n",
        "        # stack predictions into [rows, members, probabilities]\n",
        "        if stackX_boost is None:\n",
        "            stackX_boost = yhat_boost\n",
        "        else:\n",
        "            stackX_boost = np.dstack((stackX_boost, yhat_boost))\n",
        "    stackX_boost= stackX_boost.reshape((stackX_boost.shape[0], stackX_boost.shape[1],stackX_boost.shape[2]))\n",
        "    return stackX_boost\n",
        "\n",
        "        ### Fit test data on baggging ensemble ###\n",
        "\n",
        "# fit a model based on the outputs from the ensemble members\n",
        "def fit_stacked_model_test(members, inputX,inputX_lstm, inputy):\n",
        "    # create dataset using ensemble\n",
        "    from sklearn.linear_model import Ridge\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    stackedX = stacked_dataset(members, inputX,inputX_lstm)\n",
        "    return stackedX\n",
        "\n",
        "\n",
        "### Test data ###\n",
        "\n",
        "stackedX_test= fit_stacked_model_test(members, test_X_2D,test_X_1D, test_y)\n",
        "print(stackedX_test.shape)\n",
        "\n",
        "### Train data ###\n",
        "\n",
        "stackedX_train= fit_stacked_model_test(members, train_X_2D,train_X_1D, train_y_5)\n",
        "print(stackedX_train.shape)\n",
        "\n",
        "        ### Function for boosting ensemble for test data ###\n",
        "\n",
        "def fit_stacked_model_test_boost(members, stackedX_test):\n",
        "\n",
        "    stackedX_boost = stacked_dataset_boost(members_boost, stackedX_test)\n",
        "    return stackedX_boost\n",
        "\n",
        "        ### prediciton of boosting ensemble for test data ###\n",
        "\n",
        "stackedX_test_boost= fit_stacked_model_test_boost(members_boost,stackedX_test )\n",
        "print(stackedX_test_boost.shape)\n",
        "\n",
        "\n",
        "           ### prediciton of boosting ensemble for train data ###\n",
        "\n",
        "stackedX_train_boost= fit_stacked_model_test_boost(members_boost,stackedX_train )\n",
        "print(stackedX_train_boost.shape)\n",
        "\n",
        "\n",
        "\n",
        "       #### Bagging Results ###\n",
        "\n",
        "\n",
        "A_1=stackedX_test[:,:,0:1]\n",
        "A_1=A_1.reshape(A_1.shape[0]*A_1.shape[1])\n",
        "\n",
        "A_2=stackedX_test[:,:,1:2]\n",
        "A_2=A_2.reshape(A_2.shape[0]*A_2.shape[1])\n",
        "\n",
        "A_3=stackedX_test[:,:,2:3]\n",
        "A_3=A_3.reshape(A_3.shape[0]*A_3.shape[1])\n",
        "\n",
        "A_4=stackedX_test[:,:,3:4]\n",
        "A_4=A_4.reshape(A_4.shape[0]*A_4.shape[1])\n",
        "\n",
        "A_5=stackedX_test[:,:,4:5]\n",
        "A_5=A_5.reshape(A_5.shape[0]*A_5.shape[1])\n",
        "\n",
        "A_6=stackedX_test[:,:,5:6]\n",
        "A_6=A_6.reshape(A_6.shape[0]*A_6.shape[1])\n",
        "\n",
        "A_7=stackedX_test[:,:,6:7]\n",
        "A_7=A_7.reshape(A_7.shape[0]*A_7.shape[1])\n",
        "\n",
        "A_8=stackedX_test[:,:,7:8]\n",
        "A_8=A_8.reshape(A_8.shape[0]*A_8.shape[1])\n",
        "\n",
        "A_9=stackedX_test[:,:,8:9]\n",
        "A_9=A_9.reshape(A_9.shape[0]*A_9.shape[1])\n",
        "\n",
        "A_10=stackedX_test[:,:,9:10]\n",
        "A_10=A_10.reshape(A_10.shape[0]*A_10.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "B_1=np.array([A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10])\n",
        "B=np.average(B_1, weights = [1,1,1,1,1,1,1,1,1,1],axis=0)\n",
        "yhat_bag_3=B.reshape(test_y.shape[0],test_y.shape[1])\n",
        "print(yhat_bag_3.shape)\n",
        "\n",
        "\n",
        "\n",
        "       ####Boosting Results---test data ###\n",
        "\n",
        "\n",
        "\n",
        "A_1=stackedX_test_boost[:,:,0:1]\n",
        "A_1=A_1.reshape(A_1.shape[0]*A_1.shape[1])\n",
        "\n",
        "A_2=stackedX_test_boost[:,:,1:2]\n",
        "A_2=A_2.reshape(A_2.shape[0]*A_2.shape[1])\n",
        "\n",
        "A_3=stackedX_test_boost[:,:,2:3]\n",
        "A_3=A_3.reshape(A_3.shape[0]*A_3.shape[1])\n",
        "\n",
        "A_4=stackedX_test_boost[:,:,3:4]\n",
        "A_4=A_4.reshape(A_4.shape[0]*A_4.shape[1])\n",
        "\n",
        "A_5=stackedX_test_boost[:,:,4:5]\n",
        "A_5=A_5.reshape(A_5.shape[0]*A_5.shape[1])\n",
        "\n",
        "A_6=stackedX_test_boost[:,:,5:6]\n",
        "A_6=A_6.reshape(A_6.shape[0]*A_6.shape[1])\n",
        "\n",
        "A_7=stackedX_test_boost[:,:,6:7]\n",
        "A_7=A_7.reshape(A_7.shape[0]*A_7.shape[1])\n",
        "\n",
        "A_8=stackedX_test_boost[:,:,7:8]\n",
        "A_8=A_8.reshape(A_8.shape[0]*A_8.shape[1])\n",
        "\n",
        "A_9=stackedX_test_boost[:,:,8:9]\n",
        "A_9=A_9.reshape(A_9.shape[0]*A_9.shape[1])\n",
        "\n",
        "A_10=stackedX_test_boost[:,:,9:10]\n",
        "A_10=A_10.reshape(A_10.shape[0]*A_10.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "B_1=np.array([A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10])\n",
        "B=np.average(B_1, weights = [1,1,1,1,1,1,1,1,1,1],axis=0)\n",
        "yhat_bagboost_3=B.reshape(test_y.shape[0],test_y.shape[1])\n",
        "print(yhat_bagboost_3.shape)\n",
        "\n",
        "\n",
        "    ### prediciton for base model ###\n",
        "\n",
        "\n",
        "filename_base = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/model_base.h5'\n",
        "model_base = tf.keras.models.load_model(filename_base)\n",
        "yhat_base_3=model_base.predict(test_X_1D)\n",
        "\n",
        "     ### BaseBoost Result ###\n",
        "\n",
        "X_test_bag=yhat_base_3\n",
        "X_test_bag=X_test_bag.reshape(test_X_1D.shape[0]*w,6)\n",
        "\n",
        "filename_base_boost='/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/model_Base_GB_1.pkl'\n",
        "with open(filename_base_boost, 'rb') as fp:\n",
        "            model_Baseboost = pickle.load(fp)\n",
        "\n",
        "yhat_baseboost_3=model_Baseboost.predict(X_test_bag)\n",
        "yhat_baseboost_3=yhat_baseboost_3.reshape(test_X_2D.shape[0],w*6)\n",
        "print(yhat_baseboost_3.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "       #### Bagging Results---train data ###\n",
        "\n",
        "A_1=stackedX_train[:,:,0:1]\n",
        "A_1=A_1.reshape(A_1.shape[0]*A_1.shape[1])\n",
        "\n",
        "A_2=stackedX_train[:,:,1:2]\n",
        "A_2=A_2.reshape(A_2.shape[0]*A_2.shape[1])\n",
        "\n",
        "A_3=stackedX_train[:,:,2:3]\n",
        "A_3=A_3.reshape(A_3.shape[0]*A_3.shape[1])\n",
        "\n",
        "A_4=stackedX_train[:,:,3:4]\n",
        "A_4=A_4.reshape(A_4.shape[0]*A_4.shape[1])\n",
        "\n",
        "A_5=stackedX_train[:,:,4:5]\n",
        "A_5=A_5.reshape(A_5.shape[0]*A_5.shape[1])\n",
        "\n",
        "A_6=stackedX_train[:,:,5:6]\n",
        "A_6=A_6.reshape(A_6.shape[0]*A_6.shape[1])\n",
        "\n",
        "A_7=stackedX_train[:,:,6:7]\n",
        "A_7=A_7.reshape(A_7.shape[0]*A_7.shape[1])\n",
        "\n",
        "A_8=stackedX_train[:,:,7:8]\n",
        "A_8=A_8.reshape(A_8.shape[0]*A_8.shape[1])\n",
        "\n",
        "A_9=stackedX_train[:,:,8:9]\n",
        "A_9=A_9.reshape(A_9.shape[0]*A_9.shape[1])\n",
        "\n",
        "A_10=stackedX_train[:,:,9:10]\n",
        "A_10=A_10.reshape(A_10.shape[0]*A_10.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "B_1=np.array([A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10])\n",
        "B=np.average(B_1, weights = [1,1,1,1,1,1,1,1,1,1],axis=0)\n",
        "yhat_train_bag_3=B.reshape(train_y_5.shape[0],train_y_5.shape[1])\n",
        "print(yhat_train_bag_3.shape)\n",
        "\n",
        " ##### Boosting Train Results ####\n",
        "\n",
        "A_1=stackedX_train_boost[:,:,0:1]\n",
        "A_1=A_1.reshape(A_1.shape[0]*A_1.shape[1])\n",
        "\n",
        "A_2=stackedX_train_boost[:,:,1:2]\n",
        "A_2=A_2.reshape(A_2.shape[0]*A_2.shape[1])\n",
        "\n",
        "A_3=stackedX_train_boost[:,:,2:3]\n",
        "A_3=A_3.reshape(A_3.shape[0]*A_3.shape[1])\n",
        "\n",
        "A_4=stackedX_train_boost[:,:,3:4]\n",
        "A_4=A_4.reshape(A_4.shape[0]*A_4.shape[1])\n",
        "\n",
        "A_5=stackedX_train_boost[:,:,4:5]\n",
        "A_5=A_5.reshape(A_5.shape[0]*A_5.shape[1])\n",
        "\n",
        "A_6=stackedX_train_boost[:,:,5:6]\n",
        "A_6=A_6.reshape(A_6.shape[0]*A_6.shape[1])\n",
        "\n",
        "A_7=stackedX_train_boost[:,:,6:7]\n",
        "A_7=A_7.reshape(A_7.shape[0]*A_7.shape[1])\n",
        "\n",
        "A_8=stackedX_train_boost[:,:,7:8]\n",
        "A_8=A_8.reshape(A_8.shape[0]*A_8.shape[1])\n",
        "\n",
        "A_9=stackedX_train_boost[:,:,8:9]\n",
        "A_9=A_9.reshape(A_9.shape[0]*A_9.shape[1])\n",
        "\n",
        "A_10=stackedX_train_boost[:,:,9:10]\n",
        "A_10=A_10.reshape(A_10.shape[0]*A_10.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "B_1=np.array([A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10])\n",
        "B=np.average(B_1, weights = [1,1,1,1,1,1,1,1,1,1],axis=0)\n",
        "yhat_train_bagboost_3=B.reshape(train_y_5.shape[0],train_y_5.shape[1])\n",
        "print(yhat_train_bagboost_3.shape)\n",
        "\n",
        "\n",
        "\n",
        "### BaseBoost train results ###\n",
        "\n",
        "yhat_train_base_3=model_base.predict(train_X_1D)\n",
        "\n",
        "X_train_bag=yhat_train_base_3\n",
        "X_train_bag=X_train_bag.reshape(train_X_2D.shape[0]*w,6)\n",
        "\n",
        "yhat_train_baseboost_3=model_Baseboost.predict(X_train_bag)\n",
        "yhat_train_baseboost_3=yhat_train_baseboost_3.reshape(train_X_2D.shape[0],w*6)\n",
        "\n",
        "print(yhat_train_baseboost_3.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/bagging/model_1.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/bagging/model_2.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/bagging/model_3.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/bagging/model_4.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/bagging/model_5.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/bagging/model_6.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/bagging/model_7.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/bagging/model_8.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/bagging/model_9.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/bagging/model_10.h5\n",
            "Loaded 10 models\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/boosting/model_1.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/boosting/model_2.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/boosting/model_3.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/boosting/model_4.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/boosting/model_5.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/boosting/model_6.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/boosting/model_7.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/boosting/model_8.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/boosting/model_9.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv1D/boosting/model_10.pkl\n",
            "Loaded 10 models\n",
            "(676, 480, 10)\n",
            "(8852, 480, 10)\n",
            "(54080, 6, 10)\n",
            "(708160, 6, 10)\n",
            "(676, 480)\n",
            "(676, 480)\n",
            "(676, 480)\n",
            "(8852, 480)\n",
            "(8852, 480)\n",
            "(8852, 480)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNWIzvziW9XS"
      },
      "source": [
        "#      ### Bagboost model 4 ###\n",
        "\n",
        "\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.ensemble import GradientBoostingRegressor\n",
        "# from sklearn.multioutput import MultiOutputRegressor\n",
        "# import pickle\n",
        "# from lightgbm import LGBMRegressor\n",
        "# from sklearn.linear_model import Ridge\n",
        "# import xgboost as xg\n",
        "\n",
        "# n_splits=10\n",
        "# i=0\n",
        "\n",
        "# for _ in range(n_splits):\n",
        "# \t# select indexes\n",
        "#   ix = [i for i in range(len(train_X_2D))]\n",
        "#   train_ix = resample(ix, replace=True, n_samples=Bag_samples)\n",
        "#   test_ix = [x for x in ix if x not in train_ix]\n",
        "#   train_X_1D_bagged=train_X_1D[train_ix]\n",
        "#   train_X_2D_bagged=train_X_2D[train_ix]\n",
        "#   train_y=train_y_5[train_ix]\n",
        "\n",
        "#   inputs_1D = tf.keras.layers.Input( shape=(80,16) )\n",
        "#   inputs_2D = tf.keras.layers.Input( shape=(80,8,2) )\n",
        "\n",
        "\n",
        "#   inputs_1D_N =BatchNormalization()(inputs_1D)\n",
        "#   inputs_2D_N =BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "#   model_1=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)\n",
        "#   model_1=Dropout(0.5)(model_1)\n",
        "#   model_1=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)\n",
        "#   model_1=Dropout(0.5)(model_1)\n",
        "#   model_1=Flatten()(model_1)\n",
        "\n",
        "#   X=Conv2D(64, (3, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "#   X=BatchNormalization()(X)\n",
        "#   X=MaxPooling2D((2, 2))(X)\n",
        "#   X=Conv2D(128, (3, 3), activation='relu', padding='same')(X)\n",
        "#   X=BatchNormalization()(X)\n",
        "#   X=MaxPooling2D((2, 2))(X)\n",
        "#   X=Conv2D(128, (3, 3), activation='relu', padding='same')(X)\n",
        "#   X=BatchNormalization()(X)\n",
        "#   X=MaxPooling2D((2, 2))(X)\n",
        "\n",
        "#   X=Dense(128, activation='relu')(X)\n",
        "#   X=Dropout(0.5)(X)\n",
        "#   X=Dense(64,activation='relu')(X)\n",
        "#   X=Dropout(0.5)(X)\n",
        "\n",
        "#   X=Flatten()(X)\n",
        "#   merge = concatenate([model_1, X])\n",
        "\n",
        "#   output=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(merge)\n",
        "#   model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "#   sgd = SGD(lr=0.1, decay=1e-5, momentum=0.9, nesterov=True)\n",
        "#   model_2.compile(loss='mean_squared_error', optimizer='Adam')\n",
        "\n",
        "#   history=model_2.fit([train_X_1D_bagged,train_X_2D_bagged], train_y, epochs=200, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)\n",
        "\n",
        "#   # # # summarize history for loss\n",
        "#   plt.plot(history.history['loss'])\n",
        "#   plt.plot(history.history['val_loss'])\n",
        "#   plt.title('model loss')\n",
        "#   plt.ylabel('loss')\n",
        "#   plt.xlabel('epoch')\n",
        "#   plt.legend(['train', 'test'], loc='upper right')\n",
        "#   plt.show()\n",
        "#   model_2.summary()\n",
        "\n",
        "\n",
        "#   filename_1 = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/bagging/model_' + str(i + 1) + '.h5'\n",
        "#   model_2.save(filename_1)\n",
        "#   print('>Saved %s' % filename_1)\n",
        "\n",
        "\n",
        "#   X_train_bag=model_2.predict([X_validation_1D,X_validation_2D])\n",
        "#   X_train_bag=X_train_bag.reshape(X_validation_1D.shape[0]*w,6)\n",
        "#   train_y_bag=Y_validation.reshape(X_validation_1D.shape[0]*w,6)\n",
        "\n",
        "#   model = MultiOutputRegressor(GradientBoostingRegressor(n_estimators=200,verbose=2))\n",
        "#   model.fit(X_train_bag, train_y_bag)\n",
        "\n",
        "#   filename= '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/boosting/model_' + str(i + 1) + '.pkl'\n",
        "#   with open(filename, 'wb') as file:\n",
        "#         pickle.dump(model, file)\n",
        "#   #pickle.dump(model, open(\"pima.pickle.dat\", \"wb\"))\n",
        "#   print('>Saved %s' % filename)\n",
        "#   i=i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yICUkIG7W_zo"
      },
      "source": [
        "#      ### base Model 4 ###\n",
        "\n",
        "\n",
        "# inputs_1D = tf.keras.layers.Input( shape=(80,16) )\n",
        "# inputs_2D = tf.keras.layers.Input( shape=(80,8,2) )\n",
        "\n",
        "\n",
        "# inputs_1D_N =BatchNormalization()(inputs_1D)\n",
        "# inputs_2D_N =BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "# model_1=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)\n",
        "# model_1=Dropout(0.5)(model_1)\n",
        "# model_1=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)\n",
        "# model_1=Dropout(0.5)(model_1)\n",
        "# model_1=Flatten()(model_1)\n",
        "\n",
        "# X=Conv2D(64, (3, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "# X=BatchNormalization()(X)\n",
        "# X=MaxPooling2D((2, 2))(X)\n",
        "# X=Conv2D(128, (3, 3), activation='relu', padding='same')(X)\n",
        "# X=BatchNormalization()(X)\n",
        "# X=MaxPooling2D((2, 2))(X)\n",
        "# X=Conv2D(128, (3, 3), activation='relu', padding='same')(X)\n",
        "# X=BatchNormalization()(X)\n",
        "# X=MaxPooling2D((2, 2))(X)\n",
        "\n",
        "# X=Dense(128, activation='relu')(X)\n",
        "# X=Dropout(0.5)(X)\n",
        "# X=Dense(64,activation='relu')(X)\n",
        "# X=Dropout(0.5)(X)\n",
        "\n",
        "# X=Flatten()(X)\n",
        "# merge = concatenate([model_1, X])\n",
        "\n",
        "# output=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(merge)\n",
        "# model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "# model_2.compile(loss='mean_squared_error', optimizer='Adam')\n",
        "\n",
        "# history=model_2.fit([train_X_1D,train_X_2D], train_y_5, epochs=200, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=False)\n",
        "\n",
        "# # # # summarize history for loss\n",
        "# plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['val_loss'])\n",
        "# plt.title('model loss')\n",
        "# plt.ylabel('loss')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train', 'test'], loc='upper right')\n",
        "# plt.show()\n",
        "# model_2.summary()\n",
        "\n",
        "\n",
        "# filename = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/model_base.h5'\n",
        "# model_2.save(filename)\n",
        "# print('>Saved %s' % filename)\n",
        "\n",
        "# X_train_bag=model_2.predict([X_validation_1D,X_validation_2D])\n",
        "# X_train_bag=X_train_bag.reshape(X_validation_1D.shape[0]*w,6)\n",
        "# train_y_bag=Y_validation.reshape(X_validation_1D.shape[0]*w,6)\n",
        "\n",
        "# model = MultiOutputRegressor(GradientBoostingRegressor(n_estimators=200,verbose=2))\n",
        "# model.fit(X_train_bag, train_y_bag)\n",
        "\n",
        "# filename_1= '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/model_Base_GB_1.pkl'\n",
        "# with open(filename_1, 'wb') as file:\n",
        "#     pickle.dump(model, file)\n",
        "# print('>Saved %s' % filename_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74t0AaAxDZCD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "888a9418-5fb1-4653-f742-9dbeef7961d3"
      },
      "source": [
        "# load models from file\n",
        "def load_all_models(n_models):\n",
        "    all_models = list()\n",
        "    for i in range(n_models):\n",
        "        # define filename for this ensemble\n",
        "        filename = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/bagging/model_' + str(i + 1) + '.h5'\n",
        "        # load model from file\n",
        "        model = tf.keras.models.load_model(filename)\n",
        "        # add to list of members\n",
        "        all_models.append(model)\n",
        "        print('>loaded %s' % filename)\n",
        "    return all_models\n",
        "\n",
        "\n",
        "\n",
        "       ### Load model and do prediction ###\n",
        "\n",
        "n_members =10\n",
        "members = load_all_models(n_members)\n",
        "\n",
        "print('Loaded %d models' % len(members))\n",
        "\n",
        "\n",
        "# create stacked model input dataset as outputs from the ensemble\n",
        "def stacked_dataset(members, inputX, inputX_lstm):\n",
        "    stackX = None\n",
        "    l=1\n",
        "    for model in members:\n",
        "        # make prediction\n",
        "        yhat = model.predict([inputX_lstm,inputX], verbose=0)\n",
        "        # stack predictions into [rows, members, probabilities]\n",
        "        if stackX is None:\n",
        "            stackX = yhat\n",
        "        else:\n",
        "            stackX = np.dstack((stackX, yhat))\n",
        "    # flatten predictions to [rows, members x probabilities]\n",
        "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1],stackX.shape[2]))\n",
        "    return stackX\n",
        "\n",
        "             ### Function for boosting model ###\n",
        "\n",
        "\n",
        "#  models from file\n",
        "def load_all_boosting_models(n_models):\n",
        "    all_models_boost = list()\n",
        "    for i in range(n_members_boost):\n",
        "        # define filename for this ensemble\n",
        "        filename_boost = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/boosting/model_' + str(i + 1) + '.pkl'\n",
        "        # load model from file\n",
        "        with open(filename_boost, 'rb') as fp:\n",
        "            model_boost = pickle.load(fp)\n",
        "        # add to list of members\n",
        "        all_models_boost.append(model_boost)\n",
        "        print('>loaded %s' % filename_boost)\n",
        "    return all_models_boost\n",
        "\n",
        "\n",
        "\n",
        "           ### Loading and predicting the boosting result\n",
        "\n",
        "n_members_boost =10\n",
        "members_boost = load_all_boosting_models(n_members_boost)\n",
        "\n",
        "print('Loaded %d models' % len(members_boost))\n",
        "\n",
        "\n",
        "# create stacked model input dataset as outputs from the ensemble\n",
        "def stacked_dataset_boost(members_boost, stackedX_test_1):\n",
        "    stackX_boost = None\n",
        "    l=0\n",
        "    for model in members_boost:\n",
        "        # make prediction\n",
        "        stackedX_test_2=stackedX_test_1[:,:,l:l+1]\n",
        "        stackedX_test_3=stackedX_test_2.reshape(stackedX_test_2.shape[0]*w,6)\n",
        "        yhat_boost = model.predict(stackedX_test_3)\n",
        "        l=l+1\n",
        "        # stack predictions into [rows, members, probabilities]\n",
        "        if stackX_boost is None:\n",
        "            stackX_boost = yhat_boost\n",
        "        else:\n",
        "            stackX_boost = np.dstack((stackX_boost, yhat_boost))\n",
        "    stackX_boost= stackX_boost.reshape((stackX_boost.shape[0], stackX_boost.shape[1],stackX_boost.shape[2]))\n",
        "    return stackX_boost\n",
        "\n",
        "        ### Fit test data on baggging ensemble ###\n",
        "\n",
        "# fit a model based on the outputs from the ensemble members\n",
        "def fit_stacked_model_test(members, inputX,inputX_lstm, inputy):\n",
        "    # create dataset using ensemble\n",
        "    from sklearn.linear_model import Ridge\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    stackedX = stacked_dataset(members, inputX,inputX_lstm)\n",
        "    return stackedX\n",
        "\n",
        "\n",
        "### Test data ###\n",
        "\n",
        "stackedX_test= fit_stacked_model_test(members, test_X_2D,test_X_1D, test_y)\n",
        "print(stackedX_test.shape)\n",
        "\n",
        "### Train data ###\n",
        "\n",
        "stackedX_train= fit_stacked_model_test(members, train_X_2D,train_X_1D, train_y_5)\n",
        "print(stackedX_train.shape)\n",
        "\n",
        "        ### Function for boosting ensemble for test data ###\n",
        "\n",
        "def fit_stacked_model_test_boost(members, stackedX_test):\n",
        "\n",
        "    stackedX_boost = stacked_dataset_boost(members_boost, stackedX_test)\n",
        "    return stackedX_boost\n",
        "\n",
        "        ### prediciton of boosting ensemble for test data ###\n",
        "\n",
        "stackedX_test_boost= fit_stacked_model_test_boost(members_boost,stackedX_test )\n",
        "print(stackedX_test_boost.shape)\n",
        "\n",
        "\n",
        "           ### prediciton of boosting ensemble for train data ###\n",
        "\n",
        "stackedX_train_boost= fit_stacked_model_test_boost(members_boost,stackedX_train )\n",
        "print(stackedX_train_boost.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "       #### Bagging Results ###\n",
        "\n",
        "\n",
        "A_1=stackedX_test[:,:,0:1]\n",
        "A_1=A_1.reshape(A_1.shape[0]*A_1.shape[1])\n",
        "\n",
        "A_2=stackedX_test[:,:,1:2]\n",
        "A_2=A_2.reshape(A_2.shape[0]*A_2.shape[1])\n",
        "\n",
        "A_3=stackedX_test[:,:,2:3]\n",
        "A_3=A_3.reshape(A_3.shape[0]*A_3.shape[1])\n",
        "\n",
        "A_4=stackedX_test[:,:,3:4]\n",
        "A_4=A_4.reshape(A_4.shape[0]*A_4.shape[1])\n",
        "\n",
        "A_5=stackedX_test[:,:,4:5]\n",
        "A_5=A_5.reshape(A_5.shape[0]*A_5.shape[1])\n",
        "\n",
        "A_6=stackedX_test[:,:,5:6]\n",
        "A_6=A_6.reshape(A_6.shape[0]*A_6.shape[1])\n",
        "\n",
        "A_7=stackedX_test[:,:,6:7]\n",
        "A_7=A_7.reshape(A_7.shape[0]*A_7.shape[1])\n",
        "\n",
        "A_8=stackedX_test[:,:,7:8]\n",
        "A_8=A_8.reshape(A_8.shape[0]*A_8.shape[1])\n",
        "\n",
        "A_9=stackedX_test[:,:,8:9]\n",
        "A_9=A_9.reshape(A_9.shape[0]*A_9.shape[1])\n",
        "\n",
        "A_10=stackedX_test[:,:,9:10]\n",
        "A_10=A_10.reshape(A_10.shape[0]*A_10.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "B_1=np.array([A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10])\n",
        "B=np.average(B_1, weights = [1,1,1,1,1,1,1,1,1,1],axis=0)\n",
        "yhat_bag_4=B.reshape(test_y.shape[0],test_y.shape[1])\n",
        "print(yhat_bag_4.shape)\n",
        "\n",
        "       ####Boosting Results---test data ###\n",
        "\n",
        "\n",
        "\n",
        "A_1=stackedX_test_boost[:,:,0:1]\n",
        "A_1=A_1.reshape(A_1.shape[0]*A_1.shape[1])\n",
        "\n",
        "A_2=stackedX_test_boost[:,:,1:2]\n",
        "A_2=A_2.reshape(A_2.shape[0]*A_2.shape[1])\n",
        "\n",
        "A_3=stackedX_test_boost[:,:,2:3]\n",
        "A_3=A_3.reshape(A_3.shape[0]*A_3.shape[1])\n",
        "\n",
        "A_4=stackedX_test_boost[:,:,3:4]\n",
        "A_4=A_4.reshape(A_4.shape[0]*A_4.shape[1])\n",
        "\n",
        "A_5=stackedX_test_boost[:,:,4:5]\n",
        "A_5=A_5.reshape(A_5.shape[0]*A_5.shape[1])\n",
        "\n",
        "A_6=stackedX_test_boost[:,:,5:6]\n",
        "A_6=A_6.reshape(A_6.shape[0]*A_6.shape[1])\n",
        "\n",
        "A_7=stackedX_test_boost[:,:,6:7]\n",
        "A_7=A_7.reshape(A_7.shape[0]*A_7.shape[1])\n",
        "\n",
        "A_8=stackedX_test_boost[:,:,7:8]\n",
        "A_8=A_8.reshape(A_8.shape[0]*A_8.shape[1])\n",
        "\n",
        "A_9=stackedX_test_boost[:,:,8:9]\n",
        "A_9=A_9.reshape(A_9.shape[0]*A_9.shape[1])\n",
        "\n",
        "A_10=stackedX_test_boost[:,:,9:10]\n",
        "A_10=A_10.reshape(A_10.shape[0]*A_10.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "B_1=np.array([A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10])\n",
        "B=np.average(B_1, weights = [1,1,1,1,1,1,1,1,1,1],axis=0)\n",
        "yhat_bagboost_4=B.reshape(test_y.shape[0],test_y.shape[1])\n",
        "print(yhat_bagboost_4.shape)\n",
        "\n",
        "\n",
        "\n",
        "    ### prediciton for base model ###\n",
        "\n",
        "filename_base = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/model_base.h5'\n",
        "model_base = tf.keras.models.load_model(filename_base)\n",
        "yhat_base_4=model_base.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "     ### BaseBoost Result ###\n",
        "\n",
        "X_test_bag=yhat_base_4\n",
        "X_test_bag=X_test_bag.reshape(test_X_1D.shape[0]*w,6)\n",
        "\n",
        "filename_base_boost='/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/model_Base_GB_1.pkl'\n",
        "with open(filename_base_boost, 'rb') as fp:\n",
        "            model_Baseboost = pickle.load(fp)\n",
        "\n",
        "yhat_baseboost_4=model_Baseboost.predict(X_test_bag)\n",
        "yhat_baseboost_4=yhat_baseboost_4.reshape(test_X_2D.shape[0],w*6)\n",
        "print(yhat_baseboost_4.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "       #### Bagging Results---train data ###\n",
        "\n",
        "A_1=stackedX_train[:,:,0:1]\n",
        "A_1=A_1.reshape(A_1.shape[0]*A_1.shape[1])\n",
        "\n",
        "A_2=stackedX_train[:,:,1:2]\n",
        "A_2=A_2.reshape(A_2.shape[0]*A_2.shape[1])\n",
        "\n",
        "A_3=stackedX_train[:,:,2:3]\n",
        "A_3=A_3.reshape(A_3.shape[0]*A_3.shape[1])\n",
        "\n",
        "A_4=stackedX_train[:,:,3:4]\n",
        "A_4=A_4.reshape(A_4.shape[0]*A_4.shape[1])\n",
        "\n",
        "A_5=stackedX_train[:,:,4:5]\n",
        "A_5=A_5.reshape(A_5.shape[0]*A_5.shape[1])\n",
        "\n",
        "A_6=stackedX_train[:,:,5:6]\n",
        "A_6=A_6.reshape(A_6.shape[0]*A_6.shape[1])\n",
        "\n",
        "A_7=stackedX_train[:,:,6:7]\n",
        "A_7=A_7.reshape(A_7.shape[0]*A_7.shape[1])\n",
        "\n",
        "A_8=stackedX_train[:,:,7:8]\n",
        "A_8=A_8.reshape(A_8.shape[0]*A_8.shape[1])\n",
        "\n",
        "A_9=stackedX_train[:,:,8:9]\n",
        "A_9=A_9.reshape(A_9.shape[0]*A_9.shape[1])\n",
        "\n",
        "A_10=stackedX_train[:,:,9:10]\n",
        "A_10=A_10.reshape(A_10.shape[0]*A_10.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "B_1=np.array([A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10])\n",
        "B=np.average(B_1, weights = [1,1,1,1,1,1,1,1,1,1],axis=0)\n",
        "yhat_train_bag_4=B.reshape(train_y_5.shape[0],train_y_5.shape[1])\n",
        "print(yhat_train_bag_4.shape)\n",
        "\n",
        "\n",
        " ##### Boosting Train Results ####\n",
        "\n",
        "A_1=stackedX_train_boost[:,:,0:1]\n",
        "A_1=A_1.reshape(A_1.shape[0]*A_1.shape[1])\n",
        "\n",
        "A_2=stackedX_train_boost[:,:,1:2]\n",
        "A_2=A_2.reshape(A_2.shape[0]*A_2.shape[1])\n",
        "\n",
        "A_3=stackedX_train_boost[:,:,2:3]\n",
        "A_3=A_3.reshape(A_3.shape[0]*A_3.shape[1])\n",
        "\n",
        "A_4=stackedX_train_boost[:,:,3:4]\n",
        "A_4=A_4.reshape(A_4.shape[0]*A_4.shape[1])\n",
        "\n",
        "A_5=stackedX_train_boost[:,:,4:5]\n",
        "A_5=A_5.reshape(A_5.shape[0]*A_5.shape[1])\n",
        "\n",
        "A_6=stackedX_train_boost[:,:,5:6]\n",
        "A_6=A_6.reshape(A_6.shape[0]*A_6.shape[1])\n",
        "\n",
        "A_7=stackedX_train_boost[:,:,6:7]\n",
        "A_7=A_7.reshape(A_7.shape[0]*A_7.shape[1])\n",
        "\n",
        "A_8=stackedX_train_boost[:,:,7:8]\n",
        "A_8=A_8.reshape(A_8.shape[0]*A_8.shape[1])\n",
        "\n",
        "A_9=stackedX_train_boost[:,:,8:9]\n",
        "A_9=A_9.reshape(A_9.shape[0]*A_9.shape[1])\n",
        "\n",
        "A_10=stackedX_train_boost[:,:,9:10]\n",
        "A_10=A_10.reshape(A_10.shape[0]*A_10.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "B_1=np.array([A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10])\n",
        "B=np.average(B_1, weights = [1,1,1,1,1,1,1,1,1,1],axis=0)\n",
        "yhat_train_bagboost_4=B.reshape(train_y_5.shape[0],train_y_5.shape[1])\n",
        "print(yhat_train_bagboost_4.shape)\n",
        "\n",
        "\n",
        "### BaseBoost train results ###\n",
        "\n",
        "yhat_train_base_4=model_base.predict([train_X_1D,train_X_2D])\n",
        "\n",
        "X_train_bag=yhat_train_base_4\n",
        "X_train_bag=X_train_bag.reshape(train_X_2D.shape[0]*w,6)\n",
        "\n",
        "yhat_train_baseboost_4=model_Baseboost.predict(X_train_bag)\n",
        "yhat_train_baseboost_4=yhat_train_baseboost_4.reshape(train_X_2D.shape[0],w*6)\n",
        "\n",
        "print(yhat_train_baseboost_4.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/bagging/model_1.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/bagging/model_2.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/bagging/model_3.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/bagging/model_4.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/bagging/model_5.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/bagging/model_6.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/bagging/model_7.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/bagging/model_8.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/bagging/model_9.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/bagging/model_10.h5\n",
            "Loaded 10 models\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/boosting/model_1.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/boosting/model_2.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/boosting/model_3.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/boosting/model_4.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/boosting/model_5.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/boosting/model_6.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/boosting/model_7.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/boosting/model_8.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/boosting/model_9.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_1/boosting/model_10.pkl\n",
            "Loaded 10 models\n",
            "(676, 480, 10)\n",
            "(8852, 480, 10)\n",
            "(54080, 6, 10)\n",
            "(708160, 6, 10)\n",
            "(676, 480)\n",
            "(676, 480)\n",
            "(676, 480)\n",
            "(8852, 480)\n",
            "(8852, 480)\n",
            "(8852, 480)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-0QK0I3XGC0"
      },
      "source": [
        "#               ### bagboost model 5 ###\n",
        "\n",
        "\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.ensemble import GradientBoostingRegressor\n",
        "# from sklearn.multioutput import MultiOutputRegressor\n",
        "# import pickle\n",
        "# from lightgbm import LGBMRegressor\n",
        "# from sklearn.linear_model import Ridge\n",
        "# import xgboost as xg\n",
        "\n",
        "# n_splits=10\n",
        "\n",
        "# i=0\n",
        "# for _ in range(n_splits):\n",
        "# \t# select indexes\n",
        "#   ix = [i for i in range(len(train_X_2D))]\n",
        "#   train_ix = resample(ix, replace=True, n_samples=Bag_samples)\n",
        "#   train_X=train_X_2D[train_ix]\n",
        "#   train_y=train_y_5[train_ix]\n",
        "\n",
        "#          # The shape of the input image.\n",
        "#   inputs_1D = tf.keras.layers.Input( shape=(w,16) )\n",
        "#   inputs_2D = tf.keras.layers.Input( shape=(w,8,2) )\n",
        "\n",
        "\n",
        "#   inputs_1D_N =BatchNormalization()(inputs_1D)\n",
        "#   inputs_2D_N =BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "#   X=Conv2D(64, (3, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "#   X=BatchNormalization()(X)\n",
        "#   X=MaxPooling2D((2, 2))(X)\n",
        "#   X=Conv2D(128, (3, 3), activation='relu', padding='same')(X)\n",
        "#   X=BatchNormalization()(X)\n",
        "#   X=MaxPooling2D((2, 2))(X)\n",
        "#   X=Conv2D(128, (3, 3), activation='relu', padding='same')(X)\n",
        "#   X=BatchNormalization()(X)\n",
        "#   X=MaxPooling2D((2, 2))(X)\n",
        "#   X=TimeDistributed(Flatten())(X)\n",
        "#   X=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(X)\n",
        "#   X=Dropout(0.5)(X)\n",
        "#   X=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(X)\n",
        "#   X=Dropout(0.5)(X)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#   dense=Dense(w, activation='relu')(X)\n",
        "#   dense=Flatten()(dense)\n",
        "#   output=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(dense)\n",
        "#   model_1 = Model(inputs=inputs_2D, outputs=output)\n",
        "#   sgd = SGD(lr=0.1, decay=1e-5, momentum=0.9, nesterov=True)\n",
        "#   model_1.compile(loss='mean_squared_error', optimizer='Adam')\n",
        "\n",
        "#   history=model_1.fit(train_X, train_y, epochs=70, batch_size=64, validation_data=(X_validation_2D, Y_validation), verbose=2, shuffle=False)\n",
        "\n",
        "#   # # # summarize history for loss\n",
        "#   plt.plot(history.history['loss'])\n",
        "#   plt.plot(history.history['val_loss'])\n",
        "#   plt.title('model loss')\n",
        "#   plt.ylabel('loss')\n",
        "#   plt.xlabel('epoch')\n",
        "#   plt.legend(['train', 'test'], loc='upper right')\n",
        "#   plt.show()\n",
        "#   model_1.summary()\n",
        "\n",
        "\n",
        "#   filename_1 = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/bagging/model_' + str(i + 1) + '.h5'\n",
        "#   model_1.save(filename_1)\n",
        "#   print('>Saved %s' % filename_1)\n",
        "\n",
        "#   X_train_bag=model_1.predict(X_validation_2D)\n",
        "#   X_train_bag=X_train_bag.reshape(X_validation_1D.shape[0]*w,6)\n",
        "#   train_y_bag=Y_validation.reshape(X_validation_1D.shape[0]*w,6)\n",
        "\n",
        "#   model = MultiOutputRegressor(GradientBoostingRegressor(n_estimators=200,verbose=2))\n",
        "#   model.fit(X_train_bag, train_y_bag)\n",
        "\n",
        "\n",
        "\n",
        "#   filename= '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/boosting/model_' + str(i + 1) + '.pkl'\n",
        "#   with open(filename, 'wb') as file:\n",
        "#       pickle.dump(model, file)\n",
        "#   #pickle.dump(model, open(\"pima.pickle.dat\", \"wb\"))\n",
        "#   print('>Saved %s' % filename)\n",
        "#   i=i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxYWzCq5XIJ8"
      },
      "source": [
        "#          ### Base model 5 ###\n",
        "\n",
        "# #     # The shape of the input image.\n",
        "# inputs_1D = tf.keras.layers.Input( shape=(w,16) )\n",
        "# inputs_2D = tf.keras.layers.Input( shape=(w,8,2) )\n",
        "\n",
        "\n",
        "# inputs_1D_N =BatchNormalization()(inputs_1D)\n",
        "# inputs_2D_N =BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "# X=Conv2D(64, (3, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "# X=BatchNormalization()(X)\n",
        "# X=MaxPooling2D((2, 2))(X)\n",
        "# X=Conv2D(128, (3, 3), activation='relu', padding='same')(X)\n",
        "# X=BatchNormalization()(X)\n",
        "# X=MaxPooling2D((2, 2))(X)\n",
        "# X=Conv2D(128, (3, 3), activation='relu', padding='same')(X)\n",
        "# X=BatchNormalization()(X)\n",
        "# X=MaxPooling2D((2, 2))(X)\n",
        "# X=TimeDistributed(Flatten())(X)\n",
        "# X=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(X)\n",
        "# X=Dropout(0.5)(X)\n",
        "# X=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(X)\n",
        "# X=Dropout(0.5)(X)\n",
        "\n",
        "\n",
        "# dense=Dense(w, activation='relu')(X)\n",
        "# dense=Flatten()(dense)\n",
        "# output=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(dense)\n",
        "# model_1 = Model(inputs=inputs_2D, outputs=output)\n",
        "# model_1.compile(loss='mean_squared_error', optimizer='Adam')\n",
        "\n",
        "# history=model_1.fit(train_X_2D, train_y_5, epochs=70, batch_size=64, validation_data=(X_validation_2D, Y_validation), verbose=2, shuffle=False)\n",
        "\n",
        "\n",
        "# # # # summarize history for loss\n",
        "# plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['val_loss'])\n",
        "# plt.title('model loss')\n",
        "# plt.ylabel('loss')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train', 'test'], loc='upper right')\n",
        "# plt.show()\n",
        "# model_1.summary()\n",
        "\n",
        "\n",
        "# filename = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/model_base.h5'\n",
        "# model_1.save(filename)\n",
        "# print('>Saved %s' % filename)\n",
        "\n",
        "\n",
        "# X_train_bag=model_1.predict(X_validation_2D)\n",
        "# X_train_bag=X_train_bag.reshape(X_validation_2D.shape[0]*w,6)\n",
        "# train_y_bag=Y_validation.reshape(X_validation_2D.shape[0]*w,6)\n",
        "\n",
        "# model = MultiOutputRegressor(GradientBoostingRegressor(n_estimators=200,verbose=2))\n",
        "# model.fit(X_train_bag, train_y_bag)\n",
        "\n",
        "# filename_1= '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/model_Base_GB_1.pkl'\n",
        "# with open(filename_1, 'wb') as file:\n",
        "#     pickle.dump(model, file)\n",
        "# print('>Saved %s' % filename_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fij8vagLJeA7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f00ef0b-4bdf-4400-b174-45ecc0553505"
      },
      "source": [
        "# load models from file\n",
        "def load_all_models(n_models):\n",
        "    all_models = list()\n",
        "    for i in range(n_models):\n",
        "        # define filename for this ensemble\n",
        "        filename = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/bagging/model_' + str(i + 1) + '.h5'\n",
        "        # load model from file\n",
        "        model = tf.keras.models.load_model(filename)\n",
        "        # add to list of members\n",
        "        all_models.append(model)\n",
        "        print('>loaded %s' % filename)\n",
        "    return all_models\n",
        "\n",
        "\n",
        "\n",
        "       ### Load model and do prediction ###\n",
        "\n",
        "n_members =10\n",
        "members = load_all_models(n_members)\n",
        "\n",
        "print('Loaded %d models' % len(members))\n",
        "\n",
        "\n",
        "# create stacked model input dataset as outputs from the ensemble\n",
        "def stacked_dataset(members, inputX, inputX_lstm):\n",
        "    stackX = None\n",
        "    l=1\n",
        "    for model in members:\n",
        "        # make prediction\n",
        "        yhat = model.predict(inputX, verbose=0)\n",
        "        # stack predictions into [rows, members, probabilities]\n",
        "        if stackX is None:\n",
        "            stackX = yhat\n",
        "        else:\n",
        "            stackX = np.dstack((stackX, yhat))\n",
        "    # flatten predictions to [rows, members x probabilities]\n",
        "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1],stackX.shape[2]))\n",
        "    return stackX\n",
        "\n",
        "             ### Function for boosting model ###\n",
        "\n",
        "\n",
        "#  models from file\n",
        "def load_all_boosting_models(n_models):\n",
        "    all_models_boost = list()\n",
        "    for i in range(n_members_boost):\n",
        "        # define filename for this ensemble\n",
        "        filename_boost = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/boosting/model_' + str(i + 1) + '.pkl'\n",
        "        # load model from file\n",
        "        with open(filename_boost, 'rb') as fp:\n",
        "            model_boost = pickle.load(fp)\n",
        "        # add to list of members\n",
        "        all_models_boost.append(model_boost)\n",
        "        print('>loaded %s' % filename_boost)\n",
        "    return all_models_boost\n",
        "\n",
        "\n",
        "\n",
        "           ### Loading and predicting the boosting result\n",
        "\n",
        "n_members_boost =10\n",
        "members_boost = load_all_boosting_models(n_members_boost)\n",
        "\n",
        "print('Loaded %d models' % len(members_boost))\n",
        "\n",
        "\n",
        "# create stacked model input dataset as outputs from the ensemble\n",
        "def stacked_dataset_boost(members_boost, stackedX_test_1):\n",
        "    stackX_boost = None\n",
        "    l=0\n",
        "    for model in members_boost:\n",
        "        # make prediction\n",
        "        stackedX_test_2=stackedX_test_1[:,:,l:l+1]\n",
        "        stackedX_test_3=stackedX_test_2.reshape(stackedX_test_2.shape[0]*w,6)\n",
        "        yhat_boost = model.predict(stackedX_test_3)\n",
        "        l=l+1\n",
        "        # stack predictions into [rows, members, probabilities]\n",
        "        if stackX_boost is None:\n",
        "            stackX_boost = yhat_boost\n",
        "        else:\n",
        "            stackX_boost = np.dstack((stackX_boost, yhat_boost))\n",
        "    stackX_boost= stackX_boost.reshape((stackX_boost.shape[0], stackX_boost.shape[1],stackX_boost.shape[2]))\n",
        "    return stackX_boost\n",
        "\n",
        "        ### Fit test data on baggging ensemble ###\n",
        "\n",
        "# fit a model based on the outputs from the ensemble members\n",
        "def fit_stacked_model_test(members, inputX,inputX_lstm, inputy):\n",
        "    # create dataset using ensemble\n",
        "    from sklearn.linear_model import Ridge\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    stackedX = stacked_dataset(members, inputX,inputX_lstm)\n",
        "    return stackedX\n",
        "\n",
        "\n",
        "### Test data ###\n",
        "\n",
        "stackedX_test= fit_stacked_model_test(members, test_X_2D,test_X_1D, test_y)\n",
        "print(stackedX_test.shape)\n",
        "\n",
        "### Train data ###\n",
        "\n",
        "stackedX_train= fit_stacked_model_test(members, train_X_2D,train_X_1D, train_y_5)\n",
        "print(stackedX_train.shape)\n",
        "\n",
        "        ### Function for boosting ensemble for test data ###\n",
        "\n",
        "def fit_stacked_model_test_boost(members, stackedX_test):\n",
        "\n",
        "    stackedX_boost = stacked_dataset_boost(members_boost, stackedX_test)\n",
        "    return stackedX_boost\n",
        "\n",
        "        ### prediciton of boosting ensemble for test data ###\n",
        "\n",
        "stackedX_test_boost= fit_stacked_model_test_boost(members_boost,stackedX_test )\n",
        "print(stackedX_test_boost.shape)\n",
        "\n",
        "\n",
        "           ### prediciton of boosting ensemble for train data ###\n",
        "\n",
        "stackedX_train_boost= fit_stacked_model_test_boost(members_boost,stackedX_train )\n",
        "print(stackedX_train_boost.shape)\n",
        "\n",
        "\n",
        "       #### Bagging Results ###\n",
        "\n",
        "\n",
        "A_1=stackedX_test[:,:,0:1]\n",
        "A_1=A_1.reshape(A_1.shape[0]*A_1.shape[1])\n",
        "\n",
        "A_2=stackedX_test[:,:,1:2]\n",
        "A_2=A_2.reshape(A_2.shape[0]*A_2.shape[1])\n",
        "\n",
        "A_3=stackedX_test[:,:,2:3]\n",
        "A_3=A_3.reshape(A_3.shape[0]*A_3.shape[1])\n",
        "\n",
        "A_4=stackedX_test[:,:,3:4]\n",
        "A_4=A_4.reshape(A_4.shape[0]*A_4.shape[1])\n",
        "\n",
        "A_5=stackedX_test[:,:,4:5]\n",
        "A_5=A_5.reshape(A_5.shape[0]*A_5.shape[1])\n",
        "\n",
        "A_6=stackedX_test[:,:,5:6]\n",
        "A_6=A_6.reshape(A_6.shape[0]*A_6.shape[1])\n",
        "\n",
        "A_7=stackedX_test[:,:,6:7]\n",
        "A_7=A_7.reshape(A_7.shape[0]*A_7.shape[1])\n",
        "\n",
        "A_8=stackedX_test[:,:,7:8]\n",
        "A_8=A_8.reshape(A_8.shape[0]*A_8.shape[1])\n",
        "\n",
        "A_9=stackedX_test[:,:,8:9]\n",
        "A_9=A_9.reshape(A_9.shape[0]*A_9.shape[1])\n",
        "\n",
        "A_10=stackedX_test[:,:,9:10]\n",
        "A_10=A_10.reshape(A_10.shape[0]*A_10.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "B_1=np.array([A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10])\n",
        "B=np.average(B_1, weights = [1,1,1,1,1,1,1,1,1,1],axis=0)\n",
        "yhat_bag_5=B.reshape(test_y.shape[0],test_y.shape[1])\n",
        "print(yhat_bag_5.shape)\n",
        "\n",
        "\n",
        "       ####Boosting Results---test data ###\n",
        "\n",
        "\n",
        "\n",
        "A_1=stackedX_test_boost[:,:,0:1]\n",
        "A_1=A_1.reshape(A_1.shape[0]*A_1.shape[1])\n",
        "\n",
        "A_2=stackedX_test_boost[:,:,1:2]\n",
        "A_2=A_2.reshape(A_2.shape[0]*A_2.shape[1])\n",
        "\n",
        "A_3=stackedX_test_boost[:,:,2:3]\n",
        "A_3=A_3.reshape(A_3.shape[0]*A_3.shape[1])\n",
        "\n",
        "A_4=stackedX_test_boost[:,:,3:4]\n",
        "A_4=A_4.reshape(A_4.shape[0]*A_4.shape[1])\n",
        "\n",
        "A_5=stackedX_test_boost[:,:,4:5]\n",
        "A_5=A_5.reshape(A_5.shape[0]*A_5.shape[1])\n",
        "\n",
        "A_6=stackedX_test_boost[:,:,5:6]\n",
        "A_6=A_6.reshape(A_6.shape[0]*A_6.shape[1])\n",
        "\n",
        "A_7=stackedX_test_boost[:,:,6:7]\n",
        "A_7=A_7.reshape(A_7.shape[0]*A_7.shape[1])\n",
        "\n",
        "A_8=stackedX_test_boost[:,:,7:8]\n",
        "A_8=A_8.reshape(A_8.shape[0]*A_8.shape[1])\n",
        "\n",
        "A_9=stackedX_test_boost[:,:,8:9]\n",
        "A_9=A_9.reshape(A_9.shape[0]*A_9.shape[1])\n",
        "\n",
        "A_10=stackedX_test_boost[:,:,9:10]\n",
        "A_10=A_10.reshape(A_10.shape[0]*A_10.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "B_1=np.array([A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10])\n",
        "B=np.average(B_1, weights = [1,1,1,1,1,1,1,1,1,1],axis=0)\n",
        "yhat_bagboost_5=B.reshape(test_y.shape[0],test_y.shape[1])\n",
        "print(yhat_bagboost_5.shape)\n",
        "\n",
        "    ### prediciton for base model ###\n",
        "\n",
        "\n",
        "filename_base = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/model_base.h5'\n",
        "model_base = tf.keras.models.load_model(filename_base)\n",
        "yhat_base_5=model_base.predict(test_X_2D)\n",
        "\n",
        "     ### BaseBoost Result ###\n",
        "\n",
        "X_test_bag=yhat_base_5\n",
        "X_test_bag=X_test_bag.reshape(test_X_2D.shape[0]*w,6)\n",
        "\n",
        "filename_base_boost='/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/model_Base_GB_1.pkl'\n",
        "with open(filename_base_boost, 'rb') as fp:\n",
        "            model_Baseboost = pickle.load(fp)\n",
        "\n",
        "yhat_baseboost_5=model_Baseboost.predict(X_test_bag)\n",
        "yhat_baseboost_5=yhat_baseboost_5.reshape(test_X_2D.shape[0],w*6)\n",
        "print(yhat_baseboost_5.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "       #### Bagging Results---train data ###\n",
        "\n",
        "A_1=stackedX_train[:,:,0:1]\n",
        "A_1=A_1.reshape(A_1.shape[0]*A_1.shape[1])\n",
        "\n",
        "A_2=stackedX_train[:,:,1:2]\n",
        "A_2=A_2.reshape(A_2.shape[0]*A_2.shape[1])\n",
        "\n",
        "A_3=stackedX_train[:,:,2:3]\n",
        "A_3=A_3.reshape(A_3.shape[0]*A_3.shape[1])\n",
        "\n",
        "A_4=stackedX_train[:,:,3:4]\n",
        "A_4=A_4.reshape(A_4.shape[0]*A_4.shape[1])\n",
        "\n",
        "A_5=stackedX_train[:,:,4:5]\n",
        "A_5=A_5.reshape(A_5.shape[0]*A_5.shape[1])\n",
        "\n",
        "A_6=stackedX_train[:,:,5:6]\n",
        "A_6=A_6.reshape(A_6.shape[0]*A_6.shape[1])\n",
        "\n",
        "A_7=stackedX_train[:,:,6:7]\n",
        "A_7=A_7.reshape(A_7.shape[0]*A_7.shape[1])\n",
        "\n",
        "A_8=stackedX_train[:,:,7:8]\n",
        "A_8=A_8.reshape(A_8.shape[0]*A_8.shape[1])\n",
        "\n",
        "A_9=stackedX_train[:,:,8:9]\n",
        "A_9=A_9.reshape(A_9.shape[0]*A_9.shape[1])\n",
        "\n",
        "A_10=stackedX_train[:,:,9:10]\n",
        "A_10=A_10.reshape(A_10.shape[0]*A_10.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "B_1=np.array([A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10])\n",
        "B=np.average(B_1, weights = [1,1,1,1,1,1,1,1,1,1],axis=0)\n",
        "yhat_train_bag_5=B.reshape(train_y_5.shape[0],train_y_5.shape[1])\n",
        "print(yhat_train_bag_5.shape)\n",
        "\n",
        "\n",
        "\n",
        " ##### Boosting Train Results ####\n",
        "\n",
        "A_1=stackedX_train_boost[:,:,0:1]\n",
        "A_1=A_1.reshape(A_1.shape[0]*A_1.shape[1])\n",
        "\n",
        "A_2=stackedX_train_boost[:,:,1:2]\n",
        "A_2=A_2.reshape(A_2.shape[0]*A_2.shape[1])\n",
        "\n",
        "A_3=stackedX_train_boost[:,:,2:3]\n",
        "A_3=A_3.reshape(A_3.shape[0]*A_3.shape[1])\n",
        "\n",
        "A_4=stackedX_train_boost[:,:,3:4]\n",
        "A_4=A_4.reshape(A_4.shape[0]*A_4.shape[1])\n",
        "\n",
        "A_5=stackedX_train_boost[:,:,4:5]\n",
        "A_5=A_5.reshape(A_5.shape[0]*A_5.shape[1])\n",
        "\n",
        "A_6=stackedX_train_boost[:,:,5:6]\n",
        "A_6=A_6.reshape(A_6.shape[0]*A_6.shape[1])\n",
        "\n",
        "A_7=stackedX_train_boost[:,:,6:7]\n",
        "A_7=A_7.reshape(A_7.shape[0]*A_7.shape[1])\n",
        "\n",
        "A_8=stackedX_train_boost[:,:,7:8]\n",
        "A_8=A_8.reshape(A_8.shape[0]*A_8.shape[1])\n",
        "\n",
        "A_9=stackedX_train_boost[:,:,8:9]\n",
        "A_9=A_9.reshape(A_9.shape[0]*A_9.shape[1])\n",
        "\n",
        "A_10=stackedX_train_boost[:,:,9:10]\n",
        "A_10=A_10.reshape(A_10.shape[0]*A_10.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "B_1=np.array([A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10])\n",
        "B=np.average(B_1, weights = [1,1,1,1,1,1,1,1,1,1],axis=0)\n",
        "yhat_train_bagboost_5=B.reshape(train_y_5.shape[0],train_y_5.shape[1])\n",
        "print(yhat_train_bagboost_5.shape)\n",
        "\n",
        "\n",
        "\n",
        "### BaseBoost train results ###\n",
        "\n",
        "yhat_train_base_5=model_base.predict(train_X_2D)\n",
        "\n",
        "X_train_bag=yhat_train_base_5\n",
        "X_train_bag=X_train_bag.reshape(train_X_2D.shape[0]*w,6)\n",
        "\n",
        "yhat_train_baseboost_5=model_Baseboost.predict(X_train_bag)\n",
        "yhat_train_baseboost_5=yhat_train_baseboost_5.reshape(train_X_2D.shape[0],w*6)\n",
        "\n",
        "print(yhat_train_baseboost_5.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/bagging/model_1.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/bagging/model_2.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/bagging/model_3.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/bagging/model_4.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/bagging/model_5.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/bagging/model_6.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/bagging/model_7.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/bagging/model_8.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/bagging/model_9.h5\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/bagging/model_10.h5\n",
            "Loaded 10 models\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/boosting/model_1.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/boosting/model_2.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/boosting/model_3.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/boosting/model_4.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/boosting/model_5.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/boosting/model_6.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/boosting/model_7.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/boosting/model_8.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/boosting/model_9.pkl\n",
            ">loaded /content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/LSTM_Conv2D_2/boosting/model_10.pkl\n",
            "Loaded 10 models\n",
            "(676, 480, 10)\n",
            "(8852, 480, 10)\n",
            "(54080, 6, 10)\n",
            "(708160, 6, 10)\n",
            "(676, 480)\n",
            "(676, 480)\n",
            "(676, 480)\n",
            "(8852, 480)\n",
            "(8852, 480)\n",
            "(8852, 480)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gc3o46aUTMX"
      },
      "source": [
        "# Comment during evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOONMVJ70av_"
      },
      "source": [
        "#    ### Base Meta Learner Training for stacking  ###\n",
        "# yhat_train_base_a=np.concatenate((yhat_train_base_1,yhat_train_base_2,yhat_train_base_3,yhat_train_base_4,yhat_train_base_5),axis=1)\n",
        "# yhat_train_base=yhat_train_base_a.reshape(yhat_train_base_1.shape[0],yhat_train_base_1.shape[1]*5)\n",
        "\n",
        "# print(yhat_train_base.shape)\n",
        "\n",
        "# # train_y_meta_learner=train_y_5.reshape(train_y_5.shape[0],w*6)\n",
        "\n",
        "\n",
        "# # meta_learner_base=Ridge(alpha=0.01)\n",
        "# # meta_learner_base.fit(yhat_train_base,train_y_meta_learner)\n",
        "\n",
        "# # pkl_filename = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/meta_learner_base.pkl'\n",
        "# # with open(pkl_filename, 'wb') as file:\n",
        "# #     pickle.dump(meta_learner_base, file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgkOmOCS0_Fd"
      },
      "source": [
        "#    ### Base Meta learner prediction for testing ###\n",
        "# yhat_base_a=np.concatenate((yhat_base_1,yhat_base_2,yhat_base_3,yhat_base_4,yhat_base_5),axis=1)\n",
        "# yhat_base_s=yhat_base_a.reshape(yhat_base_1.shape[0],yhat_base_1.shape[1]*5)\n",
        "# print(yhat_base_s.shape)\n",
        "\n",
        "\n",
        "# # meta_learner_2_model='/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/meta_learner_base.pkl'\n",
        "# # with open(meta_learner_2_model, 'rb') as fp:\n",
        "# #             meta_learner_base = pickle.load(fp)\n",
        "\n",
        "# # yhat_base=meta_learner_base.predict(yhat_base_s)\n",
        "\n",
        "# # print(yhat_base.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "429NTq93UZZR"
      },
      "source": [
        "# Comment during evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXVIVKs4JuIp"
      },
      "source": [
        "# ## Meta Learner 2 ###\n",
        "\n",
        "\n",
        "# yhat_train_bagboost_a=np.concatenate((yhat_train_bagboost_1,yhat_train_bagboost_2,yhat_train_bagboost_3,yhat_train_bagboost_4,yhat_train_bagboost_5),axis=1)\n",
        "# yhat_train_bagboost=yhat_train_bagboost_a.reshape(yhat_train_bagboost_1.shape[0],yhat_train_bagboost_1.shape[1]*5)\n",
        "# print(yhat_train_bagboost.shape)\n",
        "\n",
        "# train_y_meta_learner=train_y_5.reshape(train_y_5.shape[0],w*6)\n",
        "\n",
        "\n",
        "# meta_learner_2=Ridge(alpha=0.01)\n",
        "# meta_learner_2.fit(yhat_train_bagboost,train_y_meta_learner)\n",
        "\n",
        "# pkl_filename = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/meta_learner_2.pkl'\n",
        "# with open(pkl_filename, 'wb') as file:\n",
        "#     pickle.dump(meta_learner_2, file)\n",
        "\n",
        "\n",
        "# ### Meta Learner 3 ###\n",
        "\n",
        "# yhat_train_bag_a=np.concatenate((yhat_train_bag_1,yhat_train_bag_2,yhat_train_bag_3,yhat_train_bag_4,yhat_train_bag_5),axis=1)\n",
        "# yhat_train_bag=yhat_train_bag_a.reshape(yhat_train_bag_1.shape[0],yhat_train_bag_1.shape[1]*5)\n",
        "# print(yhat_train_bag.shape)\n",
        "\n",
        "\n",
        "# meta_learner_3=Ridge(alpha=0.01)\n",
        "# meta_learner_3.fit(yhat_train_bag,train_y_meta_learner)\n",
        "\n",
        "# pkl_filename = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/meta_learner_3.pkl'\n",
        "# with open(pkl_filename, 'wb') as file:\n",
        "#     pickle.dump(meta_learner_3, file)\n",
        "\n",
        "# ### Meta Learner 4 ###\n",
        "\n",
        "# yhat_train_baseboost_a=np.concatenate((yhat_train_baseboost_1,yhat_train_baseboost_2,yhat_train_baseboost_3,yhat_train_baseboost_4,yhat_train_baseboost_5),axis=1)\n",
        "# yhat_train_baseboost=yhat_train_baseboost_a.reshape(yhat_train_baseboost_1.shape[0],yhat_train_baseboost_1.shape[1]*5)\n",
        "# print(yhat_train_baseboost.shape)\n",
        "\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "\n",
        "# meta_learner_4=Ridge(alpha=0.01)\n",
        "# meta_learner_4.fit(yhat_train_baseboost,train_y_meta_learner)\n",
        "\n",
        "# pkl_filename = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/meta_learner_4.pkl'\n",
        "# with open(pkl_filename, 'wb') as file:\n",
        "#     pickle.dump(meta_learner_4, file)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQkCttTMNj7N"
      },
      "source": [
        "#  ### Meta learner Results ###\n",
        "\n",
        "\n",
        "#  ##### Meta Learner 2 ###\n",
        "# yhat_test_bagboost_a=np.concatenate((yhat_bagboost_1,yhat_bagboost_2,yhat_bagboost_3,yhat_bagboost_4,yhat_bagboost_5),axis=1)\n",
        "# yhat_test_bagboost=yhat_test_bagboost_a.reshape(yhat_bagboost_1.shape[0],yhat_bagboost_1.shape[1]*5)\n",
        "# print(yhat_test_bagboost.shape)\n",
        "\n",
        "# # test_y_meta_learner=test_X_1D.reshape(test_X_1D.shape[0],w*6)\n",
        "\n",
        "\n",
        "#  ##### Meta Learner 3 ###\n",
        "# yhat_test_bag_a=np.concatenate((yhat_bag_1,yhat_bag_2,yhat_bag_3,yhat_bag_4,yhat_bag_5),axis=1)\n",
        "# yhat_test_bag=yhat_test_bag_a.reshape(yhat_bag_1.shape[0],yhat_bag_1.shape[1]*5)\n",
        "# print(yhat_test_bag.shape)\n",
        "\n",
        "# # test_y_meta_learner=test_X_1D.reshape(test_X_1D.shape[0],w*6)\n",
        "\n",
        "\n",
        "#  ##### Meta Learner 4 ###\n",
        "# yhat_test_baseboost_a=np.concatenate((yhat_baseboost_1,yhat_baseboost_2,yhat_baseboost_3,yhat_baseboost_4,yhat_baseboost_5),axis=1)\n",
        "# yhat_test_baseboost=yhat_test_baseboost_a.reshape(yhat_baseboost_1.shape[0],yhat_baseboost_1.shape[1]*5)\n",
        "# print(yhat_test_baseboost.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDLdhOTCQN6_"
      },
      "source": [
        "\n",
        "\n",
        "# meta_learner_2_model='/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/meta_learner_2.pkl'\n",
        "# with open(meta_learner_2_model, 'rb') as fp:\n",
        "#             meta_learner_2 = pickle.load(fp)\n",
        "\n",
        "# meta_learner_3_model='/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/meta_learner_3.pkl'\n",
        "# with open(meta_learner_3_model, 'rb') as fp:\n",
        "#             meta_learner_3 = pickle.load(fp)\n",
        "\n",
        "\n",
        "# meta_learner_4_model='/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/meta_learner_4.pkl'\n",
        "# with open(meta_learner_4_model, 'rb') as fp:\n",
        "#             meta_learner_4 = pickle.load(fp)\n",
        "\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBuSXbwMRPpT"
      },
      "source": [
        "# ### train Meta Learner results ###\n",
        "# # yhat_train_meta_2= meta_learner_2.predict(yhat_train_bagboost)\n",
        "# # yhat_train_meta_3= meta_learner_3.predict(yhat_train_bag)\n",
        "# # yhat_train_meta_4= meta_learner_4.predict(yhat_train_baseboost)\n",
        "\n",
        "# ### test Meta Learner results ###\n",
        "# yhat_meta_2= meta_learner_2.predict(yhat_test_bagboost)\n",
        "# yhat_meta_3= meta_learner_3.predict(yhat_test_bag)\n",
        "# yhat_meta_4= meta_learner_4.predict(yhat_test_baseboost)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYFzCcFvUjpy"
      },
      "source": [
        "# Comment during evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdNl1oZxRh_c"
      },
      "source": [
        "# #### Super Meta Learner ####\n",
        "\n",
        "# yhat_train_super_meta_a=np.concatenate((yhat_train_meta_2,yhat_train_meta_3,yhat_train_meta_4),axis=1)\n",
        "# yhat_train_super_meta=yhat_train_super_meta_a.reshape(yhat_train_meta_2.shape[0],yhat_train_meta_2.shape[1]*3)\n",
        "# print(yhat_train_super_meta.shape)\n",
        "\n",
        "# super_meta_learner=Ridge(alpha=0.001)\n",
        "# super_meta_learner.fit(yhat_train_super_meta,train_y_meta_learner)\n",
        "\n",
        "# pkl_filename = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/super_meta_learner.pkl'\n",
        "# with open(pkl_filename, 'wb') as file:\n",
        "#     pickle.dump(super_meta_learner, file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM_kmMjJVXDm"
      },
      "source": [
        "# yhat_test_super_meta_a=np.concatenate((yhat_meta_2,yhat_meta_3,yhat_meta_4),axis=1)\n",
        "# yhat_test_super_meta=yhat_test_super_meta_a.reshape(yhat_meta_2.shape[0],yhat_meta_2.shape[1]*3)\n",
        "# print(yhat_test_super_meta.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqeWoXluU3Ur"
      },
      "source": [
        "# #### Meta Learner Prediction ####\n",
        "\n",
        "# super_meta_model='/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/super_meta_learner.pkl'\n",
        "# with open(super_meta_model, 'rb') as fp:\n",
        "#             super_meta_learner = pickle.load(fp)\n",
        "\n",
        "# yhat_super_meta=super_meta_learner.predict(yhat_test_super_meta)\n",
        "\n",
        "# print(yhat_super_meta.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe_FzMbBQ-Rc"
      },
      "source": [
        "#  ##### Meta Learner Training data generation ###\n",
        "\n",
        "# test_meta=test_y.reshape((test_y.shape[0],w*6))\n",
        "\n",
        "# Base_prediction=np.concatenate((yhat_base_s,test_meta),axis=1)\n",
        "# Baseboost_prediction=np.concatenate((yhat_test_baseboost,test_meta),axis=1)\n",
        "# Bag_prediction=np.concatenate((yhat_test_bag,test_meta),axis=1)\n",
        "# Bagboost_prediction=np.concatenate((yhat_test_bagboost,test_meta),axis=1)\n",
        "\n",
        "\n",
        "# base_meta= pd.DataFrame(Base_prediction)\n",
        "# baseboost_meta= pd.DataFrame(Baseboost_prediction)\n",
        "# bag_meta= pd.DataFrame(Bag_prediction)\n",
        "# bagboost_meta= pd.DataFrame(Bagboost_prediction)\n",
        "\n",
        "\n",
        "# # saving the dataframe\n",
        "# base_meta.to_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/meta learner training/subject_1_base_meta.csv')\n",
        "# baseboost_meta.to_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/meta learner training/subject_1_baseboost_meta.csv')\n",
        "# bag_meta.to_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/meta learner training/subject_1_bag_meta.csv')\n",
        "# bagboost_meta.to_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/meta learner training/subject_1_bagboost_meta.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTcnuvtlq0H1"
      },
      "source": [
        "# Optimization and results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4fWVJfa3K4O"
      },
      "source": [
        "## Subject 1 ##\n",
        "\n",
        "base_meta_1=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/meta learner training/subject_1_base_meta.csv')\n",
        "baseboost_meta_1=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/meta learner training/subject_1_baseboost_meta.csv')\n",
        "bag_meta_1=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/meta learner training/subject_1_bag_meta.csv')\n",
        "bagboost_meta_1=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 1/meta learner training/subject_1_bagboost_meta.csv')\n",
        "\n",
        "## Subject 2 ##\n",
        "\n",
        "base_meta_2=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 2/meta learner training/subject_2_base_meta.csv')\n",
        "baseboost_meta_2=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 2/meta learner training/subject_2_baseboost_meta.csv')\n",
        "bag_meta_2=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 2/meta learner training/subject_2_bag_meta.csv')\n",
        "bagboost_meta_2=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 2/meta learner training/subject_2_bagboost_meta.csv')\n",
        "\n",
        "## Subject 3 ##\n",
        "\n",
        "base_meta_3=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 3/meta learner training/subject_3_base_meta.csv')\n",
        "baseboost_meta_3=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 3/meta learner training/subject_3_baseboost_meta.csv')\n",
        "bag_meta_3=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 3/meta learner training/subject_3_bag_meta.csv')\n",
        "bagboost_meta_3=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 3/meta learner training/subject_3_bagboost_meta.csv')\n",
        "\n",
        "\n",
        "## Subject 4 ##\n",
        "\n",
        "base_meta_4=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 4/meta learner training/subject_4_base_meta.csv')\n",
        "baseboost_meta_4=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 4/meta learner training/subject_4_baseboost_meta.csv')\n",
        "bag_meta_4=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 4/meta learner training/subject_4_bag_meta.csv')\n",
        "bagboost_meta_4=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 4/meta learner training/subject_4_bagboost_meta.csv')\n",
        "\n",
        "\n",
        "## Subject 5 ##\n",
        "\n",
        "base_meta_5=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 5/meta learner training/subject_5_base_meta.csv')\n",
        "baseboost_meta_5=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 5/meta learner training/subject_5_baseboost_meta.csv')\n",
        "bag_meta_5=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 5/meta learner training/subject_5_bag_meta.csv')\n",
        "bagboost_meta_5=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 5/meta learner training/subject_5_bagboost_meta.csv')\n",
        "\n",
        "\n",
        "## Subject 6 ##\n",
        "\n",
        "base_meta_6=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 6/meta learner training/subject_6_base_meta.csv')\n",
        "baseboost_meta_6=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 6/meta learner training/subject_6_baseboost_meta.csv')\n",
        "bag_meta_6=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 6/meta learner training/subject_6_bag_meta.csv')\n",
        "bagboost_meta_6=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 6/meta learner training/subject_6_bagboost_meta.csv')\n",
        "\n",
        "\n",
        "\n",
        "## Subject 7 ##\n",
        "\n",
        "base_meta_7=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 7/meta learner training/subject_7_base_meta.csv')\n",
        "baseboost_meta_7=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 7/meta learner training/subject_7_baseboost_meta.csv')\n",
        "bag_meta_7=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 7/meta learner training/subject_7_bag_meta.csv')\n",
        "bagboost_meta_7=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 7/meta learner training/subject_7_bagboost_meta.csv')\n",
        "\n",
        "\n",
        "\n",
        "## Subject 8 ##\n",
        "\n",
        "base_meta_8=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 8/meta learner training/subject_8_base_meta.csv')\n",
        "baseboost_meta_8=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 8/meta learner training/subject_8_baseboost_meta.csv')\n",
        "bag_meta_8=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 8/meta learner training/subject_8_bag_meta.csv')\n",
        "bagboost_meta_8=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 8/meta learner training/subject_8_bagboost_meta.csv')\n",
        "\n",
        "\n",
        "## Subject 9 ##\n",
        "\n",
        "base_meta_9=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 9/meta learner training/subject_9_base_meta.csv')\n",
        "baseboost_meta_9=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 9/meta learner training/subject_9_baseboost_meta.csv')\n",
        "bag_meta_9=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 9/meta learner training/subject_9_bag_meta.csv')\n",
        "bagboost_meta_9=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 9/meta learner training/subject_9_bagboost_meta.csv')\n",
        "\n",
        "\n",
        "## Subject 10 ##\n",
        "\n",
        "base_meta_10=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 10/meta learner training/subject_10_base_meta.csv')\n",
        "baseboost_meta_10=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 10/meta learner training/subject_10_baseboost_meta.csv')\n",
        "bag_meta_10=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 10/meta learner training/subject_10_bag_meta.csv')\n",
        "bagboost_meta_10=pd.read_csv('/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 10/meta learner training/subject_10_bagboost_meta.csv')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lg52IIVUCV56"
      },
      "source": [
        " #### Concatenation ####\n",
        "\n",
        "train_dataset_base_meta=np.concatenate((base_meta_2,base_meta_3,base_meta_4,base_meta_5,base_meta_6,base_meta_7,base_meta_8,base_meta_9,base_meta_10),axis=0)\n",
        "train_dataset_baseboost_meta=np.concatenate((baseboost_meta_2,baseboost_meta_3,baseboost_meta_4,baseboost_meta_5,baseboost_meta_6,baseboost_meta_7,baseboost_meta_8,baseboost_meta_9,baseboost_meta_10),axis=0)\n",
        "train_dataset_bag_meta=np.concatenate((bag_meta_2,bag_meta_3,bag_meta_4,bag_meta_5,bag_meta_6,bag_meta_7,bag_meta_8,bag_meta_9,bag_meta_10),axis=0)\n",
        "train_dataset_bagboost_meta=np.concatenate((bagboost_meta_2,bagboost_meta_3,bagboost_meta_4,bagboost_meta_5,bagboost_meta_6,bagboost_meta_7,bagboost_meta_8,bagboost_meta_9,bagboost_meta_10),axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSnhFfVGDwGm"
      },
      "source": [
        "### Features ####\n",
        "yhat_train_base=train_dataset_base_meta[:,1:2401]\n",
        "yhat_train_baseboost=train_dataset_baseboost_meta[:,1:2401]\n",
        "yhat_train_bag=train_dataset_bag_meta[:,1:2401]\n",
        "yhat_train_bagboost=train_dataset_bagboost_meta[:,1:2401]\n",
        "\n",
        "### Output ###\n",
        "train_y_meta_learner=train_dataset_baseboost_meta[:,2401:2881]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6AayO-nNvUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c7e357-ee3c-4618-c18a-159e27087c78"
      },
      "source": [
        "print(train_y_meta_learner.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11071, 480)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3rQNkzm6b8z"
      },
      "source": [
        "def prediction (yhat):\n",
        "\n",
        "\n",
        "    # test_o=test_y.reshape((test_y.shape[0]*w,6))\n",
        "    # yhat=yhat.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "    test_o=train_y_meta_learner.reshape((train_y_meta_learner.shape[0]*w,6))\n",
        "    yhat=yhat.reshape((train_y_meta_learner.shape[0]*w,6))\n",
        "\n",
        "    # print(test_o.shape,yhat.shape)\n",
        "\n",
        "    # test_o=test_y_ridge.reshape((test_y_ridge.shape[0]*w,6))\n",
        "    # yhat=yhat_1.reshape((test_y_ridge.shape[0]*w,6))\n",
        "\n",
        "\n",
        "    y_1_no=yhat[:,0]\n",
        "    y_2_no=yhat[:,1]\n",
        "    y_3_no=yhat[:,2]\n",
        "    y_4_no=yhat[:,3]\n",
        "    y_5_no=yhat[:,4]\n",
        "    y_6_no=yhat[:,5]\n",
        "    #y_7_no=yhat[:,6]\n",
        "    # y_8_no=yhat[:,7]\n",
        "    # y_9_no=yhat[:,8]\n",
        "    # y_10_no=yhat[:,9]\n",
        "    # y_11_no=yhat[:,10]\n",
        "    # y_12_no=yhat[:,11]\n",
        "    # y_13_no=yhat[:,12]\n",
        "    # y_14_no=yhat[:,13]\n",
        "    # y_15_no=yhat[:,14]\n",
        "    # y_16_no=yhat[:,15]\n",
        "    #\n",
        "\n",
        "    #### without filtering ###\n",
        "\n",
        "\n",
        "    # y_1=yhat[:,0]\n",
        "    # y_2=yhat[:,1]\n",
        "    # y_3=yhat[:,2]\n",
        "    # y_4=yhat[:,3]\n",
        "    # y_5=yhat[:,4]\n",
        "    # y_6=yhat[:,5]\n",
        "    # y_7=yhat[:,6]\n",
        "    # y_8=yhat[:,7]\n",
        "    # y_9=yhat[:,8]\n",
        "    # y_10=yhat[:,9]\n",
        "    # y_11=yhat[:,10]\n",
        "    # y_12=yhat[:,11]\n",
        "    # y_13=yhat[:,12]\n",
        "    # y_14=yhat[:,13]\n",
        "    # y_15=yhat[:,14]\n",
        "    # y_16=yhat[:,15]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    y_test_1=test_o[:,0]\n",
        "    y_test_2=test_o[:,1]\n",
        "    y_test_3=test_o[:,2]\n",
        "    y_test_4=test_o[:,3]\n",
        "    y_test_5=test_o[:,4]\n",
        "    y_test_6=test_o[:,5]\n",
        "    # y_test_7=test_o[:,6]\n",
        "    # y_test_8=test_o[:,7]\n",
        "    # y_test_9=test_o[:,8]\n",
        "    # y_test_10=test_o[:,9]\n",
        "    # y_test_11=test_o[:,10]\n",
        "    # y_test_12=test_o[:,11]\n",
        "    # y_test_13=test_o[:,12]\n",
        "    # y_test_14=test_o[:,13]\n",
        "    # y_test_15=test_o[:,14]\n",
        "    # y_test_16=test_o[:,15]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #print(y_1.shape,y_test_1.shape)\n",
        "\n",
        "\n",
        "\n",
        "    cutoff=6\n",
        "    fs=100\n",
        "    order=2\n",
        "\n",
        "    nyq = 0.5 * fs\n",
        "    ## filtering data ##\n",
        "    def butter_lowpass_filter(data, cutoff, fs, order):\n",
        "        normal_cutoff = cutoff / nyq\n",
        "        # Get the filter coefficients\n",
        "        b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "        y = filtfilt(b, a, data)\n",
        "        return y\n",
        "\n",
        "\n",
        "\n",
        "    y_1= butter_lowpass_filter(y_1_no, cutoff, fs, order)\n",
        "    y_2= butter_lowpass_filter(y_2_no, cutoff, fs, order)\n",
        "    y_3= butter_lowpass_filter(y_3_no, cutoff, fs, order)\n",
        "    y_4= butter_lowpass_filter(y_4_no, cutoff, fs, order)\n",
        "    y_5= butter_lowpass_filter(y_5_no, cutoff, fs, order)\n",
        "    y_6= butter_lowpass_filter(y_6_no, cutoff, fs, order)\n",
        "    # # y_7= butter_lowpass_filter(y_7_no, cutoff, fs, order)\n",
        "    # y_8= butter_lowpass_filter(y_8_no, cutoff, fs, order)\n",
        "    # y_9= butter_lowpass_filter(y_9_no, cutoff, fs, order)\n",
        "    # y_10= butter_lowpass_filter(y_10_no, cutoff, fs, order)\n",
        "    # y_11= butter_lowpass_filter(y_11_no, cutoff, fs, order)\n",
        "    # y_12= butter_lowpass_filter(y_12_no, cutoff, fs, order)\n",
        "    # y_13= butter_lowpass_filter(y_13_no, cutoff, fs, order)\n",
        "    # y_14= butter_lowpass_filter(y_14_no, cutoff, fs, order)\n",
        "    # y_15= butter_lowpass_filter(y_15_no, cutoff, fs, order)\n",
        "    # y_16= butter_lowpass_filter(y_16_no, cutoff, fs, order)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ###calculate RMSE\n",
        "\n",
        "    rmse_1 =np.sqrt(mean_squared_error(y_test_1,y_1))\n",
        "    rmse_2 =np.sqrt(mean_squared_error(y_test_2,y_2))\n",
        "    rmse_3 =np.sqrt(mean_squared_error(y_test_3,y_3))\n",
        "    rmse_4 =np.sqrt(mean_squared_error(y_test_4,y_4))\n",
        "    rmse_5 =np.sqrt(mean_squared_error(y_test_5,y_5))\n",
        "    rmse_6 =np.sqrt(mean_squared_error(y_test_6,y_6))\n",
        "    # rmse_7 =np.sqrt(mean_squared_error(y_test_7,y_7))\n",
        "    # rmse_8 =np.sqrt(mean_squared_error(y_test_8,y_8))\n",
        "    # rmse_9 =np.sqrt(mean_squared_error(y_test_9,y_9))\n",
        "    # rmse_10 =np.sqrt(mean_squared_error(y_test_10,y_10))\n",
        "    # rmse_11 =np.sqrt(mean_squared_error(y_test_11,y_11))\n",
        "    # rmse_12 =np.sqrt(mean_squared_error(y_test_12,y_12))\n",
        "    # rmse_13 =np.sqrt(mean_squared_error(y_test_13,y_13))\n",
        "    # rmse_14 =np.sqrt(mean_squared_error(y_test_14,y_14))\n",
        "    # rmse_15 =np.sqrt(mean_squared_error(y_test_15,y_15))\n",
        "    # rmse_16 =np.sqrt(mean_squared_error(y_test_16,y_16))\n",
        "\n",
        "    # calculate RMSE\n",
        "    # rmse_1 =(np.sqrt(mean_squared_error(y_test_1,y_1)))*(180/math.pi)\n",
        "    # rmse_2 =(np.sqrt(mean_squared_error(y_test_2,y_2)))*(180/math.pi)\n",
        "    # rmse_3 =(np.sqrt(mean_squared_error(y_test_3,y_3)))*(180/math.pi)\n",
        "    # rmse_4 =(np.sqrt(mean_squared_error(y_test_4,y_4)))*(180/math.pi)\n",
        "    # rmse_5 =(np.sqrt(mean_squared_error(y_test_5,y_5)))*(180/math.pi)\n",
        "    # rmse_6 =(np.sqrt(mean_squared_error(y_test_6,y_6)))*(180/math.pi)\n",
        "\n",
        "\n",
        "    ##Calculate NRMSE\n",
        "\n",
        "    # rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_1)-min(y_1)))*100\n",
        "    # rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_1)-min(y_1)))*100\n",
        "    # rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_1)-min(y_1)))*100\n",
        "    # rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_1)-min(y_1)))*100\n",
        "    # rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_1)-min(y_1)))*100\n",
        "    # rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_1)-min(y_1)))*100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # print(rmse_1)\n",
        "    # print(rmse_2)\n",
        "    # print(rmse_3)\n",
        "    # print(rmse_4)\n",
        "    # print(rmse_5)\n",
        "    # print(rmse_6)\n",
        "    # print('Test RMSE: %.3f' % rmse_7)\n",
        "    # print('Test RMSE: %.3f' % rmse_8)\n",
        "    # print('Test RMSE: %.3f' % rmse_9)\n",
        "    # print('Test RMSE: %.3f' % rmse_10)\n",
        "    # print('Test RMSE: %.3f' % rmse_11)\n",
        "    # print('Test RMSE: %.3f' % rmse_12)\n",
        "    # print('Test RMSE: %.3f' % rmse_13)\n",
        "    # print('Test RMSE: %.3f' % rmse_14)\n",
        "    # print('Test RMSE: %.3f' % rmse_15)\n",
        "    # print('Test RMSE: %.3f' % rmse_16)\n",
        "\n",
        "\n",
        "    # from scipy.stats import pearsonr\n",
        "\n",
        "    # p_1, _ =pearsonr(y_1, y_test_1)\n",
        "    # p_2, _ =pearsonr(y_2, y_test_2)\n",
        "    # p_3, _ =pearsonr(y_3, y_test_3)\n",
        "    # p_4, _ =pearsonr(y_4, y_test_4)\n",
        "    # p_5, _ =pearsonr(y_5, y_test_5)\n",
        "    # p_6, _ =pearsonr(y_6, y_test_6)\n",
        "\n",
        "\n",
        "\n",
        "    p_1=np.corrcoef(y_1, y_test_1)[0, 1]\n",
        "    p_2=np.corrcoef(y_2, y_test_2)[0, 1]\n",
        "    p_3=np.corrcoef(y_3, y_test_3)[0, 1]\n",
        "    p_4=np.corrcoef(y_4, y_test_4)[0, 1]\n",
        "    p_5=np.corrcoef(y_5, y_test_5)[0, 1]\n",
        "    p_6=np.corrcoef(y_6, y_test_6)[0, 1]\n",
        "    # p_7=np.corrcoef(y_7, y_test_7)[0, 1]\n",
        "    # p_8=np.corrcoef(y_8, y_test_8)[0, 1]\n",
        "    # p_9=np.corrcoef(y_9, y_test_9)[0, 1]\n",
        "    # p_10=np.corrcoef(y_10, y_test_10)[0, 1]\n",
        "    # p_11=np.corrcoef(y_11, y_test_11)[0, 1]\n",
        "    # p_12=np.corrcoef(y_12, y_test_12)[0, 1]\n",
        "    # p_13=np.corrcoef(y_13, y_test_13)[0, 1]\n",
        "    # p_14=np.corrcoef(y_14, y_test_14)[0, 1]\n",
        "    # p_15=np.corrcoef(y_15, y_test_15)[0, 1]\n",
        "    # p_16=np.corrcoef(y_16, y_test_16)[0, 1]\n",
        "\n",
        "    #print(p_1,p_2,p_3,p_4,p_5,p_6,p_7,p_8,p_9,p_10,p_11,p_12,p_13,p_14,p_15,p_16)\n",
        "\n",
        "    #print(p_1,p_2,p_3,p_4,p_5,p_6,p_7,p_8,p_9,p_10)\n",
        "\n",
        "    # print(\"\\n\")\n",
        "    # print(p_1)\n",
        "    # print(p_2)\n",
        "    # print(p_3)\n",
        "    # print(p_4)\n",
        "    # print(p_5)\n",
        "    # print(p_6)\n",
        "    #print(p_1,p_2,p_3,p_4)\n",
        "\n",
        "\n",
        "                ### Correlation ###\n",
        "    #p=np.array([p_1,p_2,p_3,p_4,p_5,p_6,p_7,p_8,p_9,p_10,p_11,p_12,p_13,p_14,p_15,p_16])\n",
        "    #p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])\n",
        "\n",
        "    #p=pd.DataFrame([p_1,p_2,p_3,p_4,p_5,p_6,p_7,p_8,p_9,p_10])\n",
        "\n",
        "    p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])\n",
        "\n",
        "    #p=np.array([p_1,p_2,p_3,p_4])\n",
        "\n",
        "\n",
        "\n",
        "        #### Mean and standard deviation ####\n",
        "\n",
        "    #rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6,rmse_7,rmse_8,rmse_9,rmse_10])\n",
        "    rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])\n",
        "\n",
        "    #rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4])\n",
        "\n",
        "    m=statistics.mean(rmse)\n",
        "    SD=statistics.stdev(rmse)\n",
        "    # print('Mean: %.3f' % m,'+/- %.3f' %SD)\n",
        "\n",
        "    m_c=statistics.mean(p)\n",
        "    SD_c=statistics.stdev(p)\n",
        "    # print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)\n",
        "\n",
        "\n",
        "    return m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZlr-GDGOa-e"
      },
      "source": [
        "yhat_train_base_1=yhat_train_base[:,0:480]\n",
        "yhat_train_base_2=yhat_train_base[:,480:960]\n",
        "yhat_train_base_3=yhat_train_base[:,960:1440]\n",
        "yhat_train_base_4=yhat_train_base[:,1440:1920]\n",
        "yhat_train_base_5=yhat_train_base[:,1920:2400]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjQ9aZ5lJ_EW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "603416f4-4362-4748-aadc-09174395bd3d"
      },
      "source": [
        "print(yhat_train_base_1.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11071, 480)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeYnwgIXB-PV"
      },
      "source": [
        "yhat_train_baseboost_1=yhat_train_baseboost[:,0:480]\n",
        "yhat_train_baseboost_2=yhat_train_baseboost[:,480:960]\n",
        "yhat_train_baseboost_3=yhat_train_baseboost[:,960:1440]\n",
        "yhat_train_baseboost_4=yhat_train_baseboost[:,1440:1920]\n",
        "yhat_train_baseboost_5=yhat_train_baseboost[:,1920:2400]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8_0ikqSCGuK"
      },
      "source": [
        "yhat_train_bag_1=yhat_train_bag[:,0:480]\n",
        "yhat_train_bag_2=yhat_train_bag[:,480:960]\n",
        "yhat_train_bag_3=yhat_train_bag[:,960:1440]\n",
        "yhat_train_bag_4=yhat_train_bag[:,1440:1920]\n",
        "yhat_train_bag_5=yhat_train_bag[:,1920:2400]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS3uhR8USQy7"
      },
      "source": [
        "yhat_train_bagboost_1=yhat_train_bagboost[:,0:480]\n",
        "yhat_train_bagboost_2=yhat_train_bagboost[:,480:960]\n",
        "yhat_train_bagboost_3=yhat_train_bagboost[:,960:1440]\n",
        "yhat_train_bagboost_4=yhat_train_bagboost[:,1440:1920]\n",
        "yhat_train_bagboost_5=yhat_train_bagboost[:,1920:2400]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ibchCeuKIPL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7343d09b-737b-4b0e-fd2c-1e87d40a8dc4"
      },
      "source": [
        "print(yhat_train_bagboost_5.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11071, 480)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhUIF4g8O4-9"
      },
      "source": [
        "C_1=np.array([yhat_train_base_1,yhat_train_base_2,yhat_train_base_3,yhat_train_base_4,yhat_train_base_5])\n",
        "\n",
        "G_1=np.array([yhat_train_baseboost_1,yhat_train_baseboost_2,yhat_train_baseboost_3,yhat_train_baseboost_4,yhat_train_baseboost_5])\n",
        "\n",
        "D_1=np.array([yhat_train_bag_1,yhat_train_bag_2,yhat_train_bag_3,yhat_train_bag_4,yhat_train_bag_5])\n",
        "\n",
        "E_1=np.array([yhat_train_bagboost_1,yhat_train_bagboost_2,yhat_train_bagboost_3,yhat_train_bagboost_4,yhat_train_bagboost_5])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "H_2=np.array([yhat_train_baseboost_1,yhat_train_baseboost_2,yhat_train_baseboost_3,yhat_train_baseboost_4,yhat_train_baseboost_5,yhat_train_bag_1,yhat_train_bag_2,yhat_train_bag_3,yhat_train_bag_4,yhat_train_bag_5,yhat_train_bagboost_1,yhat_train_bagboost_2,yhat_train_bagboost_3,yhat_train_bagboost_4,yhat_train_bagboost_5])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBbfzEKtrtbB"
      },
      "source": [
        "def objective_base(weights):\n",
        "\n",
        "    yhat= np.average(C_1, axis=0, weights=weights)\n",
        "    m= prediction (yhat)\n",
        "\n",
        "    return m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaIRS3kJon_q"
      },
      "source": [
        "def objective_baseboost(weights):\n",
        "\n",
        "    yhat= np.average(G_1, axis=0, weights=weights)\n",
        "    m= prediction (yhat)\n",
        "\n",
        "    return m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KPFsPZJooT8"
      },
      "source": [
        "def objective_bag(weights):\n",
        "\n",
        "    yhat= np.average(D_1, axis=0, weights=weights)\n",
        "    m= prediction (yhat)\n",
        "\n",
        "    return m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GIeklFnpCvh"
      },
      "source": [
        "def objective_bagboost(weights):\n",
        "\n",
        "    yhat= np.average(E_1, axis=0, weights=weights)\n",
        "    m= prediction (yhat)\n",
        "\n",
        "    return m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7HAVOhNFM-V"
      },
      "source": [
        "def objective_super(weights):\n",
        "\n",
        "    yhat= np.average(H_2, axis=0, weights=weights)\n",
        "    m= prediction (yhat)\n",
        "\n",
        "    return m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uOYskG2rwXU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e09f59a6-0b57-41ca-dd3f-6b64e881434c"
      },
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "# I define initial weights from which the algorithm will try searching a minima\n",
        "# I usually set the initial weigths to be the same for each columns, but they\n",
        "# can be set randomly\n",
        "w0 = np.empty(C_1.shape[0])\n",
        "w0.fill(1/C_1.shape[0])\n",
        "# w0.fill(1)\n",
        "\n",
        "# I define bounds, i.e. lower and upper values of weights.\n",
        "# I want the weights to be between 0 and 1.\n",
        "bounds = [(0,1)] * C_1.shape[0]\n",
        "\n",
        "# I set some constraints. Here, I want the sum of the weights to be equal to 1\n",
        "cons = [{'type': 'eq',\n",
        "         'fun': lambda w: w.sum() - 1}]\n",
        "\n",
        "# Then, I try to find the weights that will minimize my objective function.\n",
        "# There are several solvers (methods) to choose from. I use SLSQP because\n",
        "# it can handle constraints.\n",
        "res = minimize(objective_base,\n",
        "               w0,\n",
        "               method='SLSQP',\n",
        "               bounds=bounds,\n",
        "               options={'disp':True, 'maxiter':10000},\n",
        "               constraints=cons)\n",
        "\n",
        "best_weights_base=res.x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimization terminated successfully.    (Exit mode 0)\n",
            "            Current function value: 4.48536790020821\n",
            "            Iterations: 5\n",
            "            Function evaluations: 35\n",
            "            Gradient evaluations: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fuw-W_twpGm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f96d2e9a-90b6-4f7a-b27c-cd96827f3e5b"
      },
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "# I define initial weights from which the algorithm will try searching a minima\n",
        "# I usually set the initial weigths to be the same for each columns, but they\n",
        "# can be set randomly\n",
        "w0 = np.empty(G_1.shape[0])\n",
        "w0.fill(1/G_1.shape[0])\n",
        "# w0.fill(1)\n",
        "\n",
        "# I define bounds, i.e. lower and upper values of weights.\n",
        "# I want the weights to be between 0 and 1.\n",
        "bounds = [(0,1)] * G_1.shape[0]\n",
        "\n",
        "# I set some constraints. Here, I want the sum of the weights to be equal to 1\n",
        "cons = [{'type': 'eq',\n",
        "         'fun': lambda w: w.sum() - 1}]\n",
        "\n",
        "# Then, I try to find the weights that will minimize my objective function.\n",
        "# There are several solvers (methods) to choose from. I use SLSQP because\n",
        "# it can handle constraints.\n",
        "res = minimize(objective_baseboost,\n",
        "               w0,\n",
        "               method='SLSQP',\n",
        "               bounds=bounds,\n",
        "               options={'disp':True, 'maxiter':10000},\n",
        "               constraints=cons)\n",
        "\n",
        "best_weights_baseboost = res.x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimization terminated successfully.    (Exit mode 0)\n",
            "            Current function value: 4.498336001245719\n",
            "            Iterations: 5\n",
            "            Function evaluations: 35\n",
            "            Gradient evaluations: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Hc3ZsBppIWd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffba4304-8990-4b21-e504-112aa620ab7f"
      },
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "# I define initial weights from which the algorithm will try searching a minima\n",
        "# I usually set the initial weigths to be the same for each columns, but they\n",
        "# can be set randomly\n",
        "w0 = np.empty(D_1.shape[0])\n",
        "w0.fill(1/D_1.shape[0])\n",
        "# w0.fill(1)\n",
        "\n",
        "# I define bounds, i.e. lower and upper values of weights.\n",
        "# I want the weights to be between 0 and 1.\n",
        "bounds = [(0,1)] * D_1.shape[0]\n",
        "\n",
        "# I set some constraints. Here, I want the sum of the weights to be equal to 1\n",
        "cons = [{'type': 'eq',\n",
        "         'fun': lambda w: w.sum() - 1}]\n",
        "\n",
        "# Then, I try to find the weights that will minimize my objective function.\n",
        "# There are several solvers (methods) to choose from. I use SLSQP because\n",
        "# it can handle constraints.\n",
        "res = minimize(objective_bag,\n",
        "               w0,\n",
        "               method='SLSQP',\n",
        "               bounds=bounds,\n",
        "               options={'disp':True, 'maxiter':10000},\n",
        "               constraints=cons)\n",
        "\n",
        "best_weights_bag = res.x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimization terminated successfully.    (Exit mode 0)\n",
            "            Current function value: 4.466495077516521\n",
            "            Iterations: 9\n",
            "            Function evaluations: 63\n",
            "            Gradient evaluations: 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUC2-ds9pIwW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ec70b70-777e-448d-de07-aef472b6a14a"
      },
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "# I define initial weights from which the algorithm will try searching a minima\n",
        "# I usually set the initial weigths to be the same for each columns, but they\n",
        "# can be set randomly\n",
        "w0 = np.empty(E_1.shape[0])\n",
        "w0.fill(1/E_1.shape[0])\n",
        "# w0.fill(1)\n",
        "\n",
        "# I define bounds, i.e. lower and upper values of weights.\n",
        "# I want the weights to be between 0 and 1.\n",
        "bounds = [(0,1)] * E_1.shape[0]\n",
        "\n",
        "# I set some constraints. Here, I want the sum of the weights to be equal to 1\n",
        "cons = [{'type': 'eq',\n",
        "         'fun': lambda w: w.sum() - 1}]\n",
        "\n",
        "# Then, I try to find the weights that will minimize my objective function.\n",
        "# There are several solvers (methods) to choose from. I use SLSQP because\n",
        "# it can handle constraints.\n",
        "res = minimize(objective_bagboost,\n",
        "               w0,\n",
        "               method='SLSQP',\n",
        "               bounds=bounds,\n",
        "               options={'disp':True, 'maxiter':10000},\n",
        "               constraints=cons)\n",
        "\n",
        "best_weights_bagboost = res.x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimization terminated successfully.    (Exit mode 0)\n",
            "            Current function value: 4.471237332794907\n",
            "            Iterations: 11\n",
            "            Function evaluations: 77\n",
            "            Gradient evaluations: 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN4wxQ03GRcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75854f88-5f54-447e-9c0a-38b289cce465"
      },
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "# I define initial weights from which the algorithm will try searching a minima\n",
        "# I usually set the initial weigths to be the same for each columns, but they\n",
        "# can be set randomly\n",
        "w0 = np.empty(H_2.shape[0])\n",
        "w0.fill(1/H_2.shape[0])\n",
        "# w0.fill(1)\n",
        "\n",
        "# I define bounds, i.e. lower and upper values of weights.\n",
        "# I want the weights to be between 0 and 1.\n",
        "bounds = [(0,1)] * H_2.shape[0]\n",
        "\n",
        "# I set some constraints. Here, I want the sum of the weights to be equal to 1\n",
        "cons = [{'type': 'eq',\n",
        "         'fun': lambda w: w.sum() - 1}]\n",
        "\n",
        "# Then, I try to find the weights that will minimize my objective function.\n",
        "# There are several solvers (methods) to choose from. I use SLSQP because\n",
        "# it can handle constraints.\n",
        "res = minimize(objective_super,\n",
        "               w0,\n",
        "               method='SLSQP',\n",
        "               bounds=bounds,\n",
        "               options={'disp':True, 'maxiter':10000},\n",
        "               constraints=cons)\n",
        "\n",
        "best_weights_super = res.x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimization terminated successfully.    (Exit mode 0)\n",
            "            Current function value: 4.440122840011372\n",
            "            Iterations: 16\n",
            "            Function evaluations: 273\n",
            "            Gradient evaluations: 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x2e38HmHews"
      },
      "source": [
        "def prediction_test (yhat):\n",
        "\n",
        "\n",
        "    test_o=test_y.reshape((test_y.shape[0]*w,6))\n",
        "    yhat=yhat.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "    # test_o=train_y_5.reshape((train_y_5.shape[0]*w,6))\n",
        "    # yhat=yhat.reshape((train_y_5.shape[0]*w,6))\n",
        "\n",
        "    print(test_o.shape,yhat.shape)\n",
        "\n",
        "    # test_o=test_y_ridge.reshape((test_y_ridge.shape[0]*w,6))\n",
        "    # yhat=yhat_1.reshape((test_y_ridge.shape[0]*w,6))\n",
        "\n",
        "\n",
        "    y_1_no=yhat[:,0]\n",
        "    y_2_no=yhat[:,1]\n",
        "    y_3_no=yhat[:,2]\n",
        "    y_4_no=yhat[:,3]\n",
        "    y_5_no=yhat[:,4]\n",
        "    y_6_no=yhat[:,5]\n",
        "    #y_7_no=yhat[:,6]\n",
        "    # y_8_no=yhat[:,7]\n",
        "    # y_9_no=yhat[:,8]\n",
        "    # y_10_no=yhat[:,9]\n",
        "    # y_11_no=yhat[:,10]\n",
        "    # y_12_no=yhat[:,11]\n",
        "    # y_13_no=yhat[:,12]\n",
        "    # y_14_no=yhat[:,13]\n",
        "    # y_15_no=yhat[:,14]\n",
        "    # y_16_no=yhat[:,15]\n",
        "    #\n",
        "\n",
        "    #### without filtering ###\n",
        "\n",
        "\n",
        "    # y_1=yhat[:,0]\n",
        "    # y_2=yhat[:,1]\n",
        "    # y_3=yhat[:,2]\n",
        "    # y_4=yhat[:,3]\n",
        "    # y_5=yhat[:,4]\n",
        "    # y_6=yhat[:,5]\n",
        "    # y_7=yhat[:,6]\n",
        "    # y_8=yhat[:,7]\n",
        "    # y_9=yhat[:,8]\n",
        "    # y_10=yhat[:,9]\n",
        "    # y_11=yhat[:,10]\n",
        "    # y_12=yhat[:,11]\n",
        "    # y_13=yhat[:,12]\n",
        "    # y_14=yhat[:,13]\n",
        "    # y_15=yhat[:,14]\n",
        "    # y_16=yhat[:,15]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    y_test_1=test_o[:,0]\n",
        "    y_test_2=test_o[:,1]\n",
        "    y_test_3=test_o[:,2]\n",
        "    y_test_4=test_o[:,3]\n",
        "    y_test_5=test_o[:,4]\n",
        "    y_test_6=test_o[:,5]\n",
        "    # y_test_7=test_o[:,6]\n",
        "    # y_test_8=test_o[:,7]\n",
        "    # y_test_9=test_o[:,8]\n",
        "    # y_test_10=test_o[:,9]\n",
        "    # y_test_11=test_o[:,10]\n",
        "    # y_test_12=test_o[:,11]\n",
        "    # y_test_13=test_o[:,12]\n",
        "    # y_test_14=test_o[:,13]\n",
        "    # y_test_15=test_o[:,14]\n",
        "    # y_test_16=test_o[:,15]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #print(y_1.shape,y_test_1.shape)\n",
        "\n",
        "\n",
        "\n",
        "    cutoff=6\n",
        "    fs=100\n",
        "    order=2\n",
        "\n",
        "    nyq = 0.5 * fs\n",
        "    ## filtering data ##\n",
        "    def butter_lowpass_filter(data, cutoff, fs, order):\n",
        "        normal_cutoff = cutoff / nyq\n",
        "        # Get the filter coefficients\n",
        "        b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "        y = filtfilt(b, a, data)\n",
        "        return y\n",
        "\n",
        "\n",
        "\n",
        "    y_1= butter_lowpass_filter(y_1_no, cutoff, fs, order)\n",
        "    y_2= butter_lowpass_filter(y_2_no, cutoff, fs, order)\n",
        "    y_3= butter_lowpass_filter(y_3_no, cutoff, fs, order)\n",
        "    y_4= butter_lowpass_filter(y_4_no, cutoff, fs, order)\n",
        "    y_5= butter_lowpass_filter(y_5_no, cutoff, fs, order)\n",
        "    y_6= butter_lowpass_filter(y_6_no, cutoff, fs, order)\n",
        "    # # y_7= butter_lowpass_filter(y_7_no, cutoff, fs, order)\n",
        "    # y_8= butter_lowpass_filter(y_8_no, cutoff, fs, order)\n",
        "    # y_9= butter_lowpass_filter(y_9_no, cutoff, fs, order)\n",
        "    # y_10= butter_lowpass_filter(y_10_no, cutoff, fs, order)\n",
        "    # y_11= butter_lowpass_filter(y_11_no, cutoff, fs, order)\n",
        "    # y_12= butter_lowpass_filter(y_12_no, cutoff, fs, order)\n",
        "    # y_13= butter_lowpass_filter(y_13_no, cutoff, fs, order)\n",
        "    # y_14= butter_lowpass_filter(y_14_no, cutoff, fs, order)\n",
        "    # y_15= butter_lowpass_filter(y_15_no, cutoff, fs, order)\n",
        "    # y_16= butter_lowpass_filter(y_16_no, cutoff, fs, order)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ###calculate RMSE\n",
        "\n",
        "    rmse_1 =np.sqrt(mean_squared_error(y_test_1,y_1))\n",
        "    rmse_2 =np.sqrt(mean_squared_error(y_test_2,y_2))\n",
        "    rmse_3 =np.sqrt(mean_squared_error(y_test_3,y_3))\n",
        "    rmse_4 =np.sqrt(mean_squared_error(y_test_4,y_4))\n",
        "    rmse_5 =np.sqrt(mean_squared_error(y_test_5,y_5))\n",
        "    rmse_6 =np.sqrt(mean_squared_error(y_test_6,y_6))\n",
        "    # rmse_7 =np.sqrt(mean_squared_error(y_test_7,y_7))\n",
        "    # rmse_8 =np.sqrt(mean_squared_error(y_test_8,y_8))\n",
        "    # rmse_9 =np.sqrt(mean_squared_error(y_test_9,y_9))\n",
        "    # rmse_10 =np.sqrt(mean_squared_error(y_test_10,y_10))\n",
        "    # rmse_11 =np.sqrt(mean_squared_error(y_test_11,y_11))\n",
        "    # rmse_12 =np.sqrt(mean_squared_error(y_test_12,y_12))\n",
        "    # rmse_13 =np.sqrt(mean_squared_error(y_test_13,y_13))\n",
        "    # rmse_14 =np.sqrt(mean_squared_error(y_test_14,y_14))\n",
        "    # rmse_15 =np.sqrt(mean_squared_error(y_test_15,y_15))\n",
        "    # rmse_16 =np.sqrt(mean_squared_error(y_test_16,y_16))\n",
        "\n",
        "    # calculate RMSE\n",
        "    # rmse_1 =(np.sqrt(mean_squared_error(y_test_1,y_1)))*(180/math.pi)\n",
        "    # rmse_2 =(np.sqrt(mean_squared_error(y_test_2,y_2)))*(180/math.pi)\n",
        "    # rmse_3 =(np.sqrt(mean_squared_error(y_test_3,y_3)))*(180/math.pi)\n",
        "    # rmse_4 =(np.sqrt(mean_squared_error(y_test_4,y_4)))*(180/math.pi)\n",
        "    # rmse_5 =(np.sqrt(mean_squared_error(y_test_5,y_5)))*(180/math.pi)\n",
        "    # rmse_6 =(np.sqrt(mean_squared_error(y_test_6,y_6)))*(180/math.pi)\n",
        "\n",
        "\n",
        "    ##Calculate NRMSE\n",
        "\n",
        "    # rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_1)-min(y_1)))*100\n",
        "    # rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_1)-min(y_1)))*100\n",
        "    # rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_1)-min(y_1)))*100\n",
        "    # rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_1)-min(y_1)))*100\n",
        "    # rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_1)-min(y_1)))*100\n",
        "    # rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_1)-min(y_1)))*100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(rmse_1)\n",
        "    print(rmse_2)\n",
        "    print(rmse_3)\n",
        "    print(rmse_4)\n",
        "    print(rmse_5)\n",
        "    print(rmse_6)\n",
        "    # print('Test RMSE: %.3f' % rmse_7)\n",
        "    # print('Test RMSE: %.3f' % rmse_8)\n",
        "    # print('Test RMSE: %.3f' % rmse_9)\n",
        "    # print('Test RMSE: %.3f' % rmse_10)\n",
        "    # print('Test RMSE: %.3f' % rmse_11)\n",
        "    # print('Test RMSE: %.3f' % rmse_12)\n",
        "    # print('Test RMSE: %.3f' % rmse_13)\n",
        "    # print('Test RMSE: %.3f' % rmse_14)\n",
        "    # print('Test RMSE: %.3f' % rmse_15)\n",
        "    # print('Test RMSE: %.3f' % rmse_16)\n",
        "\n",
        "\n",
        "    # from scipy.stats import pearsonr\n",
        "\n",
        "    # p_1, _ =pearsonr(y_1, y_test_1)\n",
        "    # p_2, _ =pearsonr(y_2, y_test_2)\n",
        "    # p_3, _ =pearsonr(y_3, y_test_3)\n",
        "    # p_4, _ =pearsonr(y_4, y_test_4)\n",
        "    # p_5, _ =pearsonr(y_5, y_test_5)\n",
        "    # p_6, _ =pearsonr(y_6, y_test_6)\n",
        "\n",
        "\n",
        "\n",
        "    p_1=np.corrcoef(y_1, y_test_1)[0, 1]\n",
        "    p_2=np.corrcoef(y_2, y_test_2)[0, 1]\n",
        "    p_3=np.corrcoef(y_3, y_test_3)[0, 1]\n",
        "    p_4=np.corrcoef(y_4, y_test_4)[0, 1]\n",
        "    p_5=np.corrcoef(y_5, y_test_5)[0, 1]\n",
        "    p_6=np.corrcoef(y_6, y_test_6)[0, 1]\n",
        "    # p_7=np.corrcoef(y_7, y_test_7)[0, 1]\n",
        "    # p_8=np.corrcoef(y_8, y_test_8)[0, 1]\n",
        "    # p_9=np.corrcoef(y_9, y_test_9)[0, 1]\n",
        "    # p_10=np.corrcoef(y_10, y_test_10)[0, 1]\n",
        "    # p_11=np.corrcoef(y_11, y_test_11)[0, 1]\n",
        "    # p_12=np.corrcoef(y_12, y_test_12)[0, 1]\n",
        "    # p_13=np.corrcoef(y_13, y_test_13)[0, 1]\n",
        "    # p_14=np.corrcoef(y_14, y_test_14)[0, 1]\n",
        "    # p_15=np.corrcoef(y_15, y_test_15)[0, 1]\n",
        "    # p_16=np.corrcoef(y_16, y_test_16)[0, 1]\n",
        "\n",
        "    #print(p_1,p_2,p_3,p_4,p_5,p_6,p_7,p_8,p_9,p_10,p_11,p_12,p_13,p_14,p_15,p_16)\n",
        "\n",
        "    #print(p_1,p_2,p_3,p_4,p_5,p_6,p_7,p_8,p_9,p_10)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(p_1)\n",
        "    print(p_2)\n",
        "    print(p_3)\n",
        "    print(p_4)\n",
        "    print(p_5)\n",
        "    print(p_6)\n",
        "    #print(p_1,p_2,p_3,p_4)\n",
        "\n",
        "\n",
        "                ### Correlation ###\n",
        "    #p=np.array([p_1,p_2,p_3,p_4,p_5,p_6,p_7,p_8,p_9,p_10,p_11,p_12,p_13,p_14,p_15,p_16])\n",
        "    #p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])\n",
        "\n",
        "    #p=pd.DataFrame([p_1,p_2,p_3,p_4,p_5,p_6,p_7,p_8,p_9,p_10])\n",
        "\n",
        "    p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])\n",
        "\n",
        "    #p=np.array([p_1,p_2,p_3,p_4])\n",
        "\n",
        "\n",
        "\n",
        "        #### Mean and standard deviation ####\n",
        "\n",
        "    #rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6,rmse_7,rmse_8,rmse_9,rmse_10])\n",
        "    rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])\n",
        "\n",
        "    #rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4])\n",
        "\n",
        "    m=statistics.mean(rmse)\n",
        "    SD=statistics.stdev(rmse)\n",
        "    print('Mean: %.3f' % m,'+/- %.3f' %SD)\n",
        "\n",
        "    m_c=statistics.mean(p)\n",
        "    SD_c=statistics.stdev(p)\n",
        "    print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)\n",
        "\n",
        "\n",
        "    return rmse,p,m,m_c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMrjEImM6b82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41e844af-967f-445e-cc45-21157b5008b2"
      },
      "source": [
        "#  #### All 16 angles prediction  ####\n",
        "\n",
        "\n",
        "C_1_test=np.array([yhat_base_1,yhat_base_2,yhat_base_3,yhat_base_4,yhat_base_5])\n",
        "\n",
        "C_test=np.average(C_1_test, weights = best_weights_baseboost,axis=0)\n",
        "\n",
        "G_1_test=np.array([yhat_baseboost_1,yhat_baseboost_2,yhat_baseboost_3,yhat_baseboost_4,yhat_baseboost_5])\n",
        "\n",
        "G_test=np.average(G_1_test, weights = best_weights_baseboost,axis=0)\n",
        "\n",
        "\n",
        "D_1_test=np.array([yhat_bag_1,yhat_bag_2,yhat_bag_3,yhat_bag_4,yhat_bag_5])\n",
        "\n",
        "D_test=np.average(D_1_test, weights = best_weights_bag,axis=0)\n",
        "\n",
        "E_1_test=np.array([yhat_bagboost_1,yhat_bagboost_2,yhat_bagboost_3,yhat_bagboost_4,yhat_bagboost_5])\n",
        "\n",
        "E_test=np.average(E_1_test, weights = best_weights_bagboost,axis=0)\n",
        "\n",
        "\n",
        "F_1=np.array([G_test,D_test,E_test])\n",
        "# F=np.average(F_1, weights = [1,1,1],axis=0)\n",
        "\n",
        "# H_1=np.array([yhat_baseboost_1,yhat_baseboost_2,yhat_baseboost_3,yhat_baseboost_4,yhat_baseboost_5,yhat_bag_1,yhat_bag_2,yhat_bag_3,yhat_bag_4,yhat_bag_5,yhat_bagboost_1,yhat_bagboost_2,yhat_bagboost_3,yhat_bagboost_4,yhat_bagboost_5])\n",
        "\n",
        "# H=np.average(H_1, weights=best_weights_super,axis=0)\n",
        "\n",
        "\n",
        "\n",
        "yhat=E_test\n",
        "rmse,p,m,m_c= prediction_test(yhat)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(54080, 6) (54080, 6)\n",
            "2.5830760317758057\n",
            "4.6439414225147155\n",
            "3.8709470409814193\n",
            "2.2079163176147376\n",
            "3.606666747061417\n",
            "2.613334447529932\n",
            "\n",
            "\n",
            "0.9935174994294204\n",
            "0.9929508111941135\n",
            "0.9594228514320893\n",
            "0.9928787189910463\n",
            "0.9934649105194338\n",
            "0.9658697587461922\n",
            "Mean: 3.254 +/- 0.937\n",
            "Mean: 0.983 +/- 0.016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biISOWghsX8K"
      },
      "source": [
        "G=np.average(G_1, weights = best_weights_baseboost,axis=0)\n",
        "D=np.average(D_1, weights = best_weights_bag,axis=0)\n",
        "E=np.average(E_1, weights = best_weights_bagboost,axis=0)\n",
        "\n",
        "I_1=np.array([G,D,E])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZbxaIBrtNia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd2cbd9b-5906-4d50-806f-b2e82f1c64fb"
      },
      "source": [
        "print(I_1.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 11071, 480)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9hRem6Gs7lv"
      },
      "source": [
        "def objective_super_meta(weights):\n",
        "\n",
        "    yhat= np.average(I_1, axis=0, weights=weights)\n",
        "    m= prediction (yhat)\n",
        "\n",
        "    return m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzfhODKQtFwS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7262d40e-2767-43ee-8253-3640d2c1d104"
      },
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "# I define initial weights from which the algorithm will try searching a minima\n",
        "# I usually set the initial weigths to be the same for each columns, but they\n",
        "# can be set randomly\n",
        "w0 = np.empty(I_1.shape[0])\n",
        "w0.fill(1/I_1.shape[0])\n",
        "# w0.fill(1)\n",
        "\n",
        "# I define bounds, i.e. lower and upper values of weights.\n",
        "# I want the weights to be between 0 and 1.\n",
        "bounds = [(0,1)] * I_1.shape[0]\n",
        "\n",
        "# I set some constraints. Here, I want the sum of the weights to be equal to 1\n",
        "cons = [{'type': 'eq',\n",
        "         'fun': lambda w: w.sum() - 1}]\n",
        "\n",
        "# Then, I try to find the weights that will minimize my objective function.\n",
        "# There are several solvers (methods) to choose from. I use SLSQP because\n",
        "# it can handle constraints.\n",
        "res = minimize(objective_super_meta,\n",
        "               w0,\n",
        "               method='SLSQP',\n",
        "               bounds=bounds,\n",
        "               options={'disp':True, 'maxiter':10000},\n",
        "               constraints=cons)\n",
        "\n",
        "best_weights_super_meta = res.x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimization terminated successfully.    (Exit mode 0)\n",
            "            Current function value: 4.443584438012937\n",
            "            Iterations: 10\n",
            "            Function evaluations: 50\n",
            "            Gradient evaluations: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4wFbW4htYXJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d4b991f-9646-4e9a-fe05-993ba227cd9d"
      },
      "source": [
        "I=np.average(F_1, weights=best_weights_super_meta,axis=0)\n",
        "yhat=I\n",
        "rmse,p,m,m_c= prediction_test(yhat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(54080, 6) (54080, 6)\n",
            "2.6274463388892753\n",
            "4.956579695825089\n",
            "3.9244371466106536\n",
            "2.2106954214857253\n",
            "3.2521743082537475\n",
            "2.6585431726886504\n",
            "\n",
            "\n",
            "0.9934715942074219\n",
            "0.9931355395523916\n",
            "0.9595456690435317\n",
            "0.9926496215236542\n",
            "0.9936506606954119\n",
            "0.9661202631758771\n",
            "Mean: 3.272 +/- 1.018\n",
            "Mean: 0.983 +/- 0.016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCI74nujvfmk"
      },
      "source": [
        "def prediction (yhat):\n",
        "    import statistics\n",
        "\n",
        "\n",
        "    test_o=test_y.reshape((test_y.shape[0]*w,6))\n",
        "    yhat=yhat.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "    # test_o=train_y_5.reshape((train_y_5.shape[0]*w,6))\n",
        "    # yhat=yhat.reshape((train_y_5.shape[0]*w,6))\n",
        "\n",
        "    # print(test_o.shape,yhat.shape)\n",
        "\n",
        "    # test_o=test_y_ridge.reshape((test_y_ridge.shape[0]*w,6))\n",
        "    # yhat=yhat_1.reshape((test_y_ridge.shape[0]*w,6))\n",
        "\n",
        "\n",
        "    y_1_no=yhat[:,0]\n",
        "    y_2_no=yhat[:,1]\n",
        "    y_3_no=yhat[:,2]\n",
        "    y_4_no=yhat[:,3]\n",
        "    y_5_no=yhat[:,4]\n",
        "    y_6_no=yhat[:,5]\n",
        "    #y_7_no=yhat[:,6]\n",
        "    # y_8_no=yhat[:,7]\n",
        "    # y_9_no=yhat[:,8]\n",
        "    # y_10_no=yhat[:,9]\n",
        "    # y_11_no=yhat[:,10]\n",
        "    # y_12_no=yhat[:,11]\n",
        "    # y_13_no=yhat[:,12]\n",
        "    # y_14_no=yhat[:,13]\n",
        "    # y_15_no=yhat[:,14]\n",
        "    # y_16_no=yhat[:,15]\n",
        "    #\n",
        "\n",
        "    #### without filtering ###\n",
        "\n",
        "\n",
        "    # y_1=yhat[:,0]\n",
        "    # y_2=yhat[:,1]\n",
        "    # y_3=yhat[:,2]\n",
        "    # y_4=yhat[:,3]\n",
        "    # y_5=yhat[:,4]\n",
        "    # y_6=yhat[:,5]\n",
        "    # y_7=yhat[:,6]\n",
        "    # y_8=yhat[:,7]\n",
        "    # y_9=yhat[:,8]\n",
        "    # y_10=yhat[:,9]\n",
        "    # y_11=yhat[:,10]\n",
        "    # y_12=yhat[:,11]\n",
        "    # y_13=yhat[:,12]\n",
        "    # y_14=yhat[:,13]\n",
        "    # y_15=yhat[:,14]\n",
        "    # y_16=yhat[:,15]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    y_test_1=test_o[:,0]\n",
        "    y_test_2=test_o[:,1]\n",
        "    y_test_3=test_o[:,2]\n",
        "    y_test_4=test_o[:,3]\n",
        "    y_test_5=test_o[:,4]\n",
        "    y_test_6=test_o[:,5]\n",
        "    # y_test_7=test_o[:,6]\n",
        "    # y_test_8=test_o[:,7]\n",
        "    # y_test_9=test_o[:,8]\n",
        "    # y_test_10=test_o[:,9]\n",
        "    # y_test_11=test_o[:,10]\n",
        "    # y_test_12=test_o[:,11]\n",
        "    # y_test_13=test_o[:,12]\n",
        "    # y_test_14=test_o[:,13]\n",
        "    # y_test_15=test_o[:,14]\n",
        "    # y_test_16=test_o[:,15]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #print(y_1.shape,y_test_1.shape)\n",
        "\n",
        "\n",
        "\n",
        "    cutoff=6\n",
        "    fs=100\n",
        "    order=2\n",
        "\n",
        "    nyq = 0.5 * fs\n",
        "    ## filtering data ##\n",
        "    def butter_lowpass_filter(data, cutoff, fs, order):\n",
        "        normal_cutoff = cutoff / nyq\n",
        "        # Get the filter coefficients\n",
        "        b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "        y = filtfilt(b, a, data)\n",
        "        return y\n",
        "\n",
        "\n",
        "\n",
        "    y_1= butter_lowpass_filter(y_1_no, cutoff, fs, order)\n",
        "    y_2= butter_lowpass_filter(y_2_no, cutoff, fs, order)\n",
        "    y_3= butter_lowpass_filter(y_3_no, cutoff, fs, order)\n",
        "    y_4= butter_lowpass_filter(y_4_no, cutoff, fs, order)\n",
        "    y_5= butter_lowpass_filter(y_5_no, cutoff, fs, order)\n",
        "    y_6= butter_lowpass_filter(y_6_no, cutoff, fs, order)\n",
        "    # # y_7= butter_lowpass_filter(y_7_no, cutoff, fs, order)\n",
        "    # y_8= butter_lowpass_filter(y_8_no, cutoff, fs, order)\n",
        "    # y_9= butter_lowpass_filter(y_9_no, cutoff, fs, order)\n",
        "    # y_10= butter_lowpass_filter(y_10_no, cutoff, fs, order)\n",
        "    # y_11= butter_lowpass_filter(y_11_no, cutoff, fs, order)\n",
        "    # y_12= butter_lowpass_filter(y_12_no, cutoff, fs, order)\n",
        "    # y_13= butter_lowpass_filter(y_13_no, cutoff, fs, order)\n",
        "    # y_14= butter_lowpass_filter(y_14_no, cutoff, fs, order)\n",
        "    # y_15= butter_lowpass_filter(y_15_no, cutoff, fs, order)\n",
        "    # y_16= butter_lowpass_filter(y_16_no, cutoff, fs, order)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ###calculate RMSE\n",
        "\n",
        "    rmse_1 =np.sqrt(mean_squared_error(y_test_1,y_1))\n",
        "    rmse_2 =np.sqrt(mean_squared_error(y_test_2,y_2))\n",
        "    rmse_3 =np.sqrt(mean_squared_error(y_test_3,y_3))\n",
        "    rmse_4 =np.sqrt(mean_squared_error(y_test_4,y_4))\n",
        "    rmse_5 =np.sqrt(mean_squared_error(y_test_5,y_5))\n",
        "    rmse_6 =np.sqrt(mean_squared_error(y_test_6,y_6))\n",
        "    # rmse_7 =np.sqrt(mean_squared_error(y_test_7,y_7))\n",
        "    # rmse_8 =np.sqrt(mean_squared_error(y_test_8,y_8))\n",
        "    # rmse_9 =np.sqrt(mean_squared_error(y_test_9,y_9))\n",
        "    # rmse_10 =np.sqrt(mean_squared_error(y_test_10,y_10))\n",
        "    # rmse_11 =np.sqrt(mean_squared_error(y_test_11,y_11))\n",
        "    # rmse_12 =np.sqrt(mean_squared_error(y_test_12,y_12))\n",
        "    # rmse_13 =np.sqrt(mean_squared_error(y_test_13,y_13))\n",
        "    # rmse_14 =np.sqrt(mean_squared_error(y_test_14,y_14))\n",
        "    # rmse_15 =np.sqrt(mean_squared_error(y_test_15,y_15))\n",
        "    # rmse_16 =np.sqrt(mean_squared_error(y_test_16,y_16))\n",
        "\n",
        "    # calculate RMSE\n",
        "    # rmse_1 =(np.sqrt(mean_squared_error(y_test_1,y_1)))*(180/math.pi)\n",
        "    # rmse_2 =(np.sqrt(mean_squared_error(y_test_2,y_2)))*(180/math.pi)\n",
        "    # rmse_3 =(np.sqrt(mean_squared_error(y_test_3,y_3)))*(180/math.pi)\n",
        "    # rmse_4 =(np.sqrt(mean_squared_error(y_test_4,y_4)))*(180/math.pi)\n",
        "    # rmse_5 =(np.sqrt(mean_squared_error(y_test_5,y_5)))*(180/math.pi)\n",
        "    # rmse_6 =(np.sqrt(mean_squared_error(y_test_6,y_6)))*(180/math.pi)\n",
        "\n",
        "\n",
        "    ##Calculate NRMSE\n",
        "\n",
        "    # rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_1)-min(y_1)))*100\n",
        "    # rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_1)-min(y_1)))*100\n",
        "    # rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_1)-min(y_1)))*100\n",
        "    # rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_1)-min(y_1)))*100\n",
        "    # rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_1)-min(y_1)))*100\n",
        "    # rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_1)-min(y_1)))*100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # print(rmse_1)\n",
        "    # print(rmse_2)\n",
        "    # print(rmse_3)\n",
        "    # print(rmse_4)\n",
        "    # print(rmse_5)\n",
        "    # print(rmse_6)\n",
        "    # print('Test RMSE: %.3f' % rmse_7)\n",
        "    # print('Test RMSE: %.3f' % rmse_8)\n",
        "    # print('Test RMSE: %.3f' % rmse_9)\n",
        "    # print('Test RMSE: %.3f' % rmse_10)\n",
        "    # print('Test RMSE: %.3f' % rmse_11)\n",
        "    # print('Test RMSE: %.3f' % rmse_12)\n",
        "    # print('Test RMSE: %.3f' % rmse_13)\n",
        "    # print('Test RMSE: %.3f' % rmse_14)\n",
        "    # print('Test RMSE: %.3f' % rmse_15)\n",
        "    # print('Test RMSE: %.3f' % rmse_16)\n",
        "\n",
        "\n",
        "    # from scipy.stats import pearsonr\n",
        "\n",
        "    # p_1, _ =pearsonr(y_1, y_test_1)\n",
        "    # p_2, _ =pearsonr(y_2, y_test_2)\n",
        "    # p_3, _ =pearsonr(y_3, y_test_3)\n",
        "    # p_4, _ =pearsonr(y_4, y_test_4)\n",
        "    # p_5, _ =pearsonr(y_5, y_test_5)\n",
        "    # p_6, _ =pearsonr(y_6, y_test_6)\n",
        "\n",
        "\n",
        "\n",
        "    p_1=np.corrcoef(y_1, y_test_1)[0, 1]\n",
        "    p_2=np.corrcoef(y_2, y_test_2)[0, 1]\n",
        "    p_3=np.corrcoef(y_3, y_test_3)[0, 1]\n",
        "    p_4=np.corrcoef(y_4, y_test_4)[0, 1]\n",
        "    p_5=np.corrcoef(y_5, y_test_5)[0, 1]\n",
        "    p_6=np.corrcoef(y_6, y_test_6)[0, 1]\n",
        "    # p_7=np.corrcoef(y_7, y_test_7)[0, 1]\n",
        "    # p_8=np.corrcoef(y_8, y_test_8)[0, 1]\n",
        "    # p_9=np.corrcoef(y_9, y_test_9)[0, 1]\n",
        "    # p_10=np.corrcoef(y_10, y_test_10)[0, 1]\n",
        "    # p_11=np.corrcoef(y_11, y_test_11)[0, 1]\n",
        "    # p_12=np.corrcoef(y_12, y_test_12)[0, 1]\n",
        "    # p_13=np.corrcoef(y_13, y_test_13)[0, 1]\n",
        "    # p_14=np.corrcoef(y_14, y_test_14)[0, 1]\n",
        "    # p_15=np.corrcoef(y_15, y_test_15)[0, 1]\n",
        "    # p_16=np.corrcoef(y_16, y_test_16)[0, 1]\n",
        "\n",
        "    #print(p_1,p_2,p_3,p_4,p_5,p_6,p_7,p_8,p_9,p_10,p_11,p_12,p_13,p_14,p_15,p_16)\n",
        "\n",
        "    #print(p_1,p_2,p_3,p_4,p_5,p_6,p_7,p_8,p_9,p_10)\n",
        "\n",
        "    # print(\"\\n\")\n",
        "    # print(p_1)\n",
        "    # print(p_2)\n",
        "    # print(p_3)\n",
        "    # print(p_4)\n",
        "    # print(p_5)\n",
        "    # print(p_6)\n",
        "    #print(p_1,p_2,p_3,p_4)\n",
        "\n",
        "\n",
        "                ### Correlation ###\n",
        "    #p=np.array([p_1,p_2,p_3,p_4,p_5,p_6,p_7,p_8,p_9,p_10,p_11,p_12,p_13,p_14,p_15,p_16])\n",
        "    #p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])\n",
        "\n",
        "    #p=pd.DataFrame([p_1,p_2,p_3,p_4,p_5,p_6,p_7,p_8,p_9,p_10])\n",
        "\n",
        "    # p=np.array([p_1,p_2,p_3,p_4,p_5,p_6])\n",
        "\n",
        "    p=np.array([(p_1+p_4)/2,(p_2+p_5)/2,(p_3+p_6)/2])\n",
        "\n",
        "\n",
        "    #p=np.array([p_1,p_2,p_3,p_4])\n",
        "\n",
        "\n",
        "\n",
        "        #### Mean and standard deviation ####\n",
        "\n",
        "    #rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6,rmse_7,rmse_8,rmse_9,rmse_10])\n",
        "    # rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5,rmse_6])\n",
        "\n",
        "    rmse=np.array([(rmse_1+rmse_4)/2,(rmse_2+rmse_5)/2,(rmse_3+rmse_6)/2])\n",
        "\n",
        "    #rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4])\n",
        "\n",
        "    # m=statistics.mean(rmse)\n",
        "    # SD=statistics.stdev(rmse)\n",
        "    # print('Mean: %.3f' % m,'+/- %.3f' %SD)\n",
        "\n",
        "    # m_c=statistics.mean(p)\n",
        "    # SD_c=statistics.stdev(p)\n",
        "    # print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)\n",
        "    # print(rmse)\n",
        "    # print(p)\n",
        "\n",
        "    return rmse,p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8S9kE3_NqcJ"
      },
      "source": [
        "### Model 1 ###\n",
        "rmse_1,p_1= prediction (yhat_base_1)\n",
        "rmse_2,p_2= prediction (yhat_baseboost_1)\n",
        "rmse_3,p_3= prediction (yhat_bag_1)\n",
        "rmse_4,p_4= prediction (yhat_bagboost_1)\n",
        "\n",
        "rmse_model_1=np.array([rmse_1,rmse_2,rmse_3,rmse_4])\n",
        "p_model_1=np.array([p_1,p_2,p_3,p_4])\n",
        "\n",
        "### Model 2 ###\n",
        "\n",
        "rmse_1,p_1= prediction (yhat_base_2)\n",
        "rmse_2,p_2= prediction (yhat_baseboost_2)\n",
        "rmse_3,p_3= prediction (yhat_bag_2)\n",
        "rmse_4,p_4= prediction (yhat_bagboost_2)\n",
        "\n",
        "rmse_model_2=np.array([rmse_1,rmse_2,rmse_3,rmse_4])\n",
        "p_model_2=np.array([p_1,p_2,p_3,p_4])\n",
        "\n",
        "### Model 3 ###\n",
        "\n",
        "rmse_1,p_1= prediction (yhat_base_3)\n",
        "rmse_2,p_2= prediction (yhat_baseboost_3)\n",
        "rmse_3,p_3= prediction (yhat_bag_3)\n",
        "rmse_4,p_4= prediction (yhat_bagboost_3)\n",
        "\n",
        "rmse_model_3=np.array([rmse_1,rmse_2,rmse_3,rmse_4])\n",
        "p_model_3=np.array([p_1,p_2,p_3,p_4])\n",
        "\n",
        "\n",
        "### Model 4 ###\n",
        "\n",
        "rmse_1,p_1= prediction (yhat_base_4)\n",
        "rmse_2,p_2= prediction (yhat_baseboost_4)\n",
        "rmse_3,p_3= prediction (yhat_bag_4)\n",
        "rmse_4,p_4= prediction (yhat_bagboost_4)\n",
        "\n",
        "rmse_model_4=np.array([rmse_1,rmse_2,rmse_3,rmse_4])\n",
        "p_model_4=np.array([p_1,p_2,p_3,p_4])\n",
        "\n",
        "\n",
        "### Model 5 ###\n",
        "\n",
        "rmse_1,p_1= prediction (yhat_base_5)\n",
        "rmse_2,p_2= prediction (yhat_baseboost_5)\n",
        "rmse_3,p_3= prediction (yhat_bag_5)\n",
        "rmse_4,p_4= prediction (yhat_bagboost_5)\n",
        "\n",
        "rmse_model_5=np.array([rmse_1,rmse_2,rmse_3,rmse_4])\n",
        "p_model_5=np.array([p_1,p_2,p_3,p_4])\n",
        "\n",
        "### Stack family ###\n",
        "\n",
        "rmse_1,p_1= prediction (C_test)\n",
        "rmse_2,p_2= prediction (G_test)\n",
        "rmse_3,p_3= prediction (D_test)\n",
        "rmse_4,p_4= prediction (E_test)\n",
        "rmse_5,p_5= prediction (I)\n",
        "\n",
        "rmse_model_6=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5])\n",
        "p_model_6=np.array([p_1,p_2,p_3,p_4,p_5])\n",
        "\n",
        "\n",
        "  ## All results _summarize ###\n",
        "RMSE_all=np.concatenate((rmse_model_1,rmse_model_2,rmse_model_3,rmse_model_4,rmse_model_5,rmse_model_6),axis=0)\n",
        "corr_all=np.concatenate((p_model_1,p_model_2,p_model_3,p_model_4,p_model_5,p_model_6),axis=0)\n",
        "\n",
        "results_all=np.concatenate((RMSE_all,corr_all),axis=1)\n",
        "from numpy import savetxt\n",
        "\n",
        "savetxt('/content/drive/My Drive/data for machine learning_stair_slope/Paper results 1/subject_1_treadmill.csv', results_all, delimiter=',')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}